# Query-Competitive Sorting with Uncertainty 

Magnús M. Halldórsson ${ }^{\mathrm{a}}$, Murilo Santos de Lima ${ }^{\mathrm{b}, *}$<br>${ }^{a}$ Department of Computer Science, Reykjavik University, Menntavegi 1, 102 Reykjavik, Iceland<br>${ }^{\mathrm{b}}$ School of Informatics, University of Leicester, University Road, Leicester, LE1 7RH, United Kingdom


#### Abstract

We study the problem of sorting under incomplete information, when queries are used to resolve uncertainties. Each of $n$ data items has an unknown value, which is known to lie in a given interval. We can pay a query cost to learn the actual value, and we may allow an error threshold in the sorting. The goal is to find a nearly-sorted permutation by performing a minimum-cost set of queries.

We show that an offline optimum query set can be found in polynomial time, and that both oblivious and adaptive problems have simple query-competitive algorithms. The query-competitiveness for the oblivious problem is $n$ for uniform query costs, and unbounded for arbitrary costs; for the adaptive problem, the ratio is 2 .

We then present a unified adaptive strategy for uniform query costs that yields the following improved results: (i) a $3 / 2$-query-competitive randomized algorithm; (ii) a $5 / 3$-query-competitive deterministic algorithm if the dependency graph has no 2 -components after some preprocessing, which has query-competitive ratio $3 / 2+\mathrm{O}(1 / k)$ if the components obtained have size at least $k$; and (iii) an exact algorithm if the intervals constitute a laminar family. The first two results have matching lower bounds, and we have a lower bound of $7 / 5$ for large components.

We also give a randomized adaptive algorithm with query-competitive factor $1+\frac{4}{3 \sqrt{3}} \approx 1.7698$ for arbitrary query costs, and we show that the 2 -query competitive deterministic adaptive algorithm can be generalized for queries returning intervals and for a more general graph problem (which is also a generalization of the vertex cover problem), by using the local ratio technique. Furthermore, we prove that the advice complexity of the adaptive problem is $\lfloor n / 2\rfloor$ if no error threshold is allowed, and $\lceil n / 3 \cdot \lg 3\rceil$ for the general case.

Finally, we present some graph-theoretical results regarding co-threshold tolerance graphs, and we discuss uncertainty variants of some classical interval problems.


Keywords: online algorithms, sorting, randomized algorithms, advice complexity, threshold tolerance graphs

## 1. Introduction

Sorting is one of the most fundamental problems in computer science and an essential part of any system dealing with large amounts of data. High-performance algorithms such as QuickSort [26] have been known for decades, but the demand for fast sorting of huge amounts of data is such that improvements in sorting algorithms are still an active area of research; see, e.g., [35].

In a distributed application with dynamic data, it may not be feasible to maintain a precise copy of the information in each replica. In particular, accessing a local cached information may be much cheaper,

[^0]
[^0]:    *Partially supported by Icelandic Research Fund grant 174484-051. A preliminary version of this paper appeared in volume 138 of LIPICs, article 7, 2019. DOI: 10.4230/LIPIcs.MFCS. 2019.7
    ${ }^{\text {a }}$ Corresponding author
    Email addresses: mmb@ru.is (Magnús M. Halldórsson), mslima@ic.unicamp.br (Murilo Santos de Lima)even though not as precise, than querying a master database or to run a distributed consensus algorithm. One approach is to maintain in the replicas, for each data item, an interval that bounds the actual value. These intervals can be updated much faster than to guarantee a strict consistency of the data. When higher precision is required, the system can query the master database for a more fine-grained interval or for the actual data value. Therefore a trade-off between data precision and system performance can be established. The TRAPP system, proposed by Olston and Widom [33], relies on this concept.

This idea has led to theoretical investigation on uncertainty problems with queries [9, 15, 16, 17, 18, $21,31]$. Such problems also appear in optimization scenarios in which an extra effort can be incurred in order to obtain more precise values of the input data, such as by investing in market research, which is expensive so its cost should be minimized. These works build upon more established frameworks of optimization with uncertainty, such as online [7], robust [5] and stochastic [6] optimization. In particular, the analysis of algorithms in terms of competitiveness against an adversary is inherited from the online optimization literature.

In this paper, we investigate the problem of sorting data items whose actual values are unknown, but for which we are given intervals on which the actual values lie. We can query an interval and then learn the actual value of the corresponding data item, but this incurs some cost. The goal, then, is to sort the items by performing a set of queries of minimum cost. Furthermore, the precision in the sorting may be relaxed, so that inversions may occur if the actual values are not too far apart.

We distinguish between two types of algorithms for uncertainty problems with queries. An adaptive algorithm may decide which queries to perform based on results from previous queries. An oblivious algorithm, however, must choose the whole set of queries to perform in advance; i.e., it must choose a set of queries that certainly allow the problem to be solved without any knowledge of the actual values. In this paper, both algorithms are compared with an offline optimum query set, i.e., a minimum-cost set of queries that proves the obtained solution to be correct. ${ }^{1}$ An algorithm (either adaptive or oblivious) is $\alpha$-query-competitive if it performs a total query cost of at most $\alpha$ times the cost of an offline optimum query set.

Another related problem is that of finding an optimum query set. Here we are given the actual data values, and want to identify a minimum-cost set of queries that would be sufficient to prove that the solution is correct. Solving this problem is useful, for example, to perform experimental evaluation of online algorithms, since we are actually finding the offline optimum solution for the uncertainty problem. This is also called the verification version of the corresponding uncertainty problem with queries [11, 15].

We are also interested in the advice complexity of the problems we study. In this setting, an online algorithm has access to an oracle that can give helpful information when making decisions. The advice complexity is the number of bits of advice that are sufficient and necessary for an online algorithm to solve the problem exactly. This is a research topic that has gained substantial attention; see [8] for a survey.

Our contribution. We begin by showing how to compute an optimum query set in polynomial time, and that both oblivious and adaptive problems have simple algorithms with matching deterministic lower bounds. The query-competitive ratio of the oblivious problem is $n$ if we have uniform query costs, and unbounded for arbitrary costs; for the adaptive problem, the query-competitive ratio is 2 . The optimal oblivious algorithm is trivial; for the adaptive case, we have a simpler algorithm for uniform query costs, and a more sophisticated one for arbitrary query costs. If query costs are uniform and the error threshold is zero, then the simpler algorithm can be implemented as an oracle for any comparison-based sorting algorithm, preserving time complexity and stability.

At this point it seems like the query-competitiveness of the problem is settled. However, we present a unified adaptive strategy that attains different improvements for uniform query costs. First, we obtain

[^0]
[^0]:    ${ }^{1}$ This nomenclature differs to that used by Feder et al. [18]. They call an adaptive algorithm an online algorithm, and an oblivious algorithm an offline algorithm. We disagree with this nomenclature, since both types of algorithms are online in the standard sense of not knowing the data. Also, they compare an oblivious algorithm to an optimal oblivious strategy, and not to an offline optimum query set.a 3/2-query-competitive algorithm by using randomization. Second, if the error threshold is zero, and after some preprocessing the dependency graph has no 2-components, the strategy yields a deterministic $5 / 3$-query-competitive algorithm; if the obtained graph has components of size at least $k$, then the same algorithm has query-competitive ratio $3 / 2+\mathrm{O}(1 / k)$. The first two results have a matching lower bound, and for large components we have a lower bound of $7 / 5$. The problem can also be solved exactly if the intervals constitute a laminar family.

We then present a randomized adaptive algorithm for arbitrary query costs. We start with an algorithm with query-competitive factor $57 / 32=1.78125$, and then we improve it to $1+\frac{4}{3 \sqrt{3}} \approx 1.7698$ simply by changing the probabilities in the randomized step. We also show that the 2 -query competitive deterministic algorithm for arbitrary query costs can be generalized for queries returning intervals, a model which was proposed in [25], and that this is the best possible factor for this case, even for a randomized algorithm. The algorithm is adapted from the local ratio approximation algorithm for the vertex cover problem [4], and it can also be used to solve a more general graph problem which is also a generalization of the vertex cover problem. Furthermore, we show that the advice complexity for adaptive algorithms is exactly $\lfloor n / 2\rfloor$ bits if there is no error threshold, and exactly $\lceil n / 3 \cdot \lg 3\rceil$ bits for the general case.

Finally, we present some results regarding the graph class defined by our sorting problem with uncertainty, which turns out to be the class of co-threshold tolerance (co-TT) graphs [32]. We also discuss uncertainty variants of two classical interval problems, which inspired us to approach the sorting problem with uncertainty: the maximum independent set problem and the stabbing number problem. Both problems have query-competitive factor at least $n-1$, where $n$ is the number of intervals, even if query costs are uniform and lower bounds are trivial.

Related work. The first work to investigate the minimum number of queries to solve a problem is by Kahan [27], who showed optimal adaptive strategies to find the minimum/maximum and median of $n$ values in uncertainty intervals, and for the sorting and closest pair problems.

Olston and Widom [33] proposed the TRAPP system, a distributed database based on uncertainty intervals. The authors: (1) gave an optimal oblivious strategy for finding the minimum (and equivalently, the maximum) of a sequence of values within an error bound; (2) showed that it is NP-hard to find an optimum oblivious query set to compute the sum of a sequence of values within an error bound, with a reduction from the knapsack problem. The paper also discusses strategies for counting and finding the average of a sequence of values. Khanna and Tan [28] generalized these results for arbitrary query costs and different levels of precision.

Feder et al. [18] considered the uncertainty version of the problem of finding the $k$-th largest value on a sequence (i.e., the generalized median problem). The authors presented optimal oblivious and adaptive strategies for the problem, both running in polynomial time. Both strategies are optimal, and the ratio between the oblivious and the adaptive strategy (also called the price of obliviousness) is $\frac{2 k-1}{k}<2$ for uniform query costs, and $k$ for arbitrary query costs. ${ }^{2}$

Bruce et al. [9] studied geometric problems where the points are given in uncertainty areas. The authors gave 3 -query-competitive algorithms for finding the maximal points and the convex hull in a two-dimensional space. They also proposed the concept of witness sets, which has been used subsequently in various works on uncertainty problems with queries. Charalambous and Hoffman [11] showed that it is NP-hard to find an optimum query set for the maximal points problem.

Feder et al. [17] studied the uncertainty variant of the shortest path problem. They showed that optimally solving the oblivious version of the problem is neither NP nor co-NP, unless NP $=$ co-NP. Their paper also discusses the complexity of the problem for various particular cases.

Erlebach et al. [16] proved that the minimum spanning tree problem with uncertainty admits an adaptive 2 -query-competitive algorithm, which is the best possible for a deterministic algorithm. Erlebach and Hoffman [13] showed that an optimum query set for the minimum spanning tree problem can be computed in polynomial time. Erlebach, Hoffmann and Kammer [15] studied a generalization called the cheapest set

[^0]
[^0]:    ${ }^{2}$ The works cited up to this point do not evaluate the algorithms using the competitiveness framework.problem, for which there is an adaptive algorithm with at most $d \cdot \operatorname{opt}+d$ queries, where $d$ is the maximum cardinality of a set. They also generalized the result in [16] to obtain an adaptive 2 -query-competitive algorithm for the problem of finding a minimum-weight base on a matroid.

Gupta, Sabharwal and Sen [25] studied various of the previous problems in the setting where a query may return a refined interval, instead of the exact value of the data item.

Megow, Meißner and Skutella [31] improved the result for the minimum spanning tree problem with a randomized adaptive algorithm, obtaining query-competitive ratio about 1.7. (The problem has lower bound 1.5 for randomized algorithms.) They also considered non-uniform query costs and proved that their results can be extended to find a minimum-weight base on a matroid. Furthermore, they showed that the actual value of the minimum spanning tree can be computed in polynomial time. Some experimental evaluation of those algorithms were presented in [19].

Ryzhov and Powell [34] investigated how to solve a linear program while minimizing the query cost when the coefficients of the objective function are uncertain. They presented a policy which is asymptotically optimal. Maehara and Yamaguchi [30] studied the variant with packing constraints and coefficients following a probability distribution, and showed how to apply this to stochastic problems such as matching, matroid and stable set problems.

Note that all the work cited so far deals with problems whose classical (offline) versions can be solved in polynomial time. Uncertainty versions with queries have been proposed for the knapsack problem [21] and the scheduling problem $[2,12]$. Since those problems are NP-hard, we might include the query cost into the solution cost and look for a competitive algorithm if we are looking for a polynomial-time algorithm. Another option is to limit the maximum number of queries performed, and then to try to optimize the solution cost.

For a survey on the topic, see [14]. Other references for related problems are also cited in [15].
Another sorting problem with uncertainty was studied by Ajtain et al. [1]. In that problem, the values to be sorted are unknown, but their relative order can be tested by a comparison procedure. However, comparing values that are too close returns imprecise answers, so in principle we should compare all $\binom{n}{2}$ pairs to obtain a sorting with some error guarantee. The authors show how to solve the problem using only $\mathrm{O}\left(n^{3 / 2}\right)$ comparisons.

Organization of the paper. In Section 2, we present the sorting problem with uncertainty and some basic facts, and in Section 3 we give algorithms to find an offline optimum query set and for the oblivious setting. We treat deterministic adaptive algorithms in Section 4. In Section 5, we show how to improve the adaptive result for uniform query costs by using a randomized algorithm, or by assuming some structure in the dependency graph. We present a randomized adaptive algorithm for arbitrary query costs in Section 6, we discuss the variant of the problem in which queries may return intervals in Section 7, and in Section 8 we investigate the advice complexity for adaptive algorithms. Finally, in Section 9 we present some graphtheoretical results, and we discuss uncertainty variants of two classical interval problems in Section 10.

# 2. Sorting with Uncertainty 

In the sorting problem with uncertainty, there are $n$ numbers $v_{1}, \ldots, v_{n} \in \mathbb{R}$ whose exact value is unknown. We are given $n$ uncertainty intervals $I_{1}, \ldots, I_{n}$ with $v_{i} \in I_{i}=\left[\ell_{i}, r_{i}\right]$, a cost $w_{i} \in \mathbb{R}_{+}$for querying interval $I_{i}$, and an error threshold $\delta \geq 0$. After querying $I_{i}$, we obtain the exact value of $v_{i}$; we can also say that we replace $I_{i}$ with interval $I_{i}^{\prime}=\left[v_{i}, v_{i}\right]$. The goal is to obtain a permutation $\pi:[n] \rightarrow[n]$ such that $v_{i} \leq v_{j}+\delta$ if $\pi(i)<\pi(j)$ by performing a minimum-cost set of queries.

We begin by defining the following dependency relation between intervals, which is essential to solve the problem.

Definition 1. Two intervals $I_{i}$ and $I_{j}$ such that $r_{i}-\ell_{j}>\delta$ and $r_{j}-\ell_{i}>\delta$ are dependent. Two intervals that are not dependent are independent.Lemma 2. The relative order between two intervals can be decided without querying either of them if and only if they are independent.

Proof. Let $I_{i}$ and $I_{j}$ be such that $r_{i}-\ell_{j} \leq \delta$. Since $v_{i} \leq r_{i}$ and $v_{j} \geq \ell_{j}$, we have that $v_{i} \leq v_{j}+\delta$ and we can set $\pi(i)<\pi(j)$ without querying either of $I_{i}$ and $I_{j}$.

Conversely, let $I_{j}$ and $I_{j}$ be two dependent intervals. We cannot set $\pi(i)<\pi(j)$, because it may be the case that $v_{i}=r_{i}$ and $v_{j}=\ell_{j}$, thus $r_{i}-\ell_{j}>\delta$ implies that $v_{i}>v_{j}+\delta$. By a symmetric argument, we cannot set $\pi(j)<\pi(i)$, so we cannot decide the relative order between the intervals without making a query.

So, essentially, to solve the sorting problem with uncertainty consists in querying intervals so that the graph defined by this dependency relation has no edges. Querying both endpoints is sufficient to remove an edge, but sometimes it is enough to query one of them. A solution is, therefore, a vertex cover in the dependency graph, but an optimum offline solution may not be a minimum vertex cover.

The graphs defined by this dependency relation are exactly the co-threshold tolerance (co-TT) graphs [32]. $G=(V, E)$ is a threshold tolerance graph if there are functions $w: V \rightarrow \mathbb{R}$ and $t: V \rightarrow \mathbb{R}$ such that $u v \in E$ if and only if $w(u)+w(v) \geq \min (t(u), t(v))$. A co-TT graph is the complement of any threshold tolerance graph, or equivalently, $G=(V, E)$ is a co-TT graph if and only if there are functions $a: V \rightarrow \mathbb{R}$ and $b: V \rightarrow \mathbb{R}$ such that $u v \in E$ if and only if $a(u)<b(v)$ and $a(v)<b(u)$ [32].

Theorem 3. The graphs defined by the dependency relation in Definition 1 are exactly the co-TT graphs.
Proof. Given an instance $I_{1}, \ldots, I_{n}$ and an error threshold $\delta$, we simply define $a\left(I_{i}\right):=\ell_{i}$ and $b\left(I_{i}\right):=r_{i}-\delta$.
Conversely, given a co-TT graph $G=(V, E)$ with functions $a$ and $b$, we set $\delta:=\max _{v \in V}\{a(v)-b(v), 0\}$, and for every $v \in V$ we set $\ell_{v}:=a(v)$ and $r_{v}:=b(v)+\delta$. Both graphs have the same adjacency relation, $\delta \geq 0$, and $r_{v} \geq \ell_{v}$, so $\left[\ell_{v}, r_{v}\right]$ is an interval.

The following result will be useful.
Lemma 4 ([32]). Every co-TT graph is chordal.
When $\delta>0$, it is useful to distinguish intervals of width smaller than $\delta$, which we call trivial intervals. It is easy to check that two trivial intervals cannot be dependent, so when a trivial and a non-trivial interval are dependent, it is enough to query the non-trivial interval in order to decide their relative order. This does not mean, however, that trivial intervals should never be queried, and in particular adaptive algorithms may decide to do that.

It is also clear that the dependency graph is an interval graph when $\delta=0$. This is also true when $\delta>0$ and there are no trivial intervals, in which case we can simply replace each interval $I_{i}=\left[\ell_{i}, r_{i}\right]$ with $I_{i}^{(\delta)}=\left[\ell_{i}^{(\delta)}, r_{i}^{(\delta)}\right]:=\left[\ell_{i}+\delta / 2, r_{i}-\delta / 2\right] \neq \emptyset$, and it is easy to check that $I_{i}$ and $I_{j}$ are dependent with error threshold $\delta$ if and only if $I_{i}^{(\delta)}$ and $I_{j}^{(\delta)}$ are dependent with error threshold 0 . Note however that we cannot use this reduction to solve the sorting problem, since the precise values could fall outside of the given interval.

# 3. Warm-Up: Offline and Oblivious Algorithms 

The first result we present concerns finding the optimum query set for a given set of intervals, assuming we know the actual values in each interval. I.e., given the intervals $I_{1}, \ldots, I_{n}$ and the actual values $v_{1}, \ldots, v_{n}$, find a minimum-cost set $Q$ of intervals to query, such that $Q$ is sufficient to prove an ordering for $I_{1}, \ldots, I_{n}$ without the knowledge of $v_{1}, \ldots, v_{n}$. Solving this problem is useful, for example, to perform experimental evaluation of algorithms, since we are actually finding the offline optimum solution for the online (either oblivious or adaptive) problem. The ideas we present here will also be useful when solving the online problem.

We show that the problem can be solved optimally in polynomial time. The key observations behind the algorithm are the following. In order to simplify notation, we write $I_{i} \supset I_{j}$ for intervals $I_{i}$ and $I_{j}$ if $\ell_{i}<\ell_{j}$ and $r_{j}<r_{i}$.Fact 5. Let $I_{i}$ and $I_{j}$ be intervals with actual values $v_{i}$ and $v_{j}$. If $I_{j} \supset\left[v_{i}-\delta, v_{i}+\delta\right]$, then $I_{j}$ is queried by every optimum solution.

Proof. Even if we have queried $I_{i}$, we have to query $I_{j}$ because we may have $v_{j} \in\left[\ell_{j}, v_{i}-\delta\right)$ or $v_{j} \in$ $\left(v_{i}+\delta, r_{j}\right]$.

Fact 6. Let $I_{i}$ and $I_{j}$ be two dependent intervals, $v_{i}$ the actual value in $I_{i}$ and $v_{j}$ the actual value in $I_{j}$. If $I_{i} \not \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ and $I_{j} \not \supset\left[v_{i}-\delta, v_{i}+\delta\right]$, then it is enough to query either $I_{i}$ or $I_{j}$ to decide their relative order.

Proof. If we query $I_{i}$, then $v_{i} \notin\left[\ell_{j}+\delta, r_{j}-\delta\right]$, so we can pick a reasonable order between $I_{i}$ and $I_{j}$. The argument is symmetrical if we query $I_{j}$.

The algorithm begins with a query set $Q$ containing all intervals that satisfy the condition in Fact 5. Due to Fact 6 , it is enough to complement $Q$ with a minimum-cost vertex cover in the dependency graph defined by the remaining intervals, which can be found in polynomial time for chordal graphs [20].

Theorem 7. The problem of finding an optimum query set for the sorting problem with uncertainty can be solved optimally in polynomial time.

Now we consider oblivious algorithms. In this case, all non-trivial intervals with some dependence must be queried, and clearly this is the best possible strategy. In the following theorem, we show that this implies a tight bound of $n$ on the query-competitive ratio for the case with uniform costs, and that in the general case the query-competitive ratio is unbounded.

Theorem 8. If query costs are uniform, any oblivious algorithm for sorting with uncertainty has querycompetitive ratio exactly $n$. For arbitrary costs, the query-competitive ratio is unbounded.

Proof. For the upper bound with uniform costs, a naïve algorithm that queries all intervals and then sorts the numbers suffices.

For both lower bounds, we have $n-1$ independent intervals with length greater than $2 \delta$, plus an interval $I_{n}$ which contains all the other ones. Both an algorithm and the optimum solution must query $I_{n}$ in order to decide where $v_{n}$ fits in the order. If the algorithm does not query some $I_{i}$ with $i<n$, then the adversary can set $v_{n} \in\left(\ell_{i}+\delta, r_{i}-\delta\right) \neq \emptyset$ and the algorithm cannot decide the order. Thus, without the knowledge of $v_{n}$, the algorithm must query all $I_{i}$ with $i<n$. However, it may be the case that $v_{n} \notin I_{i}$ for all $i<n$, and querying $I_{n}$ suffices to decide the order. This gives a lower bound of $n$ on the query-competitive ratio for uniform query costs. For the general case, $w_{n}$ can be arbitrarily small and the query-competitive ratio is unbounded.

# 4. Deterministic Adaptive Algorithms 

Now let us consider deterministic adaptive algorithms. We begin with a lower bound.
Lemma 9. Any deterministic adaptive algorithm for the sorting problem with uncertainty has query-competitive ratio at least 2, even if query costs are uniform and the dependency graph has large components.

Proof. Consider intervals $I_{1}$ and $I_{2}$ with uniform query cost, $\ell_{1}<\ell_{2}<r_{1}<r_{2}$ and $r_{1}-\ell_{2}>2 \delta$. If the algorithm queries $I_{1}$, then the adversary chooses $v_{1} \in\left(\ell_{2}+\delta, r_{1}-\delta\right)$. The algorithm must also query $I_{2}$ to decide the order, but then the adversary can choose $v_{2} \in\left[r_{1}-\delta, r_{2}\right]$ and one query would be sufficient. The argument is symmetrical if the algorithm queries $I_{2}$ first, with $v_{2} \in\left(\ell_{2}+\delta, r_{1}-\delta\right)$ and $v_{1} \in\left[\ell_{1}, \ell_{2}+\delta\right]$. To obtain a large component, make several independent copies of this structure and connect them with a large interval containing all the others; in this case the lower bound approaches 2 asymptotically.First we give a simple deterministic 2 -query-competitive adaptive algorithm for the case with uniform query costs. It is inspired by the algorithm of Erlebach et al. [16] for the minimum spanning tree problem with uncertainty, and it relies on the following concepts, which were introduced in [9]. Let $\mathcal{I}=\left\{I_{1}, \ldots, I_{n}\right\}$ be a set of intervals for the sorting problem with uncertainty. We say that a set $W \subseteq \mathcal{I}$ of intervals is a witness set if at least one of the intervals in $W$ must be queried to decide the order of $\mathcal{I}$, even if all intervals except those in $W$ are queried. Due to Lemma 2, any pair of dependent intervals constitute a witness set. A set of intervals $\mathcal{I}^{\prime}=\left\{I_{1}^{\prime}, \ldots, I_{n}^{\prime}\right\}$ is a refinement of $\mathcal{I}$ if $\mathcal{I}^{\prime}$ is obtained from $\mathcal{I}$ by performing a sequence of queries. Fact 10 follows simply from $\mathcal{I}^{\prime}$ having more information than $\mathcal{I}$.

Fact 10. Let $\mathcal{I}^{\prime}$ be a refinement of $\mathcal{I}$. If some set of intervals $W \subseteq \mathcal{I}^{\prime} \cap \mathcal{I}$ is a witness set for $\mathcal{I}^{\prime}$, then it is a witness set for $\mathcal{I}$.

The algorithm, then, consists in the following. While there is some pair of dependent intervals, we query all intervals in this pair that have not been queried yet. When an interval $I_{i}$ is queried, it is replaced by $\left[v_{i}, v_{i}\right]$. (Note that, even after querying $I_{i}$, it may still be dependent to a non-trivial interval.) Finally, intervals are sorted by breaking ties arbitrarily. ${ }^{3}$

For a better understanding of the algorithm, consider the examples in Figure 1, assuming $\delta=0$. In Figure 1(a), the optimum solution must query $I_{1}$ and $I_{3}$, since $v_{1} \in I_{3}$ and $v_{3} \in I_{1}$, and this is enough because $I_{2}$ will be independent after querying $I_{1}$. If the algorithm first queries $I_{1}$ and $I_{2}$, it must also query $I_{3}$. In Figure 1(b) it is enough to query $I_{1}$, but the algorithm will query a dependent pair, say, $I_{1}$ and $I_{2}$. Either way, the algorithm does not spend more than twice the optimum number of queries. The following theorem was previously proven for $\delta=0$ by Kahan [27], which we generalize for arbitrary $\delta \geq 0$.


Figure 1: Example instances of the problem.

Theorem 11. The simple adaptive algorithm for sorting with uncertainty is 2-query-competitive for uniform query costs.

Proof. Note that the optimum solution must query at least one interval in each witness set. For every pair $\left\{I_{i}, I_{j}\right\}$ of dependent intervals selected by the algorithm, we have that: (1) if both $I_{i}$ and $I_{j}$ have not been queried yet, the algorithm queries the witness set $\left\{I_{i}, I_{j}\right\}$; (2) if $I_{i}$ has already been queried then, by Fact $5,\left\{I_{j}\right\}$ is a witness set, which is queried by the algorithm. We conclude that the algorithm only queries disjoint witness sets of size at most 2 , thus it queries at most twice the minimum number of intervals.

For arbitrary query costs, the problem also admits a 2 -query-competitive deterministic adaptive algorithm, although not as simple. The algorithm first queries a minimum-cost vertex cover $S_{1}$ on the dependency graph. Then, it queries all non-trivial intervals that are still dependent after querying $S_{1}$, which we denote by the set $S_{2}$.

Theorem 12. The adaptive algorithm for sorting with uncertainty with arbitrary query costs is 2-querycompetitive.

Proof. Let $Q$ be an optimum query set. The set of intervals not contained in $Q$ must be independent. By the duality between independent sets and vertex covers, $Q$ must be a vertex cover. Thus $w\left(S_{1}\right) \leq w(Q)$, since $S_{1}$ has minimum cost. Furthermore, note that every interval in $S_{2}$ is a singleton witness set, since $S_{2}$ is a set of independent intervals. Thus $w\left(S_{2}\right) \leq w(Q)$ as well, and $w\left(S_{1} \cup S_{2}\right) \leq 2 \cdot w(Q)$.

[^0]
[^0]:    ${ }^{3}$ If $\delta=0$, then this algorithm can be implemented with stable sorting and in $\mathrm{O}(n \lg n)$ time by running a standard stable sorting algorithm (e.g., MergeSort) and querying two intervals when MergeSort needs to know the relative order between them. It does not work, however, if $\delta>0$, since the relation $v_{i} \leq v_{j}+\delta$ is not transitive.# 5. Improved Adaptive Algorithms for Uniform Query Costs 

We now explore refined analysis of query-competitive sorting. We present a unified strategy that yields different improvements to Theorem 11, depending on what assumptions we make.

The core observation is that the bad 2 -interval instance in the proof of Lemma 9 is the only structure that prevents an algorithm from performing better than twice the optimum. The first strategy that comes to mind, then, is to use randomization: a simple randomized strategy attains query-competitive factor $3 / 2$ on the instance of Lemma 9. Before extending the algorithm to arbitrary instances, we give a lower bound for any randomized algorithm.

Lemma 13. Any randomized adaptive algorithm has query-competitive ratio at least $3 / 2$ against an adversary that is oblivious to the randomized tosses, even for uniform query costs.

Proof. Use the same bad instance as Lemma 9, set probability $1 / 2$ for each of the two possible inputs and apply Yao's minimax principle.

The algorithm is based on the following property of the dependency graph.
Lemma 14. If $I_{x}$ is a dependent interval with minimum $r_{x}$, then vertex $x$ is simplicial, i.e., its neighborhood is a clique.

Proof. The claim is trivial if $x$ has only one neighbor, so assume it has at least two, $y$ and $z$. Then $r_{y}-\ell_{z} \geq r_{x}-\ell_{z}>\delta$, since $I_{x}$ and $I_{z}$ are dependent. Analogously, $r_{z}-\ell_{y}>\delta$, so $I_{y}$ and $I_{z}$ are dependent.

The algorithm begins by querying intervals that are singleton witness sets according to a generalization of the condition in Fact 5. Then, if a component of the remaining dependency graph is an edge, the randomized strategy is applied. Else, the algorithm considers a non-isolated vertex $x$ with minimum $r_{x}$, a neighbor $y$ of $x$ with minimum $r_{y}$, and another neighbor $z$ of $x$ (or of $y$ if $y$ is the only neighbor of $x$ ) with minimum $r_{z}$. The algorithm first queries $I_{y}$. If $x$ and $y$ are still adjacent, or if $x$ and $z$ are adjacent, then we query both $I_{x}$ and $I_{z}$. We repeat this strategy until the dependency graph has no edges.

A pseudocode is presented in Algorithm 1; we parameterize the probability $p$ in the randomized strategy since the algorithm will be reused afterwards. We also maintain a set $\mathcal{V}$ of the values resulting of queried intervals.

Theorem 15. Algorithm 1 has expected query-competitive ratio $3 / 2$ if $p=1 / 2$.
Proof. We form a partition $V_{1}, \ldots, V_{m}$ of the set of input intervals with the following property. Let $a\left(V_{i}\right)$ be the number of intervals in $V_{i}$ that are queried by the algorithm, and let $q\left(V_{i}\right):=\left|Q \cap V_{i}\right|$, where $Q$ is an optimum query set. We show that $\mathbb{E}\left[a\left(V_{i}\right) / q\left(V_{i}\right)\right] \leq 3 / 2$ for every $i$, from which the theorem follows.

If the algorithm queries an interval $I_{i}$ in Line 3 or Line 18, then $\left\{I_{i}\right\}$ is the next set in the partition. Due to Fact 5 , it is a singleton witness set, so $a\left(\left\{I_{i}\right\}\right) / q\left(\left\{I_{i}\right\}\right)=1$.

If the algorithm runs Lines $6-9$ for edge $i j$, then $W=\left\{I_{i}, I_{j}\right\}$ is the next set in the partition. We consider the following cases.

1. If $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ and $I_{j} \supset\left[v_{i}-\delta, v_{i}+\delta\right]$, then $q(W)=2$ and $a(W)=2$.
2. Otherwise, $q(W) \geq 1$ because this is a witness pair.
(a) If $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ but $I_{j} \not \supset\left[v_{i}-\delta, v_{i}+\delta\right]$, then with probability $1 / 2$ the algorithm queries $I_{i}$ and this is enough, and with probability $1 / 2$ it queries both, so $\mathbb{E}[a(W)]=3 / 2$; the same holds for the symmetrical case.
(b) If $I_{i} \not \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ and $I_{j} \not \supset\left[v_{i}-\delta, v_{i}+\delta\right]$, then Line 9 is not executed and $a(W)=1$.

If the algorithm runs Lines $11-16$ for $x, y$ and $z$, then we have two cases.Input: $\left(I_{1}, \ldots, I_{n}, p\right)$
$1 \mathcal{V} \leftarrow \emptyset$;
2 while there are $i, j$ with $I_{i} \supset\left[\ell_{j}-\delta, r_{j}+\delta\right]$ or $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ with $v_{j} \in \mathcal{V}$ do query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
while there is some dependency do
if some component is an edge $i j$ then
pick $i$ with probability $p$ (and $j$ with probability $1-p$ ); assume $i$ is picked;
query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
if $I_{j} \supset\left[v_{i}-\delta, v_{i}+\delta\right]$ then
query $I_{j}$, add $v_{j}$ to $\mathcal{V}$;
else
let $I_{x}$ non-isolated with $\min r_{x}$, and $y$ be a neighbor of $x$ with $\min r_{y}$;
let $z$ be another neighbor of $x$ (or of $y$ if $x$ is a leaf), with $\min r_{z}$;
query $I_{y}$, add $v_{y}$ to $\mathcal{V}$;
if $I_{x} \supset\left[v_{y}-\delta, v_{y}+\delta\right]$ or $I_{x}, I_{z}$ are dependent then
query $I_{x}$, add $v_{x}$ to $\mathcal{V}$;
query $I_{z}$, add $v_{z}$ to $\mathcal{V}$;
while there is $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ for some $v_{j} \in \mathcal{V}$ do
query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
Algorithm 1: Improved adaptive algorithm for the sorting problem with queries.

1. If $x$ and $z$ are not neighbors, and $x$ and $y$ are not neighbors after Line 13, then $W=\left\{I_{x}, I_{y}\right\}$ is the next set in the partition. Since it is a witness set, $q(W) \geq 1$. But the algorithm will not query $I_{x}$ because $y$ is the only neighbor of $x$, so $a(W)=1$.
2. Otherwise, $W=\left\{I_{x}, I_{y}, I_{z}\right\}$ is the next set in the partition. We have two subcases.
(a) If $x$ and $z$ are neighbors, then $x y z$ is a clique by Lemma 14. So $q(W) \geq 2$, since otherwise a pair is unsolved.
(b) Otherwise, $I_{x} \supset\left[v_{y}-\delta, v_{y}+\delta\right]$ and $\left\{I_{x}\right\}$ is a singleton witness set. Since $x$ and $z$ are not neighbors, then $y$ and $z$ are neighbors and, by Lemma $2,\left\{I_{y}, I_{z}\right\}$ is a witness set.
Either way, $q(W) \geq 2$ and $a(W)=3$.
We conclude that the expected query-competitive ratio is $3 / 2$.
Our second strategy to obtain an improvement on Theorem 11 is, instead of using randomization, to assume that the graph does not have 2-components, i.e., components consisting of a single edge. This is not enough, however, since in Lemma 9 we have shown that we can have a large component. So our hypothesis is that $\delta=0$ and, after executing the loop of Lines $2-3$, the remaining dependency graph, which becomes a proper interval graph, has no 2-components. (Note that Theorem 15 is still true if we remove Lines $2-3$ of the algorithm.) Let us prove a lower bound for this case.

Lemma 16. Any deterministic adaptive algorithm has query-competitive ratio at least $5 / 3$, even if $\delta=0$ and the dependency graph is a proper interval graph with no 2-components.

Proof. Consider five proper intervals $I_{a}, I_{b}, I_{c}, I_{d}, I_{e}$ with $\ell_{a}<\ell_{b}<\ell_{c}<\ell_{d}<\ell_{e}$. The dependencies are defined by two triangles, $a b c$ and $c d e$.

If the algorithm first queries $I_{c}$, then we set $v_{c} \in I_{c} \backslash\left(I_{a} \cup I_{b} \cup I_{d} \cup I_{e}\right)$, and we can make $a b$ and $d e$ behave as the bad instance of Lemma 9.

If the algorithm first queries $I_{a}$, then we set $v_{a} \in I_{b} \cap I_{e}$, so the algorithm will be forced to query $I_{b}$ and $I_{c}$, and we set $v_{b}, v_{c} \in\left(I_{b} \cup I_{c}\right) \backslash\left(I_{a} \cup I_{d} \cup I_{e}\right)$, so the optimum can avoid $I_{a}$. Then we can make $d e$ behave as the bad instance of Lemma 9. The argument is symmetric if the algorithm first queries $I_{e}$.If the algorithm first queries $I_{b}$, then we set $v_{b} \in I_{a} \cap I_{c}$, so the algorithm will be forced to query $I_{a}$ and $I_{c}$, and we set $v_{a}, v_{c} \in\left(I_{a} \cup I_{c}\right) \backslash\left(I_{b} \cup I_{d} \cup I_{e}\right)$, so the optimum can avoid $I_{b}$. Then we can make $d e$ behave as the bad instance of Lemma 9 . The argument is symmetric if the algorithm first queries $I_{d}$.

Theorem 17. Algorithm 1 (with $p=0$ or 1 ) is $5 / 3$-query-competitive if $\delta=0$ and the dependency graph has no 2-components after finishing the loop of Lines 2-3.

Proof. The analysis is similar to that of Theorem 15. We will give a partition $V_{1}, \ldots, V_{m}$ of the set of intervals with the following property. Let $a\left(V_{i}\right)$ be the number of intervals in $V_{i}$ that are queried by the algorithm, and let $q\left(V_{i}\right):=\left|Q \cap V_{i}\right|$, where $Q$ is an optimum query set. We will have that $a\left(V_{i}\right) / q\left(V_{i}\right) \leq 5 / 3$ for every $i$, and then the theorem follows. The analysis for the cases of Lines 3, 11-16 and 18 are identical.

If the algorithm runs Lines 6-9 for edge $i j$, then let $C$ be the component containing $i j$ in the dependency graph after finishing the loop of Lines $2-3$. We claim that $i$ and $j$ are the only vertices of $C$ queried in Lines 6-9: Lines 11-16 force that intervals are queried from left to right; thus, since the dependency graph at this point is a proper interval graph, if an interval $i^{\prime}$ is queried in Line 18, then after that no interval $j^{\prime}$ with $r_{j^{\prime}}<r_{i^{\prime}}$ will have some dependency. Pick an arbitrary set $W^{\prime}$ of the partition consisting of vertices of $C$. We merge $\left\{I_{i}, I_{j}\right\}$ and $W^{\prime}$ into a single set $W$ of the partition, and from the previous cases we have that $a(W) / q(W) \leq 5 / 3$.

This proof indicates that the analysis can be improved if we require the graph to have large components after finishing the loop of Lines $2-3$.

Theorem 18. Algorithm 1 (with $p=0$ or 1 ) has query-competitive factor $3 / 2+\mathrm{O}(1 / k)$ if $\delta=0$ and each component of the dependency graph has size at least $k$ after finishing the loop of Lines 2-3.

Proof. We only have to reconsider the case of Lines 6-9 in the proof of Theorem 17. If the algorithm runs Lines $6-9$ for edge $i j$, then let $C$ be the component containing $i j$ in the dependency graph after finishing the loop of Lines $2-3$. We merge $\left\{I_{i}, I_{j}\right\}$ and all partition sets containing vertices of $C$ into a single set $W$ of the partition. Since $i$ and $j$ are the only vertices of $C$ queried in Lines $6-9$ and $C$ has size at least $k$, from the other cases we have that $a(W) / q(W) \leq 3 / 2+\mathrm{O}(1 / k)$.

The analysis is tight since we can have a chain of $k$ triangles plus 1 edge, such that we can force the algorithm to query all intervals, while the optimum can avoid one interval in each triangle and one interval in the extra edge. For large components, we still have a lower bound of $7 / 5$ for any deterministic algorithm.

Lemma 19. Any deterministic adaptive algorithm has query-competitive ratio at least $7 / 5$, even if $\delta=0$ and the dependency graph is a proper interval graph with large components.

Proof. Consider the graph of Figure 2, which has $7 k+2$ vertices. For $i=0, \ldots, k-1$, vertices $7 i+3, \ldots, 7 i+7$ consist in a copy of the instance of Lemma 16. For $i=0, \ldots, k$, vertices $x_{i}=7 i+1$ and $y_{i}=7 i+2$ are dependent, $x_{i}$ is dependent to $7(i-1)+7$ if $i>0$, and $y_{i}$ is dependent to $7 i+3$ if $i<k$. We set $v_{x_{i}}, v_{y_{i}} \in I_{x_{i}} \cap I_{y_{i}}$, so both the algorithm and the optimum must query $I_{x_{i}}$ and $I_{y_{i}}$, but querying them gives us no information about the remaining vertices. From Lemma 16, we can force any deterministic algorithm to query all vertices in the graph, while the optimum solution can query only 3 vertices of $7 i+3, \ldots, 7 i+7$.


Figure 2: Instance which attains the lower bound for proper interval graphs with large components.It remains an open question to close the gap between the lower bound of $7 / 5$ and the upper bound of $3 / 2+O(1 / k)$. Finally, we note that the problem can be solved exactly for laminar families of intervals ${ }^{4}$, since all queries will happen at Line 3 of the algorithm.

Theorem 20. Algorithm 1 obtains an optimum solution if $\delta=0$ and the intervals constitute a laminar family.

# 6. Randomized Adaptive Algorithms for Arbitrary Query Costs 

In this section, we improve over Theorem 12. We begin by showing that, when query costs are arbitrary, any deterministic algorithm has query-competitive ratio at least 2 , even for proper interval graphs with large components.

Lemma 21. Any deterministic adaptive algorithm for arbitrary query costs has query-competitive ratio at least 2 , even if $\delta=0$ and the dependency graph is a proper interval graph with large components.

Proof. Consider a path with $2 n$ intervals. The first two intervals have query cost 1 , and consist in the bad 2-component instance of Lemma 9 . The other $2 n-2$ intervals have query cost $0<\epsilon \ll 1 /(2 n)$, and we force them all to be in any solution by setting $v_{2 i+1} \in I_{2 i+2}$ and $v_{2 i+2} \in I_{2 i+1}$, for $i=1, \ldots, n-1$.

We can obtain an improvement, however, by using randomization. First, consider a 2-component with intervals $I_{a}$ and $I_{b}$. A simple strategy is to query $I_{b}$ first with probability $p=\min \left(1, \frac{w_{a}}{2 w_{b}}\right)$ (and $I_{a}$ with probability $1-p$ ), and then query the other interval if needed. If the optimum solution queries both $I_{a}$ and $I_{b}$, then the algorithm is optimal. If $w_{a} \geq 2 w_{b}$ then the algorithm is deterministic but clearly pays at most $3 / 2$ times the optimum. Else, if the optimum solution is to query only $I_{a}$, then the algorithm pays at most

$$
\left(1-\frac{w_{a}}{2 w_{b}}\right) \cdot w_{a}+\frac{w_{a}}{2 w_{b}} \cdot\left(w_{b}+w_{a}\right)=\frac{3}{2} w_{a}
$$

If the optimum solution is to query only $I_{b}$, then the algorithm pays at most

$$
\begin{aligned}
\frac{w_{a}}{2 w_{b}} \cdot w_{b}+\left(1-\frac{w_{a}}{2 w_{b}}\right) \cdot\left(w_{a}+w_{b}\right) & =w_{b} \cdot\left(1+\frac{w_{a}}{w_{b}} \cdot\left(1-\frac{w_{a}}{2 w_{b}}\right)\right) \\
& =w_{b} \cdot\left(1+\frac{w_{a}}{w_{b}} \cdot \frac{w_{b}^{2}-\left(w_{a}-w_{b}\right)^{2}}{2 w_{a} w_{b}}\right) \\
& \leq w_{b} \cdot\left(1+\frac{w_{a}}{w_{b}} \cdot \frac{w_{b}}{2 w_{a}}\right)=\frac{3}{2} \cdot w_{b}
\end{aligned}
$$

where the inequality follows from the fact that $\left(w_{a}-w_{b}\right)^{2} \geq 0$. Note that this strategy is optimal due to Lemma 13. (The same result works if we pick $I_{a}$ with probability $p^{\prime}=\min \left(1, \frac{w_{b}}{2 w_{a}}\right)$ and $I_{b}$ with probability $1-p^{\prime}$, but we will favor $I_{b}$ to simplify the discussion hereon.)

Now let us discuss how to obtain a general strategy for arbitrary graphs. The first key ingredient is the Local Ratio Theorem, which we state below (see, e.g., [3] for a proof).

Theorem 22 (Local Ratio). Consider a minimization problem whose objective function is a linear combination of the solution vector. Let $w, w^{(1)}, w^{(2)} \in \mathbb{R}^{n}$ be cost vectors such that $w=w^{(1)}+w^{(2)}$. Let $x \in \mathbb{R}^{n}$ be a feasible solution, and let $x^{*}, x^{*(1)}, x^{*(2)} \in \mathbb{R}^{n}$ be optimum solutions for costs $w, w^{(1)}, w^{(2)}$, respectively. If, for some constant $\alpha>0, w^{(1)} x \leq \alpha w^{(1)} x^{*(1)}$ and $w^{(2)} x \leq \alpha w^{(2)} x^{*(2)}$, then $w x \leq \alpha w x^{*}$.

[^0]
[^0]:    ${ }^{4} \mathrm{~A}$ set of intervals $\left\{I_{1}, \ldots, I_{n}\right\}$ is a laminar family if, for every $I_{i}, I_{j}$ with $I_{i} \cap I_{j} \neq \emptyset$, we have that either $I_{i} \subset I_{j}$ or $I_{j} \subset I_{i}$.We use this result to eliminate triangles in the dependency graph. We begin by querying intervals with zero query cost, since they do not change the solution cost. Then we consider an arbitrary triangle $a b c$ and let $i$ denote the interval in $\{a, b, c\}$ of minimum query cost. We set $w_{j} \leftarrow w_{j}-w_{i}$ for $j \in\{a, b, c\}$, which by the Local Ratio theorem makes progress towards a $3 / 2$-approximation.

After breaking all triangles, the dependency graph becomes a forest, since it is a chordal graph. In fact, it becomes a caterpillar (where all vertices are at distance at most 1 from a longest path), since co-TT graphs cannot contain the graph of Figure 4(b) as an induced subgraph [32]. We identify a longest path in a component (which can be done in polynomial time for trees) and let $a, b, c$ be its first three nodes. We focus on the node $b$ and the set $N_{1}(b)$ of the neighbors of $b$ excluding $c$. Since the graph is a caterpillar, note that $b$ is the only neighbor of the vertices in $N_{1}(b)$. Let $w\left(N_{1}(b)\right)=\sum_{i \in N_{1}(b)} w_{i}$. We query $I_{b}$ with probability $p=\min \left(1, \frac{w\left(N_{1}(b)\right)}{2 w_{b}}\right)$, and we query all intervals in $N_{1}(b)$ with probability $1-p$.

We repeat this strategy and query singleton witness sets until the dependency graph has no edges. A pseudocode is presented in Algorithm 2. Note that, after all triangles are broken, the longest path of each component can be computed just once; we enforce this, so that we can assume that the longest path is consistent between executions of Lines 10-14 in the same component.

Input: $\left(I_{1}, \ldots, I_{n}, w\right)$
$1 \mathcal{V} \leftarrow \emptyset$;
2 while there is some dependency do
if some $I_{i}$ has $w_{i}=0$ then
query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
else if there is a triangle $a b c$ then
let $i=\arg \min _{i \in\{a, b, c\}}\left\{w_{i}\right\}$;
foreach $j \in\{a, b, c\}$ do
$w_{j} \leftarrow w_{j}-w_{i} ;$
else
let $P=a b c \cdots$ be a longest path in a component;
with probability $p=\min \left(1, \frac{w\left(N_{1}(b)\right)}{2 w_{b}}\right)$ do
query $I_{b}$, add $v_{b}$ to $\mathcal{V}$;
else foreach $j \in N_{1}(b)$ do
query $I_{j}$, add $v_{j}$ to $\mathcal{V}$;
while there is $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ for some $v_{j} \in \mathcal{V}$ do
query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
Algorithm 2: Randomized adaptive algorithm for arbitrary query costs.

Theorem 23. Algorithm 2 has query-competitive ratio $57 / 32=1.78125$ for arbitrary query costs.
Proof. If an interval is queried at Line 4, then it does not change the solution cost. Also, recall that, due to Fact 5 , every interval queried in Line 16 is in the optimum solution.

If the dependency graph contains triangles, then we do an induction. The base case is when all intervals have zero cost, which has a trivial optimum solution, or when the dependency graph is a forest, for which we will prove later that the algorithm has factor $57 / 32$. If the algorithm considers a triangle $a b c$ in Lines 6 8 , then we apply the Local Ratio Theorem with $w^{(1)}(j)=\left\{\begin{array}{ll}w_{i}, & \text { if } j \in\{a, b, c\} \\ 0, & \text { otherwise }\end{array}\right.$ and $w^{(2)}=w-w^{(1)}$, where $w$ is the cost vector before Line 7. By induction hypothesis, the solution returned by the algorithm has query-competitive factor $57 / 32$ for $w^{(2)}$. The solution returned by the algorithm in the worst case queries all intervals in $a b c$; since in any triangle at least two intervals must be queried, the solution costs at most $3 / 2$ times the optimum for $w^{(1)}$. By the Local Ratio Theorem, the algorithm has query-competitive factor$57 / 32$ for $w$. In the following, we focus on the case where each component of the dependency graph is a tree (specifically, a caterpillar).

Let us define some terminology. A random trial is an execution of Lines 11-14 of the algorithm. We say that a random trial involves interval $I_{i}$, and $I_{i}$ is involved in the random trial, if $i \in N_{1}(b) \cup\{b\}$. An interval $I_{i}$ is queried in a random trial if it is queried in an execution of Line 12 or 14 which is part of that random trial. Note that some intervals involved in a random trial may not be queried in the random trial, but may be queried later on, either in another random trial or in Line 16.

We bound the cost of the returned solution by assigning a cost share to each interval in the optimum solution. Intervals that are queried and are in the optimum solution have their cost assigned to themselves. Intervals that are queried and are not in the optimum solution have their cost assigned to their neighbors in the optimum solution. However, we only assign a query cost $w_{j}$ to an interval $I_{i}$ with $i \neq j$ if $I_{j}$ is queried in a random trial involving $I_{i}$. (Note that intervals that are not in the optimum solution can only be queried in random trials.) Also, if $I_{j}$ has more than one neighbor, its cost is shared among its neighbors in proportion to their cost.

Let us bound the cost share for each interval in the optimum solution. Note that we do not have to care about intervals that are not involved in random trials, since they are not assigned extra cost. Let $P$ and $b$ be as defined in Line 10. Note that, if the optimum solution does not query $I_{b}$, then it must query all neighbors of $b$. If $w\left(N_{1}(b)\right) \geq 2 w_{b}$, then the algorithm deterministically queries $I_{b}$ first, and spends at most $3 / 2$ times the portion of the optimum solution contained in $N_{1}(b) \cup\{b\}$. If $w\left(N_{1}(b)\right)<2 w_{b}$, then we consider two cases.

1. If the optimum solution does not query $I_{b}$, then it queries all of $N_{1}(b)$. The expected total cost share for intervals in $N_{1}(b)$ will be at most

$$
\left(1-\frac{w\left(N_{1}(b)\right)}{2 w_{b}}\right) \cdot w\left(N_{1}(b)\right)+\frac{w\left(N_{1}(b)\right)}{2 w_{b}} \cdot\left(w_{b}+w\left(N_{1}(b)\right)\right)=\frac{3}{2} w\left(N_{1}(b)\right)
$$

Remember that we split this cost among the intervals in $N_{1}(b)$ in proportion to their cost, so the expected cost share for $j \in N_{1}(b)$ will be at most $\frac{3}{2} w_{j}$. Note that we do not assign any cost share to $I_{b}$, since it is not in the optimum solution. Also, if $I_{b}$ is not queried in this trial but is queried later on, its cost will not be assigned to $N_{1}(b)$.
2. If the optimum solution queries $I_{b}$, then the algorithm may not query $I_{b}$ in the trial between $b$ and $N_{1}(b)$, but in this case $I_{b}$ will be involved in the trial between $c$ and the neighbors of $c$, and in no further trials. All intervals queried in the first trial that are not in the optimum solution will have their cost assigned to $I_{b}$. For the second trial, from the previous case argument, the expected cost share of $I_{b}$ is at most $\frac{3}{2} w_{b}$ (or at most $w_{b}$ if the optimum solution also queries $I_{c}$ ). Therefore, the total expected cost share of $I_{b}$ for both trials will be at most

$$
\begin{aligned}
& \frac{w\left(N_{1}(b)\right)}{2 w_{b}} \cdot w_{b}+\left(1-\frac{w\left(N_{1}(b)\right)}{2 w_{b}}\right)\left(w\left(N_{1}(b)\right)+\frac{3}{2} w_{b}\right) \\
= & \frac{w_{b} w\left(N_{1}(b)\right)}{2 w_{b}}+w\left(N_{1}(b)\right)+\frac{3}{2} w_{b}-\frac{w^{2}\left(N_{1}(b)\right)}{2 w_{b}}-\frac{3 w\left(N_{1}(b)\right)}{4} \\
= & \frac{6 w_{b}^{2}+3 w_{b} w\left(N_{1}(b)\right)-2 w^{2}\left(N_{1}(b)\right)}{4 w_{b}} \cdot \frac{8}{8} \\
= & \frac{57 w_{b}^{2}-9 w_{b}^{2}+24 w_{b} w\left(N_{1}(b)\right)-16 w^{2}\left(N_{1}(b)\right)}{32 w_{b}} \leq \frac{57}{32} w_{b}
\end{aligned}
$$

where the inequality uses the fact that $\left(3 w_{b}-4 w\left(N_{1}(b)\right)\right)^{2} \geq 0$. The expected cost share for $j \in N_{1}(b)$ such that $I_{j}$ is the optimum solution will be at most $w_{j}$.

The analysis is tight if we consider a path with three intervals $I_{a}, I_{b}, I_{c}$ with $\delta=0, w_{a}=1, w_{b}=w_{c}=4 / 3$, $v_{a} \in I_{a} \backslash I_{b}, v_{b} \in I_{b} \backslash\left(I_{a} \cup I_{c}\right)$ and $v_{c} \in I_{b}$. Note, however, that this does not give us an improved lower bound,since for paths of length 3 we can use a similar strategy as that for 2-components, doing a randomized trial that considers querying either $I_{b}$, or $I_{a}$ and $I_{c}$.

However, the analysis indicates us that there is room for improvement if we change the probabilities a little bit. The probabilities were chosen to guarantee that the algorithm performs well for a 2-component, no matter if $I_{a}$ or $I_{b}$ is the best option. The example in the previous paragraph shows that we cannot improve the bound when $I_{b}$ is the best option, but we may use a higher probability for $I_{b}$, thus incurring some loss in the bound when $I_{a}$ is the best option, but improving the bound when $I_{b}$ is the best option. We claim that the ratio is minimum when $p=\min \left(1, \frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}}\right)$, in which case the query-competitive factor is improved to $1+\frac{4}{3 \sqrt{3}} \approx 1.7698$.

Theorem 24. Algorithm 2 has query-competitive ratio $1+\frac{4}{3 \sqrt{3}}$ for arbitrary query costs if we replace $p=$ $\min \left(1, \frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}}\right)$ in Line 11.

Proof. We only need to reanalyze the case when the dependency graph is a caterpillar. Let $P$ and $b$ be as defined in Line 10. If $w\left(N_{1}(b)\right) \geq w_{b} \sqrt{3}$, then the algorithm deterministically queries $I_{b}$ first, and spends at most $1+\frac{1}{\sqrt{3}} \approx 1.578$ times the portion of the optimum solution contained in $N_{1}(b) \cup\{b\}$. If $w\left(N_{1}(b)\right)<w_{b} \sqrt{3}$, then we consider two cases.

1. If the optimum solution does not query $I_{b}$, then the expected total cost share for intervals in $N_{1}(b)$ will be at most

$$
\left(1-\frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}}\right) \cdot w\left(N_{1}(b)\right)+\frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}} \cdot\left(w_{b}+w\left(N_{1}(b)\right)\right)=\left(1+\frac{1}{\sqrt{3}}\right) \cdot w\left(N_{1}(b)\right)
$$

2. If the optimum solution queries $I_{b}$, then the total expected cost share of $I_{b}$ for both trials will be at most

$$
\begin{aligned}
& \frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}} \cdot w_{b}+\left(1-\frac{w\left(N_{1}(b)\right)}{w_{b} \sqrt{3}}\right)\left(w\left(N_{1}(b)\right)+\left(1+\frac{1}{\sqrt{3}}\right) \cdot w_{b}\right) \\
= & \frac{(3+\sqrt{3}) w_{b}^{2}+2 w_{b} w\left(N_{1}(b)\right)-\sqrt{3} w^{2}\left(N_{1}(b)\right)}{3 w_{b}} \cdot \frac{\sqrt{3}}{\sqrt{3}} \\
= & \frac{(4+3 \sqrt{3}) w_{b}^{2}-w_{b}^{2}+2 \sqrt{3} w_{b} w\left(N_{1}(b)\right)-3 w^{2}\left(N_{1}(b)\right)}{3 \sqrt{3} w_{b}} \leq\left(1+\frac{4}{3 \sqrt{3}}\right) \cdot w_{b}
\end{aligned}
$$

where the inequality uses the fact that $\left(w_{b}-\sqrt{3} w\left(N_{1}(b)\right)\right)^{2} \geq 0$, and the expected cost share for $j \in N_{1}(b)$ such that $I_{j}$ is the optimum solution will be at most $w_{j}$.

The analysis is tight if we consider a path with three intervals $I_{a}, I_{b}, I_{c}$ with $\delta=0, w_{a}=1, w_{b}=w_{c}=\sqrt{3}$, $v_{a} \in I_{a} \backslash I_{b}, v_{b} \in I_{b} \backslash\left(I_{a} \cup I_{c}\right)$ and $v_{c} \in I_{b}$.

Now let us prove that this is the best possible factor that can be obtained for this framework. We are using a probability of the form $p=\min \left(1, \frac{w_{a}}{w_{b}}(\alpha-1)\right)$. When the optimal choice is to query $I_{a}$, we obtain a guarantee in the form

$$
\frac{\left(1-\frac{w_{a}}{w_{b}}(\alpha-1)\right) \cdot w_{a}+\frac{w_{a}}{w_{b}}(\alpha-1) \cdot\left(w_{b}+w_{a}\right)}{w_{a}}=\alpha
$$

When the optimal choice is to query $I_{b}$, this gives us a guarantee of

$$
\frac{\frac{w_{a}}{w_{b}}(\alpha-1) \cdot w_{b}+\left(1-\frac{w_{a}}{w_{b}}(\alpha-1)\right)\left(w_{a}+\alpha \cdot w_{b}\right)}{w_{b}}
$$Let us define this latter guarantee as a function

$$
\gamma(x, y)=\frac{\frac{x}{y}(\alpha-1) \cdot y+\left(1-\frac{x}{y}(\alpha-1)\right)(x+\alpha y)}{y}=\frac{x^{2}}{y^{2}}(1-\alpha)+\frac{x}{y} \cdot \alpha(2-\alpha)+\alpha
$$

We want to find $1<\alpha<2$ that minimizes the maximum of $\gamma(x, y)$ for all $x, y>0$. Since $y>0$, we have a critical point when

$$
\left\{\begin{array}{l}
\frac{\partial \gamma}{\partial x}=\frac{2 x}{y^{2}}(1-\alpha)+\frac{\alpha(2-\alpha)}{y}=0 \\
\frac{\partial \gamma}{\partial y}=\frac{2 x^{2}}{y^{3}}(\alpha-1)+\frac{x \alpha(\alpha-1)}{y^{2}}=0
\end{array} \Rightarrow 2 x(1-\alpha)+y \alpha(2-\alpha)=0 \Rightarrow \frac{x}{y}=\frac{\alpha(2-\alpha)}{2(\alpha-1)}\right.
$$

In that case, we have that

$$
\gamma(\alpha)=\left(\frac{\alpha(2-\alpha)}{2(\alpha-1)}\right)^{2} \cdot(1-\alpha)+\left(\frac{\alpha(2-\alpha)}{2(\alpha-1)}\right) \cdot \alpha(2-\alpha)+\alpha=\frac{\alpha^{2}(2-\alpha)^{2}}{4(\alpha-1)}+\alpha
$$

whose critical points have

$$
\frac{d \gamma}{d \alpha}=\frac{\left(\alpha^{2}-2 \alpha+2\right)\left(3 \alpha^{2}-6 \alpha+2\right)}{4(\alpha-1)^{2}}=0
$$

so $\alpha=1-\frac{1}{\sqrt{3}}$ or $\alpha=1+\frac{1}{\sqrt{3}}$. Since we are looking for $1<\alpha<2$, we stick with the latter.

# 7. Adaptive Algorithms with Queries Returning Intervals 

In this section, we investigate a variant of the problem in which a query may not necessarily return the exact value, but instead may return a more refined interval. We do not make any assumption on how more refined the queried interval is; even though this seems too arbitrary, the adversary also has to deal with the same uncertainty, so we can indeed devise competitive algorithms. We may even allow a query to return the same interval as before, since this is equivalent to having a very small improvement. We must assume, however, that the relative order between two dependent intervals can be solved after a finite number of queries, which may yield an output size that is super-polynomial in the input size. This model was proposed in [25]. For this problem, we assume what they call the CP-CP model, in which the input consists of closed intervals and points, and a query can return closed intervals and points as well; in [25] it was shown that this assumption is no more restricted than if we also allow open intervals.

This problem has a lower bound of 2 on the query-competitive ratio, even for randomized algorithms. This is because in [31, Section 9] it was proven that, if queries are allowed to return intervals, then any randomized algorithm that decides the relative order between two intervals has expected query-competitive ratio at least 2 . We show that this lower bound applies even if query costs are uniform and the dependency graph is a proper interval graph with large components. We present the result for deterministic algorithms, but it can be adapted for randomized algorithms using the same idea.

Lemma 25. Any deterministic adaptive algorithm for the $C P-C P$ model has query-competitive ratio at least 2 , even if $\delta=0$ and the dependency graph is a proper interval graph with large components.

Proof. Consider a path with $2 n$ intervals with uniform query cost. The first $2 n-2$ intervals turn into a point after the first query, and we force them all to be in any solution by setting $v_{2 i-1} \in I_{2 i}$ and $v_{2 i} \in I_{2 i-1}$, for $i=1, \ldots, n-1$. For the last two intervals, we claim that we can make the optimum solution solve this pair by performing $M$ queries, while any deterministic algorithm has to perform $2 M$ queries, for any integer $M>0$, so we can make the query-competitive ratio approach 2 if $M \gg n$.

The argument is similar to the tight example in [25, Section 5.1]. Suppose that the algorithm has already done $2 M-1$ queries. For the first $M-1$ queries on $I_{2 n-1}$, we obtain the same interval; the same appliesto $I_{2 n}$. Then we have two cases, depending on whether the algorithm makes $M$ queries on $I_{2 n-1}$ or on $I_{2 n}$. If the algorithm makes at least $M$ queries on $I_{2 n-1}$, then we return the same interval for all subsequent queries on $I_{2 n-1}$, and we return $v_{2 n} \in I_{2 n} \backslash I_{2 n-1}$ in the $M$-th query on $I_{2 n}$. Thus the algorithm has to make $2 M$ queries, and the optimum solution can simply query $M$ times $I_{2 n}$. The argument is symmetric if the algorithm makes at least $M$ queries on $I_{2 n}$.

Now we give a 2 -query-competitive deterministic algorithm for this version of the problem. We also cover the case in which query costs change over time, i.e., we assume that querying interval $I_{i}$ for the $t$-th time costs $w_{i}(t) \in \mathbb{R}$, for $t=1,2, \ldots$. The algorithm is a simple modification of the local ratio 2 -approximation algorithm for the vertex cover problem [4]. It is also a generalization of both algorithms in Section 4. We begin by querying intervals whose current query cost is zero, since this does not affect the solution cost. If there is some dependency between vertices $I_{i}$ and $I_{j}$, then we subtract from their current query cost the minimum of them; this will force one of them to be queried. We query intervals that are singleton witness sets according to Fact 5, and proceed until all dependencies are resolved. A pseudocode is presented in Algorithm 3.

```
Input: \(\left(I_{1}, \ldots, I_{n}, w\right)\)
for \(i \leftarrow 1\) to \(n\) do
    \(t_{i} \leftarrow 1\)
while there is some dependency do
    if some \(I_{i}\) has \(w_{i}\left(t_{i}\right)=0\) then
        query \(I_{i}\)
        \(t_{i} \leftarrow t_{i}+1\)
    else
        let \(I_{i}\) and \(I_{j}\) be two dependent intervals;
        \(W \leftarrow \min \left\{w_{j}\left(t_{j}\right), w_{i}\left(t_{i}\right)\right\}\)
        \(w_{i}\left(t_{i}\right) \leftarrow w_{i}\left(t_{i}\right)-W\)
        \(w_{j}\left(t_{j}\right) \leftarrow w_{j}\left(t_{j}\right)-W\)
    while there are \(i, j\) with \(I_{i} \supset\left[\ell_{j}-\delta, r_{j}+\delta\right]\) do
        query \(I_{i}\)
        \(t_{i} \leftarrow t_{i}+1\)
```

Algorithm 3: Adaptive algorithm for queries returning intervals.

Theorem 26. Algorithm 3 is 2-query-competitive for the sorting problem with uncertainty in the $C P-C P$ model, even if query costs change over time.

Proof. The proof is by induction and relies on the Local Ratio Theorem (Theorem 22). When we query an interval with zero query cost in Line 5, it does not affect the solution cost. Intervals that are queried in Line 13 must be in any solution, due to Fact 5 . If we run Lines $8-11$ for intervals $I_{i}$ and $I_{j}$ for given $t_{i}$ and $t_{j}$, we apply the Local Ratio Theorem with $w_{i}^{(1)}\left(t_{i}\right)=w_{j}^{(1)}\left(t_{j}\right)=W, w_{k}^{(1)}\left(t^{\prime}\right)=0$ for $\left(k, t^{\prime}\right) \notin\left\{\left(i, t_{i}\right),\left(j, t_{j}\right)\right\}$, and $w^{(2)}=w-w^{(1)}$. By induction hypothesis, the returned solution is 2 -query-competitive on $w^{(2)}$. The pair $i j$ is not resolved before we make $t_{i}$ queries in $I_{i}$ and $t_{j}$ queries in $I_{j}$, and due to Lemma 2 we must query at least one of them. In the worst case the algorithm will query both of them, so the returned solution is also 2 -query-competitive on $w^{(1)}$ and, by the Local Ratio Theorem, it is 2 -query-competitive on $w$.

This algorithm also works for the following generalization of the vertex cover problem: suppose we have an arbitrary graph, and we want to resolve all the edges. Querying a vertex may resolve an incident edge, but for some edges it may be necessary to query both endpoints. We do not know this information, so we have an online problem. The algorithm works even if vertex weights are not uniform, and if a vertex may be required to be queried multiple times before the edge is resolved.It is interesting to note that we obtain a factor of 2 , despite the fact that there are instances for which the 2 -approximation guarantee for the vertex cover problem is tight [3], and that in the proof of Theorem 12 we argue that we pay the cost of a minimum vertex cover plus the cost of the remaining dependent intervals. This indicates that in some cases the minimum vertex cover is a loose lower bound to the optimum solution.

# 8. Advice Complexity for Adaptive Algorithms 

In this section we investigate the advice complexity of solving the adaptive version of the problem. Recall that the advice complexity is the number of bits of advice from an oracle that are sufficient and necessary for an online algorithm to solve the problem exactly. We assume arbitrary query costs, and for consistency that the oracle answers questions regarding a fixed optimum solution for the given instance.

First, we deal with the case when $\delta=0$. Let $n$ be the number of given intervals. We claim that $\lfloor n / 2\rfloor$ bits of advice are sufficient to solve the problem exactly, and that there are instances for which $\lfloor n / 2\rfloor$ bits are necessary.

Lemma 27. The advice complexity of the adaptive sorting problem with uncertainty is at least $\lfloor n / 2\rfloor$, where $n$ is the number of intervals, even if $\delta=0$.

Proof. Assume $n$ even and consider $n / 2$ independent copies of the bad instance of Lemma 9. At least 1 bit of advice is necessary to decide the relative order between each pair.

For an adaptive algorithm with a matching upper bound, we note that, if $\delta=0$, then any triangle $i j k$ contains a vertex $j$ such that $I_{j} \subseteq I_{i} \cup I_{k}$ (just take $i$ with minimum $\ell_{i}$ and $k$ with maximum $r_{k}$ ). Thus, we can ask the oracle whether the optimum solution queries $I_{j}$; if not, then we must query all neighbors of $j$; otherwise, we query $I_{j}$, and since $I_{j} \subseteq I_{i} \cup I_{k}$, we will know at least one of $I_{i}$ and $I_{k}$ that also must be queried. If the dependency graph contains no triangles, then it is a forest, because any cycle in a chordal graph must contain a triangle. Therefore, we can pick a leaf $i$ and ask the oracle whether the optimum solution queries its neighbor $j$; if not, then we query all neighbors of $j$; otherwise, we query $I_{j}$ and we will know whether $I_{i}$ must be queried or not. Since we decide at least two intervals with one bit of advice, then $\lfloor n / 2\rfloor$ bits are sufficient. We present a pseudocode in Algorithm 4.

Input: $\left(I_{1}, \ldots, I_{n}\right)$
$1 \mathcal{V} \leftarrow \emptyset$;
2 while there is some dependency do
3 if there is a triangle $K$ then
$4 \quad$ let $i \in K$ with minimum $\ell_{i}, k \in K$ with maximum $r_{k}$, and $j \in K \backslash\{i, k\}$;
5 else let $i$ be a leaf, and $j$ be the neighbor of $i$;
6 ask the oracle whether the optimum solution queries $j$;
7 if yes then query $I_{j}$, add $v_{j}$ to $\mathcal{V}$;
8 else foreach neighbor $z$ of $j$ do
9 query $I_{z}$, add $v_{z}$ to $\mathcal{V}$;
10 while there is $I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]$ for some $v_{j} \in \mathcal{V}$ do
11 query $I_{i}$, add $v_{i}$ to $\mathcal{V}$;
Algorithm 4: An adaptive algorithm that finds an optimum solution with $\lfloor n / 2\rfloor$ bits of advice when $\delta=0$.

Theorem 28. The advice complexity of the adaptive sorting problem with uncertainty is $\lfloor n / 2\rfloor$ when $\delta=0$, where $n$ is the number of intervals.

Now we consider the case when $\delta>0$. Here, we can improve the lower bound to $\lceil n / 3 \cdot \lg 3\rceil$ and still have an algorithm with matching upper bound. Both are based on the fact that to encode $k$ distinct values amortized $\lg k$ bits are sufficient and necessary [36].Lemma 29. The advice complexity of the adaptive sorting problem with uncertainty is at least $\lceil n / 3 \cdot \lg 3\rceil$, where $n$ is the number of intervals.
Proof. Assume $n$ multiple of 3 and consider $n / 3$ independent triangles; it suffices to bound the number of bits of advice necessary to solve each triangle. Suppose by contradiction that there is an algorithm that solves any triangle with one bit of advice, and consider the following instances $\mathcal{I}_{1}, \mathcal{I}_{2}, \mathcal{I}_{3}$. In each $\mathcal{I}_{i}$, the $k$-th triangle has intervals $I_{1}, I_{2}, I_{3}$ such that $\ell_{1}<\ell_{2}<\ell_{3}-\delta, r_{1}+\delta<r_{2}<r_{3}, \ell_{2} \leq \ell_{1}+\delta, r_{2} \geq r_{3}-\delta$ and $r_{1}-\ell_{3}>2 \delta$. We have $v_{j}=r_{j}$ for all $j$ in $\mathcal{I}_{1}$; in $\mathcal{I}_{2}, v_{1}=\ell_{1}, v_{2} \in\left(\ell_{3}+\delta, r_{1}-\delta\right), v_{3}=r_{3}$; and $v_{j}=\ell_{j}$ for all $j$ in $\mathcal{I}_{3}$. The only optimum solution for $\mathcal{I}_{1}, \mathcal{I}_{2}, \mathcal{I}_{3}$ is not to query $I_{1}, I_{2}, I_{3}$, respectively. (See Figure 3.)


Figure 3: Instances for the lower bound on advice complexity when $\delta>0$.
By the pigeonhole principle, the algorithm must have the same advice for at least two of those inputs. So, it suffices to prove that any deterministic algorithm fails in one instance of any subset with at least two of those instances. Since the intervals are structurally identical, any algorithm for a triangle performs no better than an algorithm in the following form, for fixed $x, y \in\{1,2,3\}, x \neq y$ : query $I_{x}$, and if no helpful information is given, query $I_{y}$. The instances are constructed in such a way that, for instance $\mathcal{I}_{i}$, the algorithm does not get any helpful information by querying $I_{x}$ with $i \neq x$, so it fails on instances $\mathcal{I}_{x}$ and $\mathcal{I}_{y}$. Since one bit is not sufficient, at least three different values must be encoded in the advice for each triangle, so $\lceil n / 3 \cdot \lg 3\rceil$ bits are necessary for the whole instance.

The algorithm that attains the upper bound relies on Lemma 14. It considers the clique $K$ consisting of a non-isolated vertex $x$ with minimum $r_{x}$ and its neighborhood. Then it asks the oracle for the index of a vertex $y$ in $K$ that is not queried in the optimum solution or, if there is no such vertex in $K$, then the oracle must return $y=x$. Either way, the algorithm queries all intervals in $K \backslash\{y\}$, and if $y=x$ then the algorithm will know if $y$ must also be queried after querying everyone else. So it uses $\lg |K|$ bits of advice to decide at least $|K|$ intervals, and the bound follows since $\lg k / k$ has its maximum at $k=3$ when $k$ is integer. A pseudocode is presented in Algorithm 5.

```
Input: \(\left(I_{1}, \ldots, I_{n}\right)\)
\(1 \mathcal{V} \leftarrow \emptyset\);
2 while there is some dependency do
3 let \(x\) non-isolated with minimum \(r_{x}\), and \(K\) be the clique consisting of \(x\) and its neighborhood;
4 ask the oracle for a vertex \(y \in K\) not queried in the optimum solution, or \(y=x\) if there is no
    such vertex;
5 foreach \(z \in K \backslash\{y\}\) do
6 \(\mid\) query \(I_{z}\), add \(v_{z}\) to \(\mathcal{V}\);
7 while there is \(I_{i} \supset\left[v_{j}-\delta, v_{j}+\delta\right]\) for some \(v_{j} \in \mathcal{V}\) do
8 \(\mid\) query \(I_{i}\), add \(v_{i}\) to \(\mathcal{V}\);
```

Algorithm 5: An adaptive algorithm that finds an optimum solution with $\lceil n / 3 \cdot \lg 3\rceil$ bits of advice.

Theorem 30. The advice complexity of the adaptive sorting problem with uncertainty is $\lceil n / 3 \cdot \lg 3\rceil$, where $n$ is the number of intervals.

# 9. Towards Characterizing co-TT Graphs by Forbidden Induced Subgraphs 

In this section we discuss the importance of our sorting problem with uncertainty for the understanding of the class of co-TT graphs itself. A first point is that we are not aware of other applications of this graph classin the literature. Second, there are various characterizations of co-TT graphs in the literature [32, 24], and they can be recognized in $\mathrm{O}\left(n^{2}\right)$ time [22], but a characterization in terms of forbidden induced subgraphs is an open question.

Before we figured out that Definition 1 is equivalent to that of co-TT graphs, we spent some time trying to understand the graph class we were dealing with. Since interval graphs are an obvious subclass, and a nice characterization by forbidden induced subgraphs is known for interval graphs [29], our first direction was to test which of these graphs are also forbidden induced subgraphs of co-TT graphs. It turns out we got to extend the partial list of forbidden induced subgraphs to the one presented in Figure 4.


Figure 4: A partial list of forbidden induced subgraphs for co-TT graphs. (a) is the $k$-cycle, for $k \geq 4$. (d) is the 3 -sun.
Figure 4(a) ( $k$-cycle, $k \geq 4$ ) is inherited from chordal graphs. Figures 4(b) and 4(c) have long been known not to be co-TT [23], and Figure 4(d) (the 3 -sun) is proved not to be co-TT in [10]. We prove that the graph in Figure 4(e), which cannot be an interval graph, cannot be a co-TT graph either. The following facts will be useful; similar or equivalent facts have been known for the red/blue characterization of co-TT graphs $[24]$.

Fact 31. Two trivial intervals cannot be dependent.
Fact 32. A trivial interval is always simplicial, i.e., cannot be dependent of two independent intervals.
Proof. By Fact 31, the neighbors of a trivial interval are non-trivial. Let $I_{i}$ and $I_{j}$ be two non-trivial intervals, which are independent, and let $I_{k}$ be a trivial interval. Assume without loss of generality that $r_{i}-\ell_{j} \leq \delta$, and suppose by contradiction that $I_{k}$ is dependent of both $I_{i}$ and $I_{j}$. We have that $r_{k}-\ell_{j}>\delta$ and, since $I_{k}$ is trivial, $r_{k}-\ell_{k} \leq \delta$. Thus, $r_{i} \leq \ell_{j}+\delta<r_{k} \leq \ell_{k}+\delta$, which contradicts the fact that $I_{i}$ and $I_{k}$ are dependent.

Fact 33. Let $I_{i}$ and $I_{j}$ be two intervals such that $I_{i} \supseteq I_{j}$. If $I_{j}$ is dependent to some interval $I_{k}$, then $I_{i}$ is also dependent to $I_{k}$.
Proof. We have that $r_{i}-\ell_{k} \geq r_{j}-\ell_{k}>\delta$ and $r_{k}-\ell_{i} \geq r_{k}-\ell_{j}>\delta$.
Lemma 34. The graph of Figure 4(e) cannot be a co-TT graph.
Proof. Since $c$ and $d$ are independent, we may assume that $r_{c}-\ell_{d} \leq \delta$. Since $b$ and $d$ are dependent, we have that $r_{b}>\ell_{d}+\delta \geq r_{c}$. The dependency between $c$ and $e$ implies that $r_{b}-\ell_{e}>r_{c}-\ell_{e}>\delta$. Thus, we have that $r_{e}-\ell_{b} \leq \delta$, because $b$ and $e$ are independent. Then, since $a$ and $e$ are dependent, $\ell_{a}<r_{e}-\delta \leq \ell_{b}$.

By symmetry, we can prove that $\ell_{b}<\ell_{d}$, thus $r_{b}-\ell_{f} \leq \delta$ and $r_{a}>r_{b}$. Thus $I_{a}$ contains $I_{b}$ and $b$ cannot have a neighbor that is not adjacent to $a$.

In the opposite direction, we prove that the graphs in Figure 5, which are forbidden for interval graphs [29], can occur as co-TT graphs when $k \geq 2$. (The graph of Figure 5(a) with $k=2$ has long been known to be co-TT [10, 23].) In Figure 6, we show how to realize those graphs as instances of our sorting problem with uncertainty. In both cases, if $\ell_{b^{\prime \prime}}=x$, then we take, for some $0<\epsilon<\delta, r_{b^{\prime}}=x+\delta+\epsilon$ and $I_{e}=[x+\epsilon, x+\delta]$. Then $b^{\prime}$ and $b^{\prime \prime}$ are dependent because $r_{b^{\prime}}-\ell_{b^{\prime \prime}}=\delta+\epsilon>\delta$ (and clearly $r_{b^{\prime \prime}}-\ell_{b^{\prime}}>\delta$ ). But $r_{b^{\prime}}-\ell_{e}=\delta$ and $r_{e}-\ell_{b^{\prime \prime}}=\delta$, so $e$ is dependent to neither $b^{\prime}$ nor $b^{\prime \prime}$.

Figure 5: Two families of graphs that cannot occur as interval graphs [29]. In (a) we have $k \geq 2$. In (b) we have $k \geq 1$. Both families of graphs are co-TT graphs when $k \geq 2$. The white vertices are the only ones that can be trivial, if we take into account Facts 31 and 32.


Figure 6: (a) A realization of the graph of Figure 5(a). (b) A realization of the graph of Figure 5(b) when $k \geq 2$.

# 10. Interval Problems with Uncertainty 

In this section, we discuss uncertainty variants of some classical problems on intervals. In those variants, the boundary of each interval $I_{i}$ is given by uncertainty intervals $L_{i}=\left[\ell_{L_{i}}, r_{L_{i}}\right]$ and $R_{i}=\left[\ell_{R_{i}}, r_{R_{i}}\right]$. We denote the precise lower and upper bounds of $I_{i}$ by $\ell_{i}$ and $r_{i}$, respectively, which are initially unknown and can be learned by querying $L_{i}$ and $R_{i}$, respectively. Those were the problems we started to investigate in the model of uncertainty optimization with queries, and from them we got inspiration to work on the sorting problem of the previous sections.

We begin with the problem of finding a maximum independent set of intervals. The problem has querycompetitive ratio at least $n-1$, even if query costs are uniform and the lower bound $L_{i}$ is trivial for every interval $I_{i}$. (The same applies if the upper bounds are trivial instead of the lower bounds.) To prove this, consider an interval $I_{n}=\left[\ell_{n}, r_{n}\right]$ with trivial lower and upper bounds, and $n-1$ identical intervals, with $\ell_{R_{i}}<\ell_{n}<r_{R_{i}}$ and $r_{n}>r_{R_{i}}$, for $i=1, \ldots, n-1$. Clearly we can have at most 2 independent intervals. For the first $n-2$ queries, if the algorithm queries interval $I_{i}$, then the adversary chooses $r_{i}=r_{R_{i}}$, so $I_{i}$ and $I_{n}$ are dependent. Then, the adversary chooses $r_{j}=\ell_{R_{j}}$ for the $(n-1)$-th queried interval $I_{j}$. Clearly it would suffice to query $I_{j}$ to know that there exists an independent set of size 2 .

We also consider the problem of find the stabbing number of a set of intervals. The stabbing number isthe size of a minimum set of points $P$, such that each interval contains some point in $P$. (I.e., we wish to find a minimum transversal for a set of intervals.) This is equivalent to finding a minimum covering by cliques, which can be solved in polynomial time [20]. In the uncertainty version, this problem also has competitive ratio at least $n-1$, even if query costs are uniform and lower bounds (upper bounds) are trivial. We use the same set of intervals as in the bad instance for the maximum independent set problem. Clearly the stabbing number is either 1 or 2 . For the first $n-2$ queries, if the algorithm queries interval $I_{i}$, then the adversary chooses $r_{i}=r_{R_{i}}$, so 1 stabbing point is sufficient for now. Then, the adversary chooses $r_{j}=\ell_{R_{j}}$ for the $(n-1)$-th queried interval $I_{j}$. Clearly it would suffice to query $I_{j}$ to know that 2 stabbing points are necessary.

# References 

[1] M. Ajtai, V. Feldman, A. Hassidim, and J. Nelson. Sorting and selection with imprecise comparisons. ACM Transactions on Algorithms, 12(2):19:1-19:19, 2016. doi:10.1145/2701427.
[2] L. Arantes, E. Bampis, A. V. Kononov, M. Letsios, G. Lucarelli, and P. Sens. Scheduling under uncertainty: A querybased approach. In IJCAI 2018: 27th International Joint Conference on Artificial Intelligence, pages 4646-4652, 2018. doi:10.24963/ijcai.2018/646.
[3] R. Bar-Yehuda, K. Bendel, A. Freund, and D. Rawitz. Local ratio: A unified framework for approximation algorithms. In Memoriam: Shimon Even 1935-2004. ACM Computing Surveys, 36(4):422-463, 2004. doi:10.1145/1041680.1041683.
[4] R. Bar-Yehuda and S. Even. A linear-time approximation algorithm for the weighted vertex cover problem. Journal of Algorithms, 2(2):198-203, 1981. doi:10.1016/0196-6774(81)90020-1.
[5] H.-G. Beyer and B. Sendhoff. Robust optimization - a comprehensive survey. Computer Methods in Applied Mechanics and Engineering, 196(33-34):3190-3218, 2007. doi:10.1016/j.cma.2007.03.003.
[6] J. R. Birge and F. Louveaux. Introduction to Stochastic Programming. Springer Series in Operations Research and Financial Engineering. Springer, 2011.
[7] A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis. Cambridge University Press, 1998.
[8] J. Boyar, L. M. Favrholdt, C. Kudahl, K. S. Larsen, and J. W. Mikkelsen. Online algorithms with advice: A survey. ACM Computing Surveys, 50(2), 2017. doi:10.1145/3056461.
[9] R. Bruce, M. Hoffmann, D. Krizanc, and R. Raman. Efficient update strategies for geometric computing with uncertainty. Theory of Computing Systems, 38(4):411-423, 2005. doi:10.1007/s00224-004-1180-4.
[10] T. Calamoneri and B. Sinaimeri. Relating threshold tolerance graphs to other graph classes. In ICTCS 2014: 15th Italian Conference on Theoretical Computer Science, pages 73-79, 2014. URL: http://ceur-ws.org/Vol-1231/long5.pdf.
[11] G. Charalambous and M. Hoffmann. Verification problem of maximal points under uncertainty. In T. Lecroq and L. Mouchard, editors, IWUCA 2013: 24th International Workshop on Combinatorial Algorithms, volume 8288 of Lecture Notes in Computer Science, pages 94-105. Springer Berlin Heidelberg, 2013. doi:10.1007/978-3-642-45278-9_9.
[12] C. Dürr, T. Erlebach, N. Megow, and J. Meißner. An adversarial model for scheduling with testing. Algorithmica, 82:3630-3675, 2020. doi:10.1007/s00453-020-00742-2.
[13] T. Erlebach and M. Hoffmann. Minimum spanning tree verification under uncertainty. In D. Kratsch and I. Todinca, editors, WG 2014: International Workshop on Graph-Theoretic Concepts in Computer Science, volume 8747 of Lecture Notes in Computer Science, pages 164-175. Springer Berlin Heidelberg, 2014. doi:10.1007/978-3-319-12340-0_14.
[14] T. Erlebach and M. Hoffmann. Query-competitive algorithms for computing with uncertainty. Bulletin of EATCS, 116:22-39, 2015. URL: http://bulletin.eatcs.org/index.php/beatcs/article/view/335.
[15] T. Erlebach, M. Hoffmann, and F. Kammer. Query-competitive algorithms for cheapest set problems under uncertainty. Theoretical Computer Science, 613:51-64, 2016. doi:10.1016/j.tcs.2015.11.025.
[16] T. Erlebach, M. Hoffmann, D. Krizanc, M. Mihal'ák, and R. Raman. Computing minimum spanning trees with uncertainty. In STACS'08: 25th International Symposium on Theoretical Aspects of Computer Science, pages 277-288, 2008. URL: https://arxiv.org/abs/0802.2855.
[17] T. Feder, R. Motwani, L. O'Callaghan, C. Olston, and R. Panigrahy. Computing shortest paths with uncertainty. Journal of Algorithms, 62(1):1-18, 2007. doi:10.1016/j.jalgor.2004.07.005.
[18] T. Feder, R. Motwani, R. Panigrahy, C. Olston, and J. Widom. Computing the median with uncertainty. SIAM Journal on Computing, 32(2):538-547, 2003. doi:10.1137/S0097539701395668.
[19] J. Focke, N. Megow, and J. Meißner. Minimum spanning tree under explorable uncertainty in theory and experiments. ACM Journal of Experimental Algorithmics, 2020. doi:10.1145/3422371.
[20] F. Gavril. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing, 1(2):180-187, 1972. doi:10.1137/0201013.
[21] M. Goerigk, M. Gupta, J. Ide, A. Schöbel, and S. Sen. The robust knapsack problem with queries. Computers $\mathcal{E}$ Operations Research, 55:12-22, 2015. doi:10.1016/j.cor.2014.09.010.
[22] P. A. Golovach, P. Heggernes, N. Lindzey, R. M. McConnell, V. F. dos Santos, J. P. Spinrad, and J. L. Szwarcfiter. On recognition of threshold tolerance graphs and their complements. Discrete Applied Mathematics, 216(1):171-180, 2017. doi:10.1016/j.dam.2015.01.034.
[23] M. C. Golumbic, C. L. Monma, and W. T. Trotter Jr. Tolerance graphs. Discrete Applied Mathematics, 9(2):157-170, 1984. doi:10.1016/0166-218X(84)90016-7.[24] M. C. Golumbic, N. L. Weingarten, and V. Limouzy. Co-TT graphs and a characterization of split co-TT graphs. Discrete Applied Mathematics, 165:168-174, 2014. doi:10.1016/j.dam.2012.11.014.
[25] M. Gupta, Y. Sabharwal, and S. Sen. The update complexity of selection and related problems. Theory of Computing Systems, 59(1):112-132, 2016. doi:10.1007/s00224-015-9664-y.
[26] C. A. R. Hoare. Quicksort. The Computer Journal, 5(1):10-16, 1962. doi:10.1093/comjnl/5.1.10.
[27] S. Kahan. A model for data in motion. In STOC'91: 23rd Annual ACM Symposium on Theory of Computing, pages 265-277, 1991. doi:10.1145/103418.103449.
[28] S. Khanna and W.-C. Tan. On computing functions with uncertainty. In PODS'01: 20th ACM SIGMOD-SIGACTSIGART Symposium on Principles of Database Systems, pages 171-182, 2001. doi:10.1145/375551.375577.
[29] C. Lekkerkerker and J. Boland. Representation of a finite graph by a set of intervals on the real line. Fundamenta Mathematicae, 51(1):45-64, 1962. URL: https://eudml.org/doc/213681.
[30] T. Maehara and Y. Yamaguchi. Stochastic packing integer programs with few queries. Mathematical Programming, 182:141-174, 2020. doi:10.1007/s10107-019-01388-x.
[31] N. Megow, J. Meißner, and M. Skutella. Randomization helps computing a minimum spanning tree under uncertainty. SIAM Journal on Computing, 46(4):1217-1240, 2017. doi:10.1137/1681088375.
[32] C. L. Monma, B. Reed, and W.T. Trotter Jr. Threshold tolerance graphs. Journal of Graph Theory, 12(3):343-362, 1988. doi:10.1002/jgt. 3190120307.
[33] C. Olston and J. Widom. Offering a precision-performance tradeoff for aggregation queries over replicated data. In VLDB 2000: 26th International Conference on Very Large Data Bases, pages 144-155, 2000. URL: http://ilpubs.stanford.edu:8090/437/.
[34] I. O. Ryzhov and W. B. Powell. Information collection for linear programs with uncertain objective coefficients. SIAM Journal on Optimization, 22(4):1344-1368, 2012. doi:10.1137/12086279X.
[35] A. Salah, K. Li, and K. Li. Lazy-Merge: A novel implementation for indexed parallel $k$-way in-place merging. IEEE Transactions on Parallel and Distributed Systems, 27(7):2049-2061, 2015. doi:10.1109/TPDS.2015.2475763.
[36] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379-423, 1948. doi:10.1002/j.1538-7305.1948.tb01338.x.