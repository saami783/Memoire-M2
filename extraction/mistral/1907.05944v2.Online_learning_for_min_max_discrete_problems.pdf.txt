# Online learning for min-max discrete problems 

Evripidis Bampis ${ }^{1}$, Dimitris Christou ${ }^{1,2}$, Bruno Escoffier ${ }^{1}$, Nguyễn Kim Thắng ${ }^{1,3}$<br>${ }^{1}$ Sorbonne Université, CNRS, LIP6, F-75005 Paris, France<br>${ }^{2}$ National Technical University of Athens, Greece<br>${ }^{3}$ IBISC, Univ Evry, University Paris-Saclay, France


#### Abstract

We study various discrete nonlinear combinatorial optimization problems in an online learning framework. In the first part, we address the computational complexity of designing vanishing regret (and vanishing approximate regret) algorithms. We provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation $\alpha$-regret, for some $\alpha$, unless $N P=R P$. Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. Our reduction implies that there is no $(2-\epsilon)$-regret online randomized algorithm unless Unique Game is in $R P$. Besides, we prove a matching upper bound providing an online algorithm based on the online gradient descent method.

In the second part, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. We start by presenting an online algorithm with vanishing regret that is based on the Follow the Perturbed Leader algorithm for an interesting generalization of knapsack (Barman et al. ICALP 2012). Subsequently, we show that for several min-max (nonlinear) discrete optimization problems, it is strongly $N P$-hard to solve the offline optimization oracle, even for problems that can be solved in polynomial time in the static case (e.g. min-max vertex cover, minmax perfect matching, etc.). This also provides a useful insight into the connection between the non-linear nature of some problems and the increase of their computational hardness when moved to an online learning setting.# 1 Introduction 

Over the past years, online learning has become a very active research field. This is due to the widespread of applications with evolving or adversarial environments, e.g. routing schemes in networks [3], online marketplaces [7], spam filtering [14], etc. An online learning algorithm has to iteratively choose an action over a (possible infinite) set of feasible decisions. A loss/reward is associated to each decision which may be adversarially chosen. The losses/rewards are unknown to the algorithm beforehand, making it impossible, in general, to be competitive with the best dynamic strategy that may change decisions each period. On that note, the goal is to minimize the regret, i.e. the difference between the total loss/reward of the online algorithm and that of the best offline decision, if one had to choose a single decision in hindsight

A "good" online learning algorithm is an algorithm whose regret is sublinear as a function of the length of the time-horizon since then, on the average, the algorithm performs as well as the best single action in hindsight on the long term. Such an online algorithm is called an online learning algorithm with vanishing regret. For problems for which the offline version is $N P$-hard, the notions of regret and vanishing regret have been extended to the notions of $\alpha$-regret and $\alpha$-vanishing regret in order to take into account the existence of an $\alpha$-approximation algorithm instead of an exact algorithm for solving the offline optimization problem.

While a lot of online learning problems can be modeled as the so called "experts problem" by associating a feasible solution to an expert, there is clearly an efficiency challenge since there are potentially an exponential number of solutions making problematic the use of such an approach in practice. Hazan and Koren [16] proved that a vanishing regret algorithm with running-time polynomial in the size of the problem does not exist in general settings without any assumption on the structure of the problem. However, there are many results that prove that the structure of a problem can be indeed exploited in order to get such an algorithm. Notably, Kalai and Vempala [18] proved that for linear problems that are polynomially solvable, such an algorithm can always be constructed by using any algorithm for the offline problem as an oracle. Other methods have been used such as the online gradient descent [27] for convex objectives, a generalization of the Follow the Perturbed Leader algorithm for submodular objective functions [15] and the Generalized Follow the Perturbed Leader [9] algorithm for general objectives.

Our work takes into account the computational efficiency of online learning algorithms in the same vein as the works in $[1,18,15,25,8,9,17,12]$. We study various discrete nonlinear combinatorial optimization problems in an online learning framework. We focus in particular on min-max (discrete) optimization problems, for two main reasons. First, together with the utilitarian objective, the egalitarian objective - represented by the min-max function - is arguably one of the most studied families of objective functions in Optimization, Operations Research, Game Theory, etc (see for instance [11, 5, 6]). Second and most importantly with the current article, as we will see the non-linearity of min-max problems lead to several hardness results in the online learning framework, even for min-max problems which are trivial to solve efficiently in the offline setting, thus exhibiting an interesting difference with linear discrete problems.

While most works focus on analyzing online learning algorithms that can achieve vanishing regret or $\alpha$-vanishing regret for certain families of problems, not many results that show the computational hardness of achieving those properties are known. Note that, a hardness result for offline optimization do not necessarily translate to an analogous hardness result for online learning because an online learning algorithm could conceivably achieve a small cost with different solutions at different times, while never figuring out the best fixed solution in hindsight. In generally, our goal in this work is to address the three following central questions:

- (Q1): Are there negative results showing that getting vanishing regret (or even vanishing approximate regret) is computationally hard?- (Q2): How does the non-linear nature of an online learning problem tie in with its computational hardness?
- (Q3): Are there some notable differences in the efficiencies of Follow the Leader and gradient descent strategies for discrete problems?


# 1.1 The online learning framework 

An online learning problem consists of a decision-space $\mathcal{X}$, a state-space $\mathcal{Y}$ and an objective function $f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$ that can be either a cost or a reward function. Any problem of this class can be viewed as an iterative adversarial game with $T$ rounds where the following procedure is repeated for $t=1, \ldots, T$ : (a) the algorithm first chooses an action $x^{t} \in \mathcal{X}$; (b) after the algorithm has committed to its choice, the nature reveals a state $y^{t} \in \mathcal{Y}$; (c) the algorithm observe the state $y^{t}$ and suffers a loss (or gains a reward) $f^{t}\left(x^{t}\right)=f\left(x^{t}, y^{t}\right)$. We use $f^{t}(x)$ as another way to refer to the objective function $f$ after observing the state $y^{t}$, i.e. the objective function at round $t$.

The objective of the player is to minimize/maximize the accumulative cost/reward of his decided actions, which is given by the aggregation $\sum_{t=1}^{T} f\left(x^{t}, y^{t}\right)$. An online learning algorithm is any algorithm that decides the actions $x^{t}$ at every round before observing $y^{t}$. We compare the decisions $\left(x^{1}, \ldots, x^{T}\right)$ of the algorithm with those of the best static action in hindsight, defined as: $x^{*}=$ $\arg \min _{x \in \mathcal{X}} \sum_{t=1}^{T} f\left(x, y^{t}\right)$, or $x^{*}=\arg \max _{x \in \mathcal{X}} \sum_{t=1}^{T} f\left(x, y^{t}\right)$, for minimization or maximization problems respectively. This is the action that a (hypothetical) offline oracle would compute, if it had access to the entire sequence $y^{1}, \ldots, y^{T}$. The typical measurement for the efficiency of an online learning algorithm is the regret, defined as:

$$
R_{T}=\sum_{t=1}^{T} f\left(x^{t}, y^{t}\right)-\sum_{t=1}^{T} f\left(x^{*}, y^{t}\right)
$$

As generally no deterministic learning algorithm has a good worst case behaviour, a learning algorithm typically uses some kind of randomness, and the regret denotes the expectation of the above quantity; in all the article, by referring to a learning algorithm we mean a randomized learning algorithm (including for the impossibility results). We are interested in online learning algorithms that have the "vanishing regret" property. This means that as the "game" progresses $(T \rightarrow \infty)$, the average deviation between the algorithm's average cost/payoff to the average cost/payoff of the optimum action in hindsight tends to zero. Typically, a vanishing regret algorithm is an algorithm with regret $R_{T}$ such that: $\lim _{T \rightarrow \infty} \frac{R_{T}}{T}=0$. However, as we are interested in polynomial time algorithms, we consider only vanishing regret $R_{T}=O\left(T^{c}\right)$ where $0 \leq c<1$ (that guarantees the convergence in polynomial time). Throughout the paper, whenever we mention vanishing regret, we mean regret $R_{T}=O\left(T^{c}\right)$ where $0 \leq c<1$.

For many online learning problems, even their offline versions are $N P$-hard. Thus, it is not feasible to produce a vanishing regret sequence with an efficient algorithm. For such cases, the notion of $\alpha$-regret has been defined as:

$$
R_{T}^{\alpha}=\sum_{t=1}^{T} f\left(x^{t}, y^{t}\right)-\alpha \sum_{t=1}^{T} f\left(x^{*}, y^{t}\right)
$$

Hence, we are interested in vanishing $\alpha$-regret sequences for some $\alpha$ for which we know how to approximate the offline problem. The notion of vanishing $\alpha$-regret is defined in the same way as that of vanishing regret. In this article we focus on computational issues. Efficiency for an online learning algorithm needs to capture both the computation of $x^{t}$ and the convergence speed. This is formalized in the following definition (where $n$ denotes the size of the instance).Definition 1. A polynomial time vanishing $\alpha$-regret algorithm is an online learning algorithm for which (1) the computation of $x^{t}$ is polynomial in $n$ and $t$ (2) the expected $\alpha$-regret is bounded by $p(n) T^{c}$ for some polynomial $p$ and some constant $0 \leq c<1$. We note that $n$ is the size of the instance which can be much smaller than the decision space of the problem.

Note that in case $\alpha=1$, we simply use the term polynomial time vanishing regret algorithm.

# 1.2 Our contribution 

In Section 2, we provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation $\alpha$-regret, for some $\alpha$, unless $N P=R P$. The result holds for online randomized algorithms against an oblivious adversary. This result gives an answer to question $(Q 1)$ that was stated above. Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. The previous reduction proves that there is no $(2-\epsilon)$-regret online randomized algorithm unless Unique Game is in $R P$. Besides, we prove a matching upper bound providing an online algorithm based on the online gradient descent method.

In Section 3, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. This approach is known in the literature as the Follow the Leader method. To the best of our knowledge, up to now algorithms based on the Follow the Leader method for non-linear objective functions require an exact oracle or a constant additive error oracle in order to obtain vanishing regret. By slightly extending the Generalized Follow the Perturbed Leader algorithm of Dudik et al. [9], we are able to replace the assumption of a constant additive error oracle with that of a FPTAS oracle. We use this extension to present an online algorithm with vanishing regret that is based on the Follow the Perturbed Leader algorithm for a well-studied generalization of the knapsack problem with applications in different areas like cloud computing or connection management in wireless access points [2, 4].

Subsequently, in Section 3.1 we show strong NP-hardness results for the multiple instance version of some offline problems, which indicates that follow-the-leader-type strategies cannot be used for the online learning problem, at least with our current knowledge. More precisely, we show that such hardness results hold even for problems that can be solved in polynomial time in the static case (such as min-max vertex cover or min-max perfect matching). In particular, we also prove that the offline optimization oracle is strongly $N P$-hard for the problem of scheduling a set of jobs on $m$ identical machines, where $m$ is a fixed constant. These results provide some useful insight towards answering question $(Q 2)$.

Furthermore, the hardness results on the oracle for the min-max vertex cover problem, paired with the upper bound that is presented in Section 2.2 using the online gradient descent method, provide some answer to question $(Q 3)$.

### 1.3 Further related works

Online Learning, or Online Convex Optimization, is an active research domain. In this section, we only summarize works which are directly related to ours. We refer the reader to comprehensive books $[24,14]$ and references therein for a more complete overview. The first vanishing regret algorithm has been given by Hannan [13]. Subsequently, Littlestone and Warmuth [21] and Freund and Schapire [10] gave improved algorithms with regret $\sqrt{\log (|\mathcal{A}|) o}(T)$ where $|\mathcal{A}|$ is the size of the action space. However, these algorithms have running-time $\Omega(|\mathcal{A}|)$ which is exponential in the size of the input for many applications, in particular for combinatorial optimization problems. An intriguing question is whether there exists a vanishing regret online algorithm with running-time polynomial in $\log (|\mathcal{A}|)$. Hazan and Koren [16] proved that no such algorithm exists in general settings without any assumption on the struc-ture. Designing online polynomial-time algorithms with approximation and vanishing regret guarantees for combinatorial optimization problems is a major research agenda.

In their breakthrough paper, Kalai and Vempala [18] presented the first efficient online algorithm, called Follow the Perturbed Leader (FTPL), for linear objective functions. The strategy consists of adding perturbation to the cumulative gain (payoff) of each action and then selecting the action with the highest perturbed gain. This strategy has been generalized and successfully applied to several settings [15, 25, 8, 9]. Specifically, FTPL and its generalized versions have been used to design efficient online vanishing regret algorithms with oracles beyond linear settings: to submodular settings [15] and nonconvex settings [1]. However, all these approaches require best-response oracles, and as we show in the current paper, for several problems such best-response oracles require exponential time computation.

Another direction is to design online learning algorithms using (offline polynomial-time) approximation algorithms as oracles. Kakade et al.[17] provided an algorithm which is inspired by Zinkevich's algorithm [27] (gradient descent): at every step, the algorithm updates the current solution in the direction of the gradient and project back to the feasible set using an approximation algorithm. They showed that given an $\alpha$-approximation algorithm for a linear optimization problem, after $T$ prediction rounds (time steps) the online algorithm achieves an $\alpha$-regret bound of $O\left(T^{-1 / 2}\right)$ using $T$ calls to the approximation algorithm per round in average. Later on, Garber [12] gave an algorithm with $\alpha$-regret bound of $O\left(T^{-1 / 3}\right)$ using only $O(\log T)$ calls to the approximation algorithm per round in average. These algorithms rely crucially on the linearity of the objective functions and it remains an interesting open question to design algorithms for online non-linear optimization problems.

# 2 Hardness of online learning for min-max problems 

### 2.1 General reduction

As mentioned in the introduction, in this section we give some answers to question $(Q 1)$ on ruling out the existence of vanishing regret algorithm for a broad family of online min-max problems, even for ones that are polynomial-time solvable in the offline case. In fact, we provide a general reduction (see Theorem 1) showing that many min-max problems do not admit vanishing $\alpha$-regret for some $\alpha>1$ unless $N P=R P$.

More precisely, we focus on a class of cardinality minimization problems where, given an $n$-elements set $\mathcal{U}$, a set of constraints $\mathcal{C}$ on the subsets of $\mathcal{U}$ (defining feasible solutions) and an integer $k$, the goal is to determine whether there exists a feasible solution of size at most $k$. This is a general class of problems, including for instance graph problems such as Vertex Cover, Dominating Set, Feedback Vertex Set, etc.

Given such a cardinality problem $\mathcal{P}$, let min-max- $\mathcal{P}$ be the optimization problem where given nonnegative weights for all the elements of $\mathcal{U}$, one has to compute a feasible solution (under the same set of constraints $\mathcal{C}$ as in problem $\mathcal{P}$ ) such that the maximum weight of all its elements is minimized. The online min-max- $\mathcal{P}$ problem is the online learning variant of min-max- $\mathcal{P}$, where the set of elements $\mathcal{U}$ and the set of constrains $\mathcal{C}$ remain static but the weights on the elements of $\mathcal{U}$ change over time.

Interestingly, the min-max versions of all the problems mentioned above are polynomially solvable. This is actually true as soon as, for problem $\mathcal{P}$, every superset of a feasible solution is feasible. Then one just has to check for each possible weight $w$ if the set of all elements of weight at most $w$ agrees with the constraints. For example, one can decide if there exists a vertex cover with the maximum weight $w$ as follows: remove all vertices of weight strictly larger than $w$, and check if the remaining vertices form a vertex cover.

We will show that, in contrast, if $\mathcal{P}$ is $N P$-complete then its online learning min-max version has no vanishing regret algorithm (unless $N P=R P$ ), and that if $\mathcal{P}$ has an inapproximability gap $r$, then there is no vanishing $\alpha$-regret algorithm for its online learning min-max version, for any $\alpha<r$. Let us first recall the notion of approximation gap, where $x_{\text {opt }}$ denotes a feasible solution of minimum size tothe cardinality problem $\mathcal{P}$.
Definition 2. Given two numbers $0 \leq A<B \leq 1$, let [A,B]-Gap- $\mathcal{P}$ be the decision problem where given an instance of $\mathcal{P}$ such that $\left|x_{\text {opt }}\right| \leq A n$ or $\left|x_{\text {opt }}\right| \geq B n$, we need to decide whether $\left|x_{\text {opt }}\right|<B n$.

Now we can state the main result of the section.
Theorem 1. Let $\mathcal{P}$ be a cardinality minimization problem and $A, B$ be real numbers with $0 \leq A<$ $B \leq 1$. Assume that the problem [A,B]-Gap- $\mathcal{P}$ is $N P$-complete. Then, for every $\alpha<\frac{B}{A}$ there is no randomized polynomial-time vanishing $\alpha$-regret algorithm for online min-max- $\mathcal{P}$ unless $N P=R P$.

Proof. We prove this theorem by deriving a polytime algorithm for [A,B]-Gap- $\mathcal{P}$ that gives, under the assumption of a vanishing $\alpha$-regret algorithm for online min-max- $\mathcal{P}$ with $\alpha<\frac{B}{A}$, the correct answer with probability of error at most $D$ for some constant $D<1$ if $\left|x_{\text {opt }}\right| \leq A n$, and with no error if $\left|x_{\text {opt }}\right| \geq B n$. This would imply that the [A,B]-Gap- $\mathcal{P}$ problem is in $R P$ and thus $N P=R P$.

Let $\mathcal{O}$ be a (randomized) vanishing $\alpha$-regret algorithm for online min-max- $\mathcal{P}$ for some $\alpha=\left(\frac{B}{A}-\epsilon\right)=$ $\left(1-\epsilon^{\prime}\right) \frac{B}{A}$ where $\epsilon>0$ is a constant and $\epsilon^{\prime}=\frac{A}{B} \epsilon$. Let $T$ be a time horizon which will be fixed later. We construct the following (offline) randomized algorithm for [A,B]-Gap- $\mathcal{P}$ using $\mathcal{O}$ as an oracle (subroutine). At every step $1 \leq t \leq T$, the algorithm:

- uses the oracle $\mathcal{O}$ to compute a solution $x^{t}$ (i.e., it draws $x^{t}$ from the random distribution $X_{t}$ given by the oracle),
- fixes the weights randomly as follows: it chooses one element uniformly at random in $\mathcal{U}$ and assign weight 1 to that element, and weight 0 to all other ones (so each element has weight 1 with probability $1 / n$ ).

Note that these weights are chosen in an oblivious manner (independently on the solutions computed by the oracle $\mathcal{O}$ ). The formal description is given in Algorithm 1.

```
Algorithm 1: Algorithm for the [A,B]-Gap- \(\mathcal{P}\) problem
for \(t=1,2, \ldots, T\) do
    Choose \(x^{t} \in \mathcal{X}\) according to the random distribution \(X_{t}\) given by algorithm \(\mathcal{O}\).
    if \(\left|x^{t}\right|<B n\) then return \(\mathrm{Yes}\), i.e., \(\left|x_{\text {opt }}\right| \leq A n\).
    Assign weight 1 to an element of \(\mathcal{U}\) chosen uniformly at random and 0 to all other elements of
        \(\mathcal{U}\).
    Feed the weight vector and the cost \(f^{t}\left(x^{t}\right)=\max _{u \in x^{t}} w^{t}(u)\) back to \(\mathcal{O}\).
end
return No, i.e., \(\left|x_{\text {opt }}\right| \geq B n\).
```

We now analyze the probability of error of Algorithm 1.
If $\left|x_{\text {opt }}\right| \geq B n$, then obviously all the solutions $x^{t}$ computed by $\mathcal{O}$ have sizes at least $B n$, so the algorithm return No, with no error.

In the remaining of the proof, assume that $\left|x_{\text {opt }}\right| \leq A n$. We now analyse the probability that Algorithm 1 fails (returns No) in this case, i.e., conditionally to the fact that $\left|x_{\text {opt }}\right| \leq A n$ (all the probabilities in the sequel are conditional to $\left|x_{\text {opt }}\right| \leq A n$ ).

Let $E$ denote the event that the algorithm returns a wrong answer (when $\left|x_{\text {opt }}\right| \leq A n$ ). The algorithm fails iff at each time step $\left|x^{t}\right| \geq B n$. We get:

$$
\mathbb{P}[E]=\mathbb{P}\left[\cap_{t=1}^{T}\left\{\left|X_{t}\right| \geq B n\right\}\right] \leq \mathbb{P}[X \geq T B n]
$$where $X=\sum_{t=1}^{T}\left|X_{t}\right|$ is the random variable equal to the sum of sizes of (random) solutions outputs by $\mathcal{O}$. As $X \geq 0, \mathbb{P}[X \geq T B n] \leq \frac{\mathbb{E}[X]}{T B n}=\frac{\sum_{t=1}^{T} \mathbb{E}\left[|X_{t}|\right]}{T B n}$. Since the element of weight 1 at time $t$ is chosen uniformly at random, $\mathbb{E}\left[f^{t}\left(X_{t}\right)\right]=\mathbb{E}\left[\left|X_{t}\right|\right] / n$, so

$$
\mathbb{P}[E] \leq \frac{\mathbb{E}[X]}{T B n} \leq \frac{\sum_{t=1}^{T} \mathbb{E}\left[f^{t}\left(X_{t}\right)\right]}{T B}
$$

Now, since $\left|x_{o p t}\right| \leq A n$, and since at each time $t$ there is only one element of weight 1 picked uniformly at random, $f^{t}\left(x_{o p t}\right)=1$ with probability at most $A$, we get $\sum_{t=1}^{T} \mathbb{E}\left[f^{t}\left(x_{o p t}\right)\right] \leq A T$. Besides, as $\mathcal{O}$ is a vanishing $\alpha$-regret algorithm with $\alpha=\left(1-\epsilon^{\prime}\right) \frac{B}{A}$, we have that:

$$
\sum_{t=1}^{T} \mathbb{E}\left[f^{t}\left(X_{t}\right)\right] \leq \alpha \sum_{t=1}^{T} \mathbb{E}\left[f^{t}\left(x_{o p t}\right)\right]+p(n) T^{c} \leq\left(1-\epsilon^{\prime}\right) B T+p(n) T^{c}
$$

Hence, we deduce that

$$
\mathbb{P}[E] \leq \frac{\left(1-\epsilon^{\prime}\right) B T+p(n) T^{c}}{B T}=1-\epsilon^{\prime}+\frac{p(n) T^{c-1}}{B}
$$

Choose parameter $T=\left(\frac{B \epsilon^{\prime}}{2 p(n)}\right)^{\frac{1}{c-1}}=\left(\frac{A \epsilon}{2 p(n) B}\right)^{\frac{1}{c-1}}$, we get that $\mathbb{P}[E] \leq 1-\frac{\epsilon^{\prime}}{2}=1-\frac{A \epsilon}{2 B}$, which is a constant strictly smaller than 1. Besides, the running time of Algorithm 1 is polynomial since it consists of $T$ (polynomial in the size of the problem) iterations and the running time of each iteration is polynomial (as $\mathcal{O}$ is a polynomial time algorithm).

In conclusion, if there exists a vanishing $\alpha$-regret algorithm for online min-max- $\mathcal{P}$, then the $N P$ complete problem $[A, B]-G a p-\mathcal{P}$ is in $R P$, implying $N P=R P$.

The inapproximability (gap) results for the aforementioned problems give lower bounds on the approximation ratio $\alpha$ of any vanishing $\alpha$-regret algorithm for their online min-max version. Note that these lower bounds hold with an oblivious adversary (weights are chosen in a non adaptive way), and that it applies to many min-max problems that are (trivially) polynomially solvable.

For instance, the online min-max dominating set problem has no vanishing constant-regret algorithm based on the approximation hardness in [22]. We state the lower bound explicitly for the online min-max vertex cover problem in the following corollary, as we refer to it later by showing a matching upper bound. They are based on the hardness results for vertex cover in [20] and [19] ( $N P$-hardness and UGC-hardness, respectively).

Corollary 2. The online min-max vertex cover problem does not admit a polynomial time vanishing $(\sqrt{2}-\epsilon)$-regret unless $N P=R P$. It does not admit a polynomial time vanishing $(2-\epsilon)$-regret unless Unique Game is in $R P$.

Now, consider $N P$-complete cardinality problems which have no known inapproximability gap (for instance Vertex Cover in planar graphs, which admits a PTAS). Then we can show the following impossibility result (see Appendix A. 1 for the proof).

Corollary 3. If a cardinality problem $\mathcal{P}$ is $N P$-complete, then there is no vanishing regret algorithm for online min-max- $\mathcal{P}$ unless $N P=R P$.

# 2.2 Min-max Vertex Cover: matching upper bound with Gradient Descent 

In this section we will present an online algorithm for the min-max vertex cover problem based on the classic Online Gradient Descent (OGD) algorithm. In the latter, at every step the solution is obtainedby updating the previous one in the direction of the (sub-)gradient of the objective and projecting to a feasible convex set. The particular nature of the min-max vertex cover problem is that the objective function is the $l_{\infty}$ norm and the set of feasible solutions (all subsets of the vertex set $V$ that are vertex covers) $\mathcal{X}$ is discrete (non-convex). In our algorithm, we consider the following standard relaxation of the problem:

$$
\min \max _{i \in V} w_{i} x_{i} \quad \text { s.t. } \quad x \in \mathcal{Q}: \quad x_{i}+x_{j} \geq 1 \forall(i, j) \in E, \quad 0 \leq x_{i} \leq 1 \forall i \in V
$$

At time step $t$, after deciding on a (fractional) action $x^{t} \in \mathcal{Q}$ we update the solution by a subgradient $g^{t}\left(x^{t}\right)=\left[0, \ldots, 0, w_{i}^{t}, 0, \ldots, 0\right]$ with $w_{i}^{t}$ in coordinate $i^{t}\left(x^{t}\right)=\arg \max _{1 \leq i \leq n} w_{i}^{t} x_{i}^{t}$ and 0 in other coordinates. Moreover, after projecting the solution to the polytope $\mathcal{Q}$, we round the solution by a simple procedure: if $x_{i}^{t+1} \geq 1 / 2$ then $X_{i}^{t+1}=1$ and $X_{i}^{t+1}=0$ otherwise. The formal algorithm is given in Algorithm 2.

```
Algorithm 2: OGD-based algorithm for Online MinMax Vertex Cover
1 Select an arbitrary fractional vertex cover \(x^{1} \in \mathcal{Q}\).
2 for \(t=1,2, \ldots\) do
3 Round \(x^{t}\) to \(X^{t}: X_{i}^{t}=1\) if \(x_{i}^{t} \geq 1 / 2\) and \(X_{i}^{t}=0\) otherwise.
4 Play \(X^{t} \in\{0,1\}^{n}\). Observe \(w^{t}\) (weights of vertices) and incur the \(\operatorname{cost} g^{t}\left(X^{t}\right)=\max _{i} w_{i}^{t} X_{i}^{t}\).
5 Update \(y^{t+1}=x^{t}-\frac{1}{\sqrt{t}} g^{t}\left(x^{t}\right)\).
6 Project \(y^{t+1}\) to \(\mathcal{Q}\) w.r.t the \(\ell_{2}\)-norm: \(x^{t+1}=\operatorname{Proj}_{\mathcal{Q}}\left(y^{t+1}\right):=\arg \min _{x \in \mathcal{Q}}\left\|y^{t+1}-x\right\|_{2}\).
7 end
```

The following theorem (see Appendix A. 2 for the proof), coupled with Corollary 2, show the tight bound of 2 on the approximation ratio of polynomial-time online algorithms for Min-max Vertex Cover (assuming UGC conjecture).

Theorem 4. Assume that $W=\max _{1 \leq t \leq T} \max _{1 \leq i \leq n} w_{i}^{t}$ is the maximum assigned weight. Then, after $T$ time steps, Algorithm 2 achieves

$$
\sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} X_{i}^{t} \leq 2 \cdot \min _{X^{*} \in \mathcal{X}} \sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} X_{i}^{*}+3 W \sqrt{n T}
$$

# 3 Computational issues for Follow the Leader based methods 

In the previous section, we proved that a large family of online-learning problems does not admit a vanishing regret algorithm (Corollary 3) even though they can be easily solved in polynomial time in the offline case. The purpose of this section is to further explore the role of non-linearity in the hardness gap between an offline problem and the its corresponding online-learning variant. In order to do so, we focus our attention to the family of Follow the Leader (FTL) algorithms that has been extensively studied and used for the online-learning setting.

We begin by providing the reader with a short introduction to the Follow the Leader based methods. The most natural approach in online learning is for the player to always pick the leading action, i.e. the action $x^{t}$ that is optimal to the observed history $y^{1}, \ldots, y^{t-1}$. However it can be proven ([18]) that any deterministic algorithm that always decides on the leading action can be "tricked" by the adversary in order to make decision that are worse than the optimal action in hindsight, thus leading to large regret algorithms. On this regard, we need to add a regularization term containing randomness to the optimization oracle in order to make our algorithms less predictable and more stable. Thus, the Followthe Regularized Leader strategy in a minimization problem, consists of deciding on an action $x^{t}$ such that:

$$
x^{t}=\underset{x \in \mathcal{X}}{\arg \min }\left(\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+R(x)\right)
$$

where $R(x)$ is the regularization term.
There are many variations of the Follow the Regularized Leader (FTRL) algorithm that differentiate on the applied objective functions and the type of regularization term. For linear objectives, Kalai and Vempala [18] suggested the Follow the Perturbed Leader algorithm where the regularization term is simply the cost/payoff of each action on a randomly generated instance of the problem. Dudik et al. [9] were able to generalize the FTPL algorithm of Kalai and Vempala [18] for non-linear objectives, by introducing the concept of shared randomness and a much more complex perturbation mechanism.

A common element between every Follow the Leader based method, is the need for an optimization oracle over the observed history of the problem. This is a minimum requirement since the regularization term can make determining the leader even harder, but most algorithms are able to map the perturbations to the value of the objective function on a set of instances of the problem and thus eliminate this extra complexity. In particular, Kalai and Vempala [18] showed that for every linear objective, if the offline version of the problem can be solved exactly or it admits a FPTAS, then the same holds for the multiinstance offline version of the problem (solved by the oracle) which leads to a vanishing regret algorithm for the online-learning variant of the problem.

On the same note, Dudik et al. [9] introduced the Generalized Follow the Perturbed Leader (GFTPL) algorithm. While analyzing the algorithm, they showed that an exact or an additive error oracle that solves the multi-instance problem is sufficient to achieve the same regret bounds under some extra mild assumptions, even for non-linear objectives. In this section, we will turn our focus to the existence of such oracles that can be efficiently computed.

If the problem solved by the oracle is $N P$-hard, having an efficient algorithm that solves it with any additive error $\epsilon$ is quite improbable. We remark that the assumption of an additive error $\epsilon$ can be replaced by the assumption of the existence of a FPTAS for the oracle. This is captured by the following Theorem:

Theorem 5. The upper bounds of the Generalized Follow the Perturbed Leader algorithm hold even under the assumption that there exists a FPTAS oracle instead of an additive error oracle.

Proof. While the proof relies on a single technical observation, it requires to establish the full framework of the GFTPL algorithm. On that note, it is moved to the Appendix B. 2 and is presented after a (brief) introduction to the GFTPL algorithm (Appendix B.1).

We now give a concrete example of the way that GFTPL algorithm can be applied for non-linear objectives in discrete problems using an FPTAS oracle. Let us consider the generalized knapsack problem where we are allowed to exceed the capacity of the knapsack but then pay a penalty proportional to the excess weight. Formally:

Definition 3 (Generalized Knapsack Problem (GKP)). Given a set of items $i=1,2, \ldots, n$ with nonnegative weights $w_{i}$ and non-negative profits $p_{i}$, a knapsack capacity $B \in R_{+}$and a constant $c \in$ $R_{+}$, determine a set of items $A \subseteq[n]$ that maximizes the total profit $\operatorname{profit}(A)=\sum_{i \in A} p_{i}-$ $c \max \left\{0, \sum_{i \in A} w_{i}-B\right\}$.

This problem, as well as generalizations with other penalty costs for overweight, have been studied for instance in [4, 2] (see there for practical motivations). In an online learning setting, we assume that the capacity of the knapsack and the profit of objects may change over time (while weights $w_{i}$ and a the constant $c$ are static).Since the problem is not linear, we use the the generalized FTPL (GFTPL) framework of Dudik et al. [9], which uses a much more complex perturbation mechanism than the "extra" random observation in FTPL.

Theorem 6. There is a polynomial time vanishing regret algorithm for GKP.
The proof is based on showing that (1) the GFTPL method can be applied to GKP (see appendix B.3) and (2) there exists a FPTAS for the multi-instance oracle (see appendix B.4). Then Theorem 5 applies.

The proof of Theorem 6 also leads to an interesting observation; although we were able to acquire a vanishing regret algorithm for GKP which is non-linear, we were "forced" to solve a more general problem (that of the convex knapsack) in order to get the desired oracle. This indicates that the nonlinear nature of the problem lead to a significant increase in its online-learning version, offering some useful insight towards question $(Q 2)$.

# 3.1 Computational hardness results 

To the best of our knowledge, up to now FTL algorithms for non-linear objective functions require an exact or a FPTAS oracle in order to obtain vanishing regret. Thus, strong $N P$-hardness for the multiple instance version of the offline problem indicates that the FTL strategy cannot be used for the online problem, at least with our current knowledge.

As we mentioned, algorithms that use the "Follow the Leader" strategy heavily rely on the existence of an optimization oracle for the multi-instance version of the offline problem. For linear objectives, it is easy to see ([18]) that optimization over a set of instances is equivalent to optimization over a single instance and thus any algorithm for the offline problem can be transformed to an online learning algorithm. However, for non-linear problems this assumption is not always justified since even when the offline problem is polytime-solvable, the corresponding variation with multiple instances can be strongly $N P$-hard.

In this section we present some problems where we can prove that the optimum solution over a set of instances is hard to approximate. More precisely, in the multi-instance version of a given problem, we are given an integer $N>0$, a set of feasible solutions $\mathcal{X}$, and $N$ objective functions $f_{1}, \ldots, f_{N}$ over $\mathcal{X}$. The goal is to minimize (over $\mathcal{X}$ ) $\sum_{i=1}^{N} f_{i}(x)$.

We will show computational hardness results for the multi-instance versions of:

- min-max vertex cover (already defined).
- min-max perfect matching, where we are given an undirected graph $G(V, E)$ and a weight function $w: E \rightarrow \mathbb{R}^{+}$on the edges and we need to determine a perfect matching such that the weight of the heaviest edge on the matching is minimized.
- min-max path, where we are given an undirected graph $G(V, E)$, two vertices $s$ and $t$, and a weight function $w: E \rightarrow \mathbb{R}^{+}$on the edges and we need to determine an $s-t$ path such that the weight of the heaviest edge in the path is minimized.
- $P 3 \| C \max$, where we are given 3 identical parallel machines, a set of $n$-jobs $J=\left\{j_{1}, \ldots, j_{n}\right\}$ and processing times $p: J \rightarrow \mathbb{R}^{+}$and we need to determine a schedule of the jobs to the machines (without preemption) such that the makespan, i.e. the time that elapses until the last job is processed, is minimized.

Hence, in the multi-instance versions of these problems, we are given $N$ weight functions over vertices (min-max vertex cover) or edges (min-max perfect matching, min-max path), or $N$ processing time vectors $(P 3 \| C \max$ ).Theorem 7. The multi-instance versions of min-max perfect matching, min-max path, min-max vertex cover and $P 3 \| C \max$ are strongly $N P$-hard.

Proof. Let us present the proof for the multi-instance version of the min-max perfect matching and the min-max path problems, which use a similar reduction from the Max-3-DNF problem. Due to lack of space, the proof for the two other problems are in Appendix C.

In the Max-3-DNF problem, we are given a set of $n$ boolean variables $X=\left\{x_{1}, \ldots, x_{n}\right\}$ and $m$ clauses $C_{1}, \ldots, C_{m}$ that are conjunctions of three variables in $X$ or their negations and we need to determine a truth assignment $\sigma: X \rightarrow\{T, F\}$ such that the number of satisfied clauses is maximized. We start with the multi-instance min-max perfect matching problem. For every instance $\mathcal{I}$ of the Max-3-DNF problem we construct a graph $G(V, E)$ and $m$ weight functions defined as follows:

- To each variable $x_{i}$ is associated a 4-cycle on vertices $\left(u_{i}, u_{i}^{t}, \overline{u_{i}}, u_{i}^{f}\right)$. This 4-cycle has two perfect matchings: either $u_{i}$ is matched with $u_{i}^{t}$ and $\overline{u_{i}}$ is matched with $u_{i}^{f}$, corresponding to setting the variable $x_{i}$ to true, or vice-versa, corresponding to setting $x_{i}$ to false. This specifies a one-to-one correspondence between the solutions of the two problems.
- Each weight function corresponds to one conjunction: $w^{j}\left(u_{i} u_{i}^{t}\right)=1$ if $\neg x_{i} \in C_{j}$, otherwise $w^{j}\left(u_{i} u_{i}^{t}\right)=0$. Edges incident to vertices $\overline{u_{i}}$ always get weight 0 .

The above construction can obviously be done in polynomial time to the size of the input. It remains to show the correlation between the objective values of these solutions. If a clause $C_{j}$ is satisfied by a truth assignment $\sigma$ then (since it is a conjunction) every literal on the clause must be satisfied. From the construction of the instance $\mathcal{I}^{\prime}$ of multi-instance min-max matching, the corresponding matching $M_{\sigma}$ will have a maximum weight of 0 for the weight function $w^{j}$. If a clause $C_{j}$ is not satisfied by a truth assignment, then the corresponding matching $M_{\sigma}$ will have a maximum weight of 1 for the weight function $w^{j}$. Thus, we get: $\operatorname{val}(\mathcal{I}, \sigma)=m-\operatorname{val}\left(\mathcal{I}^{\prime}, M_{\sigma}\right)$, where val stands for the value of a solution. This equation already proves the hardness result of Theorem 7. It actually also shows $A P X$-hardness. Indeed, the optimal value OPT of Max-3-DNF verifies $\frac{m}{8} \leq O P T \leq m$. Assuming the existence of a $(1+\epsilon)$ approximation algorithm for multi-instance min-max perfect matching problem, we can get a $(1-7 \epsilon)$ approximation algorithm for Max-3-DNF. Since Max-3-DNF is $A P X$-Hard, multi-instance min-max perfect matching is also $A P X$-Hard.

A similar reduction leads to the same result for the min-max path problem: starting from an instance of 3-DNF, build a graph $G$ where $V=\left\{v_{0}, v_{1}, \ldots, v_{n}\right\}$. Vertex $v_{i}$ corresponds to variable $x_{i}$ There are two arcs $e_{i}^{t}$ and $e_{i}^{f}$ between $v_{i-1}$ and $v_{i}$. We are looking for $v_{0}-v_{n}$ paths. Taking edge $e_{i}^{t}$ (resp. $e_{i}^{f}$ ) corresponds to setting $x_{i}$ to true (resp. false). As previously this gives a one-to-one correspondence between solutions. Each clause corresponds to one weight function: if $x_{i} \in C_{j}$ then $w^{j}\left(e_{i}^{f}\right)=1$, if $\neg x_{i} \in C_{j}$ then $w^{j}\left(e_{i}^{t}\right)=1$. All other weights are 0 . Then for a $v_{0}-v_{n}$ path $P, w^{j}(P)=0$ if and only if $C_{j}$ is satisfied by the corresponding truth assignment. The remainder of the proof is exactly the same as the one of min-max perfect matching.

Theorem 7 gives insight on the hardness of non-linear multi-instance problems compared to their single-instance counterparts. As we proved, the multi-instance $P 3 \| C \max$ is strongly NP-Hard while $P 3 \| C \max$ is known to admit a FPTAS [23, 26]. Also, the multi-instance version of min-max perfect matching, min-max path and min-max vertex cover are proved to be $A P X$-Hard while their singleinstance versions can be solved in polynomial time. We also note that these hardness results hold for the very specific case where weights/processing times are in $\{0,1\}$, for which $P \| C \max$, as well as the other problems, become trivial. This shows that non-linearity indeed plays an important role to the computational hardness of an online-learning problem, partially answering question $(Q 2)$ addressed in the introduction.We also note that the inapproximability bound we acquired for the multi-instance min-max vertex cover under UGC is tight, since we can formulate the problem as a linear program, solve its continuous relaxation and then use a rounding algorithm to get a vertex cover of cost at most twice the optimum for the problem. The results on the min-max vertex cover problem also provide some answer to question $(Q 2)$ addressed in the introduction. As we proved in Section 2.2, the online gradient descent method (paired with a rounding algorithm) suffices to give a vanishing 2-regret algorithm for online min-max vertex cover. However, since the multi-instance version of the problem is APX-hard there is no indication that the FTL approach can be used to get the same result and match the lower bound of Corollary 2.

# 4 Conclusion 

In the paper, we have presented a general framework showing the hardness of online learning algorithms for min-max problems. We have also showed a sharp separation between two widely-studied online learning algorithms, online gradient descent and FTL, from the approximation and computational complexity aspects. The paper gives rise to several interesting directions. A first one is to extend the reduction framework to objectives other than min-max. A second direction is to design online vanishing regret algorithms with approximation ratio matched to the lower bound guarantee.

## References

[1] Naman Agarwal, Alon Gonen, and Elad Hazan. Learning in non-convex games with an optimization oracle. In Proc. 32nd Conference on Learning Theory, volume 99, pages 18-29, 2019.
[2] Antonios Antoniadis, Chien-Chung Huang, Sebastian Ott, and José Verschae. How to pack your items when you have to buy your knapsack. In Proc. Mathematical Foundations of Computer Science, pages 62-73, 2013.
[3] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of Computer and System Sciences, 74(1):97 - 114, 2008.
[4] Siddharth Barman, Seeun Umboh, Shuchi Chawla, and David L. Malec. Secretary problems with convex costs. In Proc. 39th Colloquium on Automata, Languages, and Programming, pages 75-87, 2012.
[5] Oded Berman and Gabriel Y. Handler. Optimal minimax path of a single service unit on a network to nonservice destinations. Transportation Science, 2(21):115—122, 1987.
[6] Paolo M. Camerini. The min-max spanning tree problem and some extensions. Inf. Process. Lett., 7(1):10-14, 1978.
[7] N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in secondprice auctions. IEEE Transactions on Information Theory, 61(1):549-564, Jan 2015.
[8] Constantinos Daskalakis and Vasilis Syrgkanis. Learning in auctions: Regret is hard, envy is easy. In Proc. 57th Symposium on Foundations of Computer Science, pages 219-228, 2016.
[9] Miroslav Dudik, Nika Haghtalab, Haipeng Luo, Robert E Schapire, Vasilis Syrgkanis, and Jennifer Wortman Vaughan. Oracle-efficient online learning and auction design. In Proc. 58th Symposium on Foundations of Computer Science (FOCS), pages 528-539, 2017.
[10] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.[11] Harold N. Gabow and Robert Endre Tarjan. Algorithms for two bottleneck optimization problems. J. Algorithms, 9(3):411-417, 1988.
[12] Dan Garber. Efficient online linear optimization with approximation algorithms. In Advances in Neural Information Processing Systems, pages 627-635, 2017.
[13] James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games, 3:97-139, 1957.
[14] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157-325, 2016.
[15] Elad Hazan and Satyen Kale. Online submodular minimization. Journal of Machine Learning Research, 13:2903-2922, 2012.
[16] Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In Proc. 48th Symposium on Theory of Computing, pages 128-141, 2016.
[17] Sham M Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation algorithms. SIAM Journal on Computing, 39(3):1088-1106, 2009.
[18] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291 - 307, 2005.
[19] Subhash Khot and Oded Regev. Vertex cover might be hard to approximate to within 2-epsilon. J. Comput. Syst. Sci., 74(3):335-349, 2008.
[20] Subhash Khot, Dor Minzer, and Muli Safra. Pseudorandom sets in grassmann graph have nearperfect expansion. In Proc. 59th Symposium on Foundations of Computer Science, pages 592-601, 2018.
[21] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212-261, 1994.
[22] Ran Raz and Shmuel Safra. A sub-constant error-probability low-degree test, and a sub-constant error-probability PCP characterization of NP. In Proc. 29th Symposium on the Theory of Computing, pages 475-484, 1997.
[23] Sartaj K. Sahni. Algorithms for scheduling independent tasks. J. ACM, 23(1):116-127, 1976.
[24] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107-194, 2012.
[25] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. Efficient algorithms for adversarial contextual learning. In International Conference on Machine Learning, pages 2159-2168, 2016.
[26] Gerhard J. Woeginger. When does a dynamic programming formulation guarantee the existence of a fully polynomial time approximation scheme (FPTAS)? INFORMS J. on Computing, 12(1), January 2000.
[27] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc. 10th International Conference on Machine Learning, pages 928-935, 2003.# A Missing proofs in Section 2 

## A. 1 Proof of Corollary 3

Corollary 3. If a cardinality problem $\mathcal{P}$ is $N P$-complete, then there is no vanishing regret algorithm for online min-max- $\mathcal{P}$ unless $N P=R P$.

Proof. We note that the proof of Theorem 1 does not require $A, B$ and $\alpha$ to be constant: they can be functions of the instance, and then we have to check the running time and the probability of error.

For the running time, the algorithm remains polynomial as soon as the $1 /\left(1-\alpha \frac{A}{B}\right)$ is polynomially bounded (so that $T$ remains polynomially bounded in $n$ ).

For the probability of error it is still 0 when $\left|x_{\text {opt }}\right| \geq B n$, and it is at most $1-\frac{A \epsilon}{2 B}$ otherwise.
Then, for a cardinality problem $\mathcal{P}$, if $A=k / n$ and $B=\frac{k+1}{\epsilon}=A+\frac{1}{n}$, then deciding whether $\left|x_{\text {opt }}\right| \leq k$ is the same as deciding whether $\left|x_{\text {opt }}\right| \leq A n$ or $\left|x_{\text {opt }}\right| \geq B n$. By setting $\alpha=1, A=k / n$, $B=\frac{k+1}{n}$ and $\epsilon=\frac{1}{k}$ in the proof of Theorem 1 we get that the algorithm remains polynomial. The probability of error (when $\left|x_{\text {opt }}\right| \leq A n$ ) is at most $1-\frac{1}{2(k+1)} \leq 1-\frac{1}{2(n+1)}$. It is well known that this is sufficient to show membership in RP.

## A. 2 Proof of Theorem 4

Theorem 4 Assume that $W=\max _{1 \leq t \leq T} \max _{1 \leq i \leq n} w_{i}^{t}$ is the maximum assigned weight. Then, after $T$ time steps, Algorithm 2 achieves

$$
\sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} X_{i}^{t} \leq 2 \cdot \min _{X^{*} \in \mathcal{X}} \sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} X_{i}^{*}+3 W \sqrt{n T}
$$

Proof. By the OGD algorithm (see [27] or [14, Chapter 3]), we have

$$
\sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} x_{i}^{t} \leq \min _{x^{*} \in \mathcal{Q}} \sum_{t=1}^{t} \max _{1 \leq i \leq n} w_{i}^{t} x_{i}^{*}+\frac{3 D G}{2} \sqrt{T}
$$

where $D$ is the diameter of $\mathcal{Q}$ (i.e. $D \leq \sqrt{n}$ ) and $G$ is the Lipschitz constant of the cost vectors $g^{t}$ (i.e. $G \leq W)$. Moreover, by the rounding procedure it always holds that

$$
\max _{i=1, \ldots, n} X_{i}^{t} w_{i}^{t} \leq 2 \max _{i=1, \ldots, n} x_{i}^{t} w_{i}^{t}
$$

Combining these inequalities, the theorem follows.

## B A polynomial time vanishing regret algorithm for GKP (Theorem 6)

## B. 1 Generalized Follow the Perturbed Leader

For the sake of completeness, we introduce the generalized FTPL (GFTPL) method of Dudik et al. [9], which can be used to achieve a vanishing regret for non linear objective functions for some discrete problems. The key idea of the GFTPL algorithm is to use common randomness for every feasible action but apply it in a different way. This concept was referred by the authors of [9] as shared randomness. In their algorithm, the regularization term $R(x)$ of the FTPL algorithm is substituted by the inner product $\Gamma_{x} \cdot a$ where $a$ is a random vector and $\Gamma_{x}$ is a vector corresponding to the action $x$. In FTPL it was sufficient to have $\Gamma_{x}=x$ but in this general setting, $\Gamma_{x}$ must be the row of a translation matrix that corresponds to action $x$.Definition 4 (Admissible Matrix [9]). A matrix $\Gamma$ is admissible if its rows are distinct. It is $(\kappa, \delta)$ admissible if it is admissible and also (i) the number of distinct elements within each column is at most $\kappa$ and (ii) the distinct elements within each column differ by at least $\delta$.

Definition 5 (Translation Matrix [9]). A translation matrix $\Gamma$ is a $(\kappa, \delta)$-admissible matrix with $|\mathcal{X}|$ rows and $N$-columns. Since the number of rows is equal to the number of feasible actions, we denote as $\Gamma_{x}$ the row corresponding to action $x \in \mathcal{X}$. In the general case, $\Gamma \in\left[\gamma_{m}, \gamma_{M}\right]^{\mathcal{X} \times N}$ and $G_{\gamma}=\gamma_{M}-\gamma_{m}$ is used to denote the diameter of the translation matrix.

From the definition of the translation matrix it becomes clear that the action space $\mathcal{X}$ needs to be finite. Note that the number of feasible actions can be exponential to the input size, since we do not need to directly compute the translation matrix. The generalized FTPL algorithm for a maximization problem is presented in algorithmic box 3. At time $t$, the algorithm decides the perturbed leader as the action that maximizes the total payoff on the observed history plus some noise that is given by the inner product of $\Gamma_{x}$ and the perturbation vector $\alpha$. Note that in [9] the algorithm only needs an oracle with an additive error $\epsilon$. We will see later that it works also for a multiplicative error $\epsilon$ (more precisely, for an FPTAS).

```
Algorithm 3: Generalized FTPL algorithm
    Data: A \((\kappa, \delta)\)-admissible translation matrix \(\Gamma \in\left[\gamma_{m}, \gamma_{M}\right]^{\mathcal{X} \times N}\), perturbation parameter \(\eta\),
            optimization parameter \(\epsilon\).
1 Draw \(a\) randomly from hypercube $[0, \eta]^{N}$.
2 for \(t=1,2, \ldots, T\) do
3 Decide \(x^{t}\) such that \(\forall x \in \mathcal{X}\)
                            \(\sum_{\tau=1}^{t-1} f\left(x^{t}, y^{\tau}\right)+a \cdot \Gamma_{x^{t}} \geq \sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+a \cdot \Gamma_{x}-\epsilon\)
4 Observe \(y^{t}\) and gain payoff \(f\left(x^{t}, y^{t}\right)\).
5 end
```

Let us denote $G_{f}$ as the diameter of the objective function, i.e., $G_{f}=\max _{x, x^{\prime} \in \mathcal{X}, y, y^{\prime} \in \mathcal{Y}}|f(x, y)-$ $f\left(x^{\prime}, y^{\prime}\right) \mid$.

Theorem 8 ([9]). By using an appropriate $\eta$ to draw the random vector, the regret of the generalized FTPL algorithm is:

$$
R_{T} \leq N \sqrt{\kappa G_{f} G_{\gamma} \frac{G_{f}+2 \epsilon}{\delta} T}+\epsilon T
$$

By setting $\epsilon=\Theta(1 / \sqrt{T})$, this clearly gives a vanishing regret.
Let us quote two difficulties to use this algorithm. First, if the multi-instance version is $N P$-hard, having an efficient algorithm solving the oracle with an additive error $\epsilon$ is quite improbable. In theorem 5 we remark that the assumption of an additive error $\epsilon$ can be replaced by the assumption of the existence of a FPTAS for the oracle. This is proven formally in B.2. Second, the oracle has to solve a problem where the objective function is the sum of a multi-instance version of the offline problem and the perturbation. We will see in Appendix B. 3 how we can implement the perturbation mechanism $\Gamma_{x} \cdot \alpha$ as the payoff of action $x$ on a set of (random) observations of the problem.# B. 2 Extending the GFTPL algorithm for FPTAS oracles (Theorem 5) 

Recall that the GFTPL algorithm (algorithmic box 3) assumed the existence of an oracle that could return an optimal action over the observed perturbed history within any constant additive error $\epsilon$ in polynomial time (both to the input and $\epsilon$ ). We will show that this is equivalent to assuming the existence of a FPTAS for the multi-instance perturbed offline problem. Namely, let us consider a modification of Algorithm 3 where at at each time $t$ we compute a solution $x^{t}$ such that $\forall x \in \mathcal{X}$ :

$$
\sum_{\tau=1}^{t-1} f\left(x^{t}, y^{\tau}\right)+a \cdot \Gamma_{x^{t}} \geq\left(1-\epsilon^{\prime}\right)\left(\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+a \cdot \Gamma_{x}\right)
$$

Then, if we use $F_{M}$ to denote the maximum payoff, i.e., $F_{M}=\max _{x \in \mathcal{X}, y \in \mathcal{Y}} f(x, y)$, by applying the same analysis as in [9], we can show that by fixing $\epsilon^{\prime}=\frac{\epsilon}{T F_{M}+N \eta \Gamma_{M}}$ we are guaranteed to get an action that has at least the same total perturbed payoff of decision $x^{t}$ if an additive optimization parameter $\epsilon$ was used. The computation is polynomial if we use an FPTAS. Then, we can still get a vanishing regret by using $\epsilon^{\prime}=O\left(T^{-\frac{3}{2}}\right)$ instead of $\epsilon=O\left(T^{-\frac{1}{2}}\right)$ (considering all parameters of the problem as constants).

Thus, we can achieve a vanishing regret for any online learning problem in our setting by assuming access to an oracle OPT that can compute (for any $\epsilon^{\prime}$ ) in polynomial time a decision $x^{t}$ satisfying Equation (1).

## B. 3 Distinguisher sets and a translation matrix for GKP

As noted above, an important issue in the method arises from the perturbation. Until now, the translation matrix $\Gamma$ could be any $(\kappa, \delta)$-admissible matrix as long as it had one distinct row for every feasible action in $\mathcal{X}$. However, this matrix has to be considered by the oracle in order to decide $x^{t}$. In [9] the authors introduce the concept of implementability that overcomes this problem. We present a simplified version of this property.

Definition 6 (Distinguisher Set). A distinguisher set for an offline problem $P$ is a set of instances $S=$ $\left\{y_{1}, y_{2}, \ldots, y_{N}\right\} \in \mathcal{Y}^{N}$ such that for any feasible actions $x, x^{\prime} \in \mathcal{X}$ :

$$
x \neq x^{\prime} \Leftrightarrow \exists j \in[N]: f\left(x, y_{j}\right) \neq f\left(x^{\prime}, y_{j}\right)
$$

This means that $S$ in a set of instances that "forces" any two different actions to differentiate in at least one of their payoffs over the instances in $S$. If we can determine such a set, then we can construct a translation matrix $\Gamma$ that significantly simplifies our assumptions on the oracle.

Let $S=\left\{y_{1}, y_{2}, \ldots, y_{N}\right\}$ be a distinguisher set for our problem. Then, for every feasible action $x \in \mathcal{X}$ we can construct the corresponding row of $\Gamma$ such that:

$$
\Gamma_{x}=\left[f\left(x, y_{1}\right), f\left(x, y_{2}\right), \ldots, f\left(x, y_{N}\right)\right]
$$

Since $S$ is a distinguisher set, the translation matrix $\Gamma$ is guaranteed to be admissible. Furthermore, according to the set we can always determine some $\kappa$ and $\delta$ parameters for the translation matrix. By implementing $\Gamma$ using a distinguisher set, the expression we need to (approximately) maximize at each round can be written as:

$$
\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+\alpha \Gamma_{x}=\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+\sum_{i=1}^{N} a_{i} f\left(x, y_{i}\right)
$$

This shows that the perturbations transform into a set of weighted instances, were the weights $a_{i}$ are randomly drawn from uniform distribution $[0, \eta]$. This is already a significant improvement, since nowthe oracle has to consider only weighted instances of the offline problem and not the arbitrary perturbation $\alpha \Gamma_{x}$ we were assuming until now. Furthermore, for a variety of problems (including GKP), we can construct a distinguisher set $y_{1}, \ldots, y_{N}$ such that:

$$
a f\left(x, y_{j}\right)=f\left(x, a y_{j}\right) \forall a \in \mathbb{R}, j \in[N]
$$

If this is true, then we can shift the random weights of the oracle inside the instances:

$$
\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+\alpha \Gamma_{x}=\sum_{\tau=1}^{t-1} f\left(x, y^{\tau}\right)+\sum_{i=1}^{N} f\left(x, a_{i} y_{i}\right)
$$

Thus, if we have a distinguisher set for a given problem, to apply GFTPL all we need is an FPTAS for optimizing the total payoff over a set of weighted instances.

We now provide a distinguisher set for the generalized knapsack problem. Consider a set of $n$ instances $\left(p_{j}, B_{j}\right)$ of the problem such that in instance $\left(p_{j}, B_{j}\right)$ item $j$ has profit $P$, all other items have profit 0 and the knapsack capacity is $B_{j}=W_{s}$. Since the total weight of a set of items can never exceed $W_{s}$, it is easy to see that $\forall x \in \mathcal{X}$ :

$$
f\left(x, p_{j}, B_{j}\right)= \begin{cases}P & \text { if item } j \text { is selected in set } x \\ 0 & \text { otherwise }\end{cases}
$$

For any two different assignments $x$ and $x^{\prime}$, there is at least one item $j \in[n]$ that they don't have in common. It is easy to see that in the corresponding instance $\left(y_{j}, B_{j}\right)$ one of the assignments will have total profit $P$ and the other will have total profit 0 . Thus, the proposed set of instances is indeed a distinguisher set for the generalized knapsack problem. We use this set of instances to implement the $\Gamma$ matrix. Then, every column of $\Gamma$ will have exactly 2 distinct values 0 and $P$, making the translation matrix $(2, P)$-admissible. As a result, in order to achieve a vanishing regret for online learning GKP, all we need is an FPTAS for the multi-instance generalized knapsack problem.

# B. 4 An FPTAS for the multi-instance version of GKP 

To get an FPTAS, we show that we can map a set of instances of the generalized knapsack problem to a single instance of the more general convex-generalized knapsack problem. Suppose that we have a set of $m$ instances $\left(p^{t}, B^{t}\right)$ of GKP. Then, the total profit of every item set $x \in \mathcal{X}$ is:

$$
\operatorname{profit}(x)=\sum_{t=1}^{m}\left(x \cdot p^{t}-c \max \left\{0, w \cdot x-B^{t}\right\}\right)=x \cdot p_{s}-c k\left(x \mid B^{1}, \ldots, B^{m}\right)
$$

where $p_{s}=\sum_{t=1}^{m} p^{t}$ and $k\left(x \mid B^{1}, \ldots, B^{m}\right)=\sum_{t=1}^{m} \max \left\{0, w \cdot x-B^{t}\right\}$. Let $W=w \cdot x$ the total weight of the item set and $\overrightarrow{B^{1}}, \ldots, \overrightarrow{B^{m}}$ a non-decreasing ordering of the knapsack capacities. Then:

$$
k\left(x \mid B^{1}, \ldots, B^{m}\right)=k\left(W \mid \overrightarrow{B_{1}}, \ldots, \overrightarrow{B^{m}}\right) \begin{cases}0 & , W \leq \overrightarrow{B^{1}} \\ W-\overrightarrow{B^{1}} & , \overrightarrow{B^{1}}<W \leq \overrightarrow{B^{2}} \\ 2 W-\left(\overrightarrow{B^{1}}+\overrightarrow{B^{2}}\right) & , \overrightarrow{B^{2}}<W \leq \overrightarrow{B^{3}} \\ \vdots & \\ m W-\left(\overrightarrow{B^{1}}+\overrightarrow{B^{2}}+\cdots+\overrightarrow{B^{m}}\right) & , \overrightarrow{B^{m}}<W\end{cases}
$$

Note that the above function is always convex. This means that at every time step $t$, we need a FPTAS for the maximization problem $x \cdot p-f(W)$ where $f$ is a convex function. We know that such an FPTAS exists ([2]). In this paper, the authors suggest a FPTAS with time complexity $O\left(n^{3} / \epsilon^{2}\right)$ by assuming that the convex function can be computed at constant time. In our case the convex function $k$ is part of the input; with binary search we can compute it in logarithmic time.# C Proof of Theorem 7 

Theorem 7. The multi-instance versions of min-max perfect matching, min-max path, min-max vertex cover and $P 3 \| C \max$ are strongly $N P$-hard.

To complete the proof, we consider here the min-max vertex cover and $P 3 \| C \max$ problems.

## min-max vertex cover

We make a straightforward reduction from the vertex cover problem. Consider any instance $G(V, E)$ of the vertex cover problem, with $V=\left\{v_{1}, \ldots, v_{n}\right\}$. We construct $n$ weight functions $w^{1}, \ldots, w^{n}$ : $V \rightarrow \mathbb{R}^{+}$such that in $w^{i}$ vertex $v_{i}$ has weight 1 and all other vertices have weight 0 . If we consider the instance of the multi-instance min-max vertex cover with graph $G(V, E)$ and weight functions $w^{1}, \ldots, w^{n}$, it is clear that any vertex cover has total cost that is equal to its size, since for any vertex $v_{i} \in V$ there is exactly one weight function where $w^{i}=1$ and $w^{i}=0$ for every other weight function.

Since vertex cover is strongly $N P$-hard, $N P$-hard to approximate within ratio $\sqrt{2}-\epsilon$ and UGC-hard to approximate within ratio $2-\epsilon$, the same negative results hold for the multi-instance min-max vertex cover problem.

## P3||Cmax

We prove that the multi-instance $P 3 \| C \max$ problem is strongly $N P$-hard even when the processing times are in $\{0,1\}$, using a reduction from the $N P$-complete 3-coloring problem. In the 3-coloring (3C) problem, we are given a graph $G(V, E)$ and we need to decide whether there exists a coloring of its vertices with 3 colors such that if two vertices are connected by an edge, they cannot have the same color.

For every instance $G(V, E)$ of the 3C problem with $|V|=n$ and $|E|=m$, we construct (in polynomial time) an instance of the multi-instance $P 3 \| C \max$ with $n$-jobs and $N=m$ processing time vectors. Every edge $(i, j) \in E$ corresponds to a processing time vector with jobs $i$ and $j$ having processing time 1 and every other job having processing time 0 . It is easy to see that at each time step the makespan is either 1 or 2 and thus the total makespan is at least $m$ and at most $2 m$.

If there exists a 3-coloring on $G$ then by assigning every color to a machine, at each time step there will not be two jobs with non-zero processing time in the same machine and thus the makespan will be 1 and the total solution will have cost $m$. If the total solution has cost $m$ then this means that at every time step the makespan was 1 and by assigning to the jobs of every machine the same color we get a 3 coloring of $G$. Hence, the multi-instance variation of the $P 3 \| C \max$ problem is strongly $N P$-hard.