# Online Learning for Approximately-Convex Functions with Long-term Adversarial Constraints  


Dhruv Sarkar IIT Kharagpur, India  


Samrat Mukhopadhyay IIT (ISM) Dhanbad, India  


Abhishek Sinha TIFR Mumbai, India  


## Abstract  


## 1 Introduction  


We study an online learning problem with long- term budget constraints in the adversarial setting. In this problem, at each round \(t\) , the learner selects an action from a convex decision set, after which the adversary reveals a cost function \(f_{t}\) and a resource consumption function \(g_{t}\) . The cost and consumption functions are assumed to be \(\alpha\) - approximately convex â€” a broad class that generalizes convexity and encompasses many common non- convex optimization problems, including DR- submodular maximization, Online Vertex Cover, and Regularized Phase Retrieval. The goal is to design an online algorithm that minimizes cumulative cost over a horizon of length \(T\) while approximately satisfying a long- term budget constraint of \(B_{T}\) . We propose an efficient first- order online algorithm that guarantees \(O(\sqrt{T})\) \(\alpha\) - regret against the optimal fixed feasible benchmark while consuming at most \(O(B_{T}\log T) + O(\sqrt{T})\) resources in both full- information and bandit feedback settings. In the bandit feedback setting, our approach yields an efficient solution for the Adversarial Bandits with Knapsack problem with improved guarantees. We also prove matching lower bounds, demonstrating the tightness of our results. Finally, we characterize the class of \(\alpha\) - approximately convex functions and show that our results apply to a broad family of problems.  


Online Convex Optimization (OCO) with long- term constraints has emerged as a fundamental framework for modeling sequential decision- making while satisfying time- varying and uncertain constraints (Mansour et al., 2009, Guo et al., 2022, Immorlica et al., 2022, Sinha and Vaze, 2024). This problem can be formulated as a sequential game played between a learner and an adversary over a horizon of length \(T\) . Specifically, a learner with a long- term resource budget \(B_{T}\) selects an action \(x_{t}\) at round \(t \in [T]\) from a convex decision set \(\mathcal{X}\) . Consequently, at the \(t^{\text{th}}\) round, the learner incurs a cost of \(f_{t}(x_{t})\) and consumes \(g_{t}(x_{t}) \geq 0\) amount of resources. The cost and consumption functions may also non- convex and can be chosen adversarially. The goal is to minimize the cumulative cost over the horizon, i.e., \(\sum_{t = 1}^{T} f_{t}(x_{t})\) , while approximately satisfying the given budget constraint \(\sum_{t = 1}^{T} g_{t}(x_{t}) \leq B_{T}\) .  


Constrained learning problems arise in a variety of practical settings, including online resource allocation, dynamic pricing (Besbes and Zeevi, 2009), online and markets (Sivikumar, 2013, Liakopoulos et al., 2019), learning with fairness and safety constraints (Sun et al., 2017, Sinha, 2023), and Bandits with Knapsacks (Immorlica et al., 2022), and online packing problems (Mehta et al., 2013, Agarwal and Devanur, 2014). Significant progress has been made in the setting where the cost and consumption functions are convex and the static comparator satisfies per- round constraints (Sinha and Vaze, 2024, Guo et al., 2022, Mahdavi et al., 2012). However, the non- convex setting with long- term budget constraints presents substantial computational and algorithmic challenges. Please refer to Section 2 for the state- of- the- art results on this problem. In this paper, we simultaneously address the challenges of the non- convexity of the cost and constraints, along with satisfying the long- term budget constraints in an adversarial setup. Our contributions are twofold:  


1. Characterizing \(\alpha\) -approximately Convex functions: We focus on the setting where both

<--- Page Split (0) --->

the cost and constraint functions belong to the class of \(\alpha\) - approximately convex functions [Pedramdar and Aggarwal, 2023]. In Section 3.1, we motivate our study by showing that the objective function for some foundational problems, such as Online Vertex Cover, belongs to the category. In Theorem 3, we provide several necessary and sufficient characterizations of this class using convex conjugate theory. Using this result, in Appendix A.3 and A.4, we demonstrate that DR- submodular functions (which arises as a continuous extension of submodular function) and the objective function of the Regularized Phase Retrieval problem belongs to this class.  


2. Learning with Long-term Budget Constraints: We propose an efficient first- order online learning algorithm, which guarantees a sublinear \(\alpha\) - regret while approximately satisfying a given long-term budget constraint of \(B_{T}\) , when the cost and constraint functions are \(\alpha\) - approximately convex. These results improve the state- of- the- art even in the cases setting where \(\alpha = 1\) . The regret of the proposed algorithm is measured against the best fixed offline action in hindsight that satisfies the budget constraint over the entire horizon of length \(T\) . This must be contrasted with a weaker benchmark, commonly used in the literature, which is required to remain feasible in _every_ round [Sinai and Vaze, 2024, Guo et al., 2022, Yi et al., 2021]. These results are obtained by reducing the constrained learning problem to an instance of standard Online Linear Optimization (OLO) problem, and showing that the proposed algorithm achieves \(O(\sqrt{T} )\) \(\alpha\) - regret while consuming at most \(O(\log T)B_{T} + O(\sqrt{T})\) resources. We do not make any assumptions on Salter's condition. In Section 6, we establish converses results that show that the above performance bounds are tight. Our proposed algorithms generalize almost immediately to the difficult feedback setting, described in Appendix D.  


This paper is organized as follows. Section 2 reviews the related work to contextualize our contribution within the existing literature. Section 3 formally defines the problem and motivates it via the Online Vertex Cover problem. Section 4 introduces the class of \(\alpha\) - approximately convex functions and provides several equivalent characterizations. Our main algorithm is presented in Section 5, followed by lower bounds in Section 6. We conclude the paper in Section 7.  


## 2 Related Work  


Online Learning with Per- Round Constraints:  


Online learning with long- term constraints was first studied by Mannor et al. [2009]. In the context of two- player infinite- horizon stochastic games, they established a fundamental impossibility result: it is not possible to simultaneously achieve sublinear bounds for both regret as well as cumulative constraint violation (CCV) against the best fixed offline action that satisfies the long- term constraint over the entire horizon. This negative result motivated subsequent works to consider a weaker benchmark, in which the benchmark satisfies the budget constraints at _every_ round [Makhdoumi et al., 2012, Neely and Yu, 2017, Guo et al., 2022]. The goal in this line of work is to obtain the tightest regret and CCV bounds. For the linear constraint, Mahdavi et al. [2012] were the first to use Online Gradient Descent (OGD) and mirror procedure online policies to obtain sublinear regret and sublinear CCV guarantees. Later, Castiglioni et al. [2022] proposed a unified meta- algorithm that achieves \(O(T^{5 / 4})\) bounds for both approximate regret and CCV in the non- convex setting with long- term constraints. However, their results rely on Salter's condition and the assumption that the constraint functions vary no faster than \(O(T^{1 - \alpha})\) . Guo et al. [2022] studied the setting with adversarial constraints, without assuming Salter's condition, and achieved \(O(\sqrt{T})\) regret and \(O(T^{2 / 3})\) CCV. The closest related work is Sinha and Vaze [2024], which achieves optimal \(O(\sqrt{T})\) regret and \(O(\sqrt{T})\) CCV guarantees in the convex setting by reducing the problem to standard online convex optimization. However, all of the above works compare against a weaker fixed- offline benchmark that is required to remain feasible at _every_ round. In addition to being too restrictive, it is possible that there does not exist any fixed action that satisfies all \(T\) (potentially adversarially chosen) constraints simultaneously, leading to vacuous guarantees.  


Online Learning with Long- Term Constraints: Online learning with long- term constraints have also been considered in the literature, where the goal is to establish tight competitive ratio guarantees [Immerlohs et al., 2022, Slivkins, 2013, Badanidiyuru et al., 2018, Rivera Cardoso et al., 2023]. In this problem, known as Bandits with Knapsacks (BwK), we have \(K\) arms and \(d\) resources. Corresponding to each pull of the arm, the algorithm incurs some loss and some of the resources get consumed where the total consumption of each resource is limited by a budget \(B\) . The problem is to find the best possible regret guarantee within the budget constraint. Naturally, the algorithm stops at time horizon \(T\) , or when the total consumption of some resource exceeds its budget.  


Closely related to our work is the paper by Immerlohs et al. [2022]. The authors studied the BwK prob

<--- Page Split (1) --->

lem in the adversarial setting where both the cost and consumption vectors are chosen by an oblivious adversary. The authors showed that it is impossible to achieve sublinear regret guarantees for this problem. Thus, they instead try to get meaningful guarantees on the competitive ratio. In this setting, they designed a primal- dual based online algorithm that achieves \(O(\log T)\) competitive ratio. However, in addition to assume existence of a \(NLLR\) arm (defined to be an arm with zero cost and consumption at all rounds) and running two different regret minimizers (for each of the primal and dual problems), their multi- phase algorithm needs to guess the value of the optimal benchmark on an exponential scale, which further adds to its complexity. Furthermore, a major limitation of their theoretical result is that their regret bound becomes much worse unless the budget \(B_{T}\) is at least \(\Omega (\sqrt{T})\) . On the contrary, our first- order and efficient algorithm yields near- optimal guarantees for any budget, including \(B_{T} = 0\) , without any further assumptions (see Appendix D).  


The work of Immorlica et al. [2022] was further extended by Castiglioni et al. [2022a] where they designed a primal- dual based algorithm for both stochastic and adversarial setting in the regime where the long- term budget scales linearly with the time- horizon, i.e., \(B_{T} = \Omega (T)\) . They showed that, their algorithm achieves a constant competitive ratio in this regime. Striai et al. [2022] considers the setting where the learner is equipped with a \(genius\) plan, which is essentially a budget allocation profile that prescribes how much of each resource may be consumed at each round. They design a primal- dual based algorithm that achieves sublinear regret of \(\tilde{\mathcal{O}} ((\rho_{\min}^{- 1} + \sqrt{T}))\) , where \(\rho_{\min}\) is the Slater constant. In the worst case, they design a meta- algorithm that guarantees \(\tilde{\mathcal{O}} (T^{3 / 2})\) regret, even when the spending plan is highly imbalanced. Raut et al. [2021] considered the infinite DR- submodular maximization problem with long- term constraints where the constraints are assumed to be linear and stochastic. A common limitation of primal- dual- based algorithms, e.g., the ones proposed by Castiglioni et al. [2022a] and Straizl et al. [2025], is that they assume Slater's condition. This assumption results in bounds which depends inversely on the Slater's constant, resulting in vacuous bounds when the Slater's constant is arbitrarily small. In contrast, we do not make any assumptions on the allocated budget \(B_{T}\) or the spending plan, or assume Slater's condition.  


Online Non- Convex Optimization: Online nonconvex optimization has garnered increasing attention in recent years. Agarwal et al. [2019] showed that online learning with adversarial losses is computationally intractable in the absence of structural assumptions. Setting aside computational challenges, Sugioka and Netrapalli [2020] showed that the Follow- the- Perturbed- Leader policy (FTPL) can attain sublinear regret for non- convex losses when equipped with an optimization oracle. However, their approach relies on a strong assumption: the existence of an offline oracle capable of approximately solving non- convex optimization problems. The online non- convex learning problem has also been studied under structural assumptions, such as the Odyssey- Lojasiewicz condition by Mkhitaryan- Kemp [2022] and the weak pseudo- convexity condition by Cao et al. [2018]. Closely related to our work, the paper by Potelmariu and Aggarwal [2025] introduced the notion of upper- linearizable functions and proved \(\alpha\) - regret guarantees for the same. The class of \(\alpha\) - convex functions introduces in this paper coincides with the negative of upper- linearizable functions and extends the class of concave functions. We allow both the cost and constraint functions to be \(\alpha\) - approximately convex functions, which strictly generalize convex functions and encompass a wide range of practical relevancy objects such as phase retrieval and DR- submodular optimization.  


ally intractable in the absence of structural assumptions. Setting aside computational challenges, Sugioka and Netropalli [2020] showed that the Follow- the- Perturbed- Leader policy (FTPL) can attain sublinear regret for non- convex losses when equipped with an optimization oracle. However,t their approach relies on a strong assumption: the existence of an offline oracle capable of approximately solving non convex optimization problems. The online non- convex learning problem has also been studied under structural assumptions,such as the Odyssey- Lojasiewicz condition by Mkhitaryan- Kemp [2022]and the weak pseudo convexity condition by Cao et al. [2018]. Closely related to our work, they presented a sublinear regret bound [2025] introduced the notion of upper- linearizable functions and proved \(\alpha\) - regrete guarantees for the same. The class of \(\alpha\) - convex functions introduces in this paper coincides withe the negative of upper- linearizable functions and extends the class of concave functions. We allow both th cost and constraint functions to be \(\alpha\) - approximately convex functions, which strictly generalize convex functions an d encompass a wide range of practical relevancy objects such as phase retrieval and DR- submodular optimizatio n.  


## 3 Problem Formulation  


Consider the following repeated game between a learner and an adversary played for \(T\) rounds. At each round \(t\) , the learner chooses an action \(x_{t}\) from an admissible set \(\mathcal{X} \subseteq \mathbb{R}^{d}\) for some \(d \geq 1\) . The set \(\mathcal{X}\) is assumed to be non- empty, closed, and convex with a finite Euclidean diameter of \(D\) . Upon observing the action \(x_{t}\) , the adversary chooses two nonnegative functions \(a\) - cost functions \(f_{t} : \mathcal{X} \to \mathbb{R}_{+}\) and a resource- consumption function \(c(a, k, x_{- k})\) , a constant function \(g_{t} : \mathcal{X} \to \mathbb{R}_{+}\) . The cost and constraint functions are assumed to belong to the class of \(\alpha\) - approximately convex functions, which will be defined and characterized in Section 4. The consumption \(g_{t}(a, x_{- k})\) denotes the amount of resources consumed on round \(t\) due to the action \(x_{t}\) . The allosteric resource consumption budget for the entire horizon of length \(T\) is specified to be \(B_{T}\) . The performance of any online algorithm is characterized by comparing its cumulative cost against that of a fixed feasible action \(x^{*} \in \mathcal{X}\) which satisfies the long- term budget constraint \(\sum_{t = 1}^{T} g_{t}(a^{*}, x_{- k}) \leq B_{T}\) . Since we allow the cost and consumption functions to be nonconvex, we use the \(\alpha\) - regret as the performance metric [Chen et al., 2018], which generalizes the notion of the static regret [Hazan, 2022]. In the \(\alpha\) - regret metric, we use a potentially weaker benchmark by scaling up its cumulative cost by a factor of \(\alpha \geq 1\) . Specifically, let \(\mathcal{X}^{*} \subseteq \mathcal{X}\) be the (non- empty) subset of all fixed actions

<--- Page Split (2) --->

satisfying the long- term constraint, i.e.,  


\[\mathcal{X}^{*} = \{x\in \mathcal{X}:\sum_{t = 1}^{T}g_{t}(x)\leq B_{T}\} . \quad (1)\]  


Assuming the feasible set to be non- empty, we define the \(\alpha\) - Regret and the Cumulative Consumption (CC) of any algorithm as follows:  


\[\mathrm{Regret}_T(\alpha) = \sup_{x^* \in \mathcal{X}^*}\sum_{t = 1}^{T}\left(f_t(x_t) - \alpha f_t(x^*)\right), \quad (2)\]  


Our objective is to simultaneously upper bound the \(\alpha\) Regret (2) and the CC (3) for a suitably chosen small value of \(\alpha\) . For \(\alpha = 1\) , the \(\alpha\) - regret reduces to the standard (static) regret.  


Remarks: As mentioned in Section 2, previous works on the COCO problem considered round waste feasibility with a restricted benchmark which is required to incur zero constraint violation in every round, i.e., \(g_{t}(x^{*}) = 0,\forall 1\leq t\leq T\) . Apart from being severely restrictive, a more serious issue with this assumption is that the feasible set may be empty, i.e., \(\cap_{t = 1}^{T}\{x^{*}:\ g_{t}(x^{*}) = 0\} = \emptyset\) , resulting in unavoidable losses. In this paper, we avoid this restrictive assumption by requiring the offline benchmark to satisfy the budget constraint only over the entire horizon of length \(T\) . Note that by setting \(B_{T} = 0\) , and using the non-negativity of the consumption function, we recover the instantaneous feasibility condition as above. In this setting, the cumulative consumption (CC) metric is known as Cumulative Constraint Violation (Cov) [Sinha and Vaze, 2024, Guo et al., 2022].  


Secondly, unlike motivation of [22], which assumes the existence of a BULA action with zero cost and zero consumption, we only assume the feasible set \(\mathcal{X}^{*}\) to be non- empty, which is necessary to make the problem well- defined. Although the above formulation considers only a single resource, extension of our algorithm to multiple resources is straightforward; see Appendix C. In the following, we describe the Online Vertex Cover problem, which concretely illustrates various components of the above problem.  


### 3.1 A Motivating Example: The Online Vertex Cover Problem  


Consider a sequence of graphs defined on a fixed set of \(n\) vertices \(V\) , with time- varying vertex prices \(\{\alpha_{t}\}_{t\geq 1}\) and time- varying edges \(\{E_{t}\}_{t\geq 1}\) . A learner and an adversary play the following repeated game on this sequence of graphs. At each round, the learner selects  


a subset of vertices, and, at the same time, the vertex prices and the current edges of the graph are chosen by the adversary. The goal of the learner is to select a subset of vertices on each round to maximize the total number of edges covered over a horizon of length \(T\) with a given long- term budget constraint \(B_{T}\) . Specifically, in every round \(t\geq 1\) , assume that the learner randomly selects a subset of vertices, denoted by the indicator variables \(\boldsymbol {X}_t\in \{0,1\} ^T\) . Simultaneously, the adversary reveals the current set of edges \(E_{t}\) and the current prices for the vertices \(c_{t}:V\to \mathbb{R}_{+}^{n}\) . Hence, on round \(t\) , the learner pays an expected price of  


\[C_{t} = \mathbb{E}\sum_{i\in V}c_{t,i}X_{t,i}, \quad (4)\]  


and receives an expected reward equal to the expected number of edges covered, i.e.,  


\[\begin{array}{rcl}{R_{t}} & = & {\mathbb{E}\sum_{(i,j)\in E_{t}}\max (X_{t,i},X_{t,j})}\\ {} & = & {\sum_{(i,j)\in E_{t}}\mathbb{P}(X_{t,i} = 1\vee X_{t,j} = 1),} \end{array} \quad (5)\]  


end- points were not able to be covered if either of its endpoints are selected at the learner. In the above, the expectations are taken with respect to the randomness of the policy. We emphasize the fact that, unlike the classical Minimum Vertex Cover problem, in the online version, the learner selects the vertices on round \(t\) _without observing the current edges \(E_{t}\) or the current prices \(c_{t}\) . Since the classical offline variant of the vertex cover problem, where the graph is revealed _a priori_, is well- known to be NP- hard [Garey and Johnson, 2002], we instead seek approximate solutions in the online setting.  


Towards this end, we consider a class of randomized policies where, on round \(t\) , vertex \(i\) is independently selected with probability \(x_{t,i}\) \(i\in V\) . Thus the decision set \(X\) is given by the hypercube \([0,1]^{n}\) . The goal is to design a sequence of inclusions probability vectors \(\{x_{t}\}_{t\geq 1}\) to maximize the cumulative rewards subject to the long- term budget constraints.  


For any randomized policy, the probability that an edge \((i,j)\) is covered on round \(t\) is given by:  


\[\begin{array}{rcl}\mathbb{P}(X_{t,i} = 1\vee X_{t,j} = \mathbf{1}) & = & 1 - \mathbb{P}(X_{t,i} = 0\wedge X_{t,j} = 0)\\ & = & 1 - (1 - x_{t,i})(1 - x_{t,j})\\ & = & x_{t,i} + x_{t,j} - x_{t,i}x_{t,j}\\ & \geq & \frac{1}{2} (x_{t,i} + x_{t,j}), \end{array} \quad (6)\]  


where in the last inequality, we have used the fact that  


\[\frac{1}{2} (x_{t,i} + x_{t,j})\stackrel {(a)}{=}\frac{1}{2} (x_{t,i}^{2} + x_{t,j}^{2}),\quad x_{t,i}x_{t,j},\mathrm{~where~}x_{t,i}\mathrm{~and~}x_{t,j}\mathrm{~are~independent.}\]

<--- Page Split (3) --->

equality (a) holds because \(0 \leq x_{t,i} \leq 1, \forall i, i\) . Furthermore, using the union bound, we have  


\[\mathbb{P}(X_{t,i} = 1 \vee X_{t,j} = 1) \leq \mathbb{P}(X_{t,i} = 1) + \mathbb{P}(X_{t,j} = 1) = x_{t,i} + x_{t,j}. \quad (7)\]  


Clearly, the cost (4) and the reward (5) are functions of the inclusion probability vector \(\mathbf{x}\) . Using the linearity of expectation, while the cost \(C_{t} = \sum_{i} c_{t,i} x_{t,i}\) is linear, the reward function \(R_{t}(x_{t}) = \sum_{(i,j) \in E_{t}} \mathbb{P}(X_{t,i} = 1 \vee X_{t,j} = \mathbf{1}) = \sum_{(i,j) \in E_{t}} (x_{t,i} + x_{t,j} - x_{t,i} x_{t,j})\) is non- linear and monotonic in the decision variable \(x_{t}\) . Nevertheless, from Eqns. (6) and (7), it follows that the function \(R_{t}(x_{t})\) satisfies the following inequality for any \(x_{t}, u_{t} \in \mathcal{X}\) , which generalizes the first- order condition for concavity:  


\[R_{t}(x_{t}) - \frac{1}{2} R_{t}(u_{t})\geq\] \[\frac{1}{2}\sum_{(i,j)\in E_{t}}\{(x_{t,i} + x_{t,j}) - (u_{t,i} + u_{t,j})\}\] \[= (\mathrm{Diag}_{x_{t}}x_{t} - u_{t}),\]  


where \(\mathrm{Diag}_{x_{t}}\) denotes the degree of vertex \(i\) on round \(t\) . Inequality (8) motivates our definition of the class of approximately convex (equivalently, concave) functions given in the following section.  


## 4 The Class of \(\alpha\) -Approximately Convex Functions  


Definition 1. The class of \(\alpha\) - approximately convex functions \((\alpha \geq 1)\) , denoted by \(\mathcal{L}_{\alpha}\) , is defined to be the family of non- negative real- valued functions defined on a convex domain \(\mathcal{X}\) such that for any point \(x \in \mathcal{X}\) , there exists a vector \(H(x)\) , called a generalized subgradient at \(x\) , so that the following inequality, which we call \(\alpha\) - approximation convexity, holds uniformly for any \(x, u \in \mathcal{X}\) :  


\[f(x) \leq \alpha f(u) + \langle H(x), x - u \rangle , \quad \forall f \in \mathcal{L}_{\alpha}. \quad (9)\]  


We analogously define \(\alpha\) - approximately concave functions with \(0 \leq \alpha \leq 1\) , where the function of the inequality (9) is reversed (see, e.g., Eqn. (8)). Clearly, with \(\alpha = 1\) , the class \(\mathcal{L}_{\alpha}\) includes the class of all nonnegative convex functions where \(H(x)\) can be taken to be a sub- gradient at \(x\) . Under standard assumptions, we can also bound the norms of the generalized subgradients (please refer to Lemma 1 in the Appendix).  


As we show in Appendix A.3, the class \(\mathcal{L}_{\alpha}\) appears in several common non- convex optimization problems, including weakly DR- submodular maximization, regularized phase retrieval, and online vertex cover. The class of \(\alpha\) - approximately convex functions was first  


introduced in an equivalent form by Pedramfar and Aggarwal [2025], who called it the class of upper- linearizable functions.  


Remarks: It is interesting to note that, in sharp contrast with convex functions, even if an \(\alpha\) - approximately convex (concave) function is differentiable, its gradient need not correspond to a generalized sub- gradient. For example, in the online vertex cover example in Section 3.1, the \(t^{\mathrm{th}}\) reward function \(R_{t}(x) \equiv \sum_{(i,j) \in E_{t}} (x_{t,i} + \frac{1}{2} x_{t,j} - x_{t,i} x_{t,j})\) is differentiable and \(\beta\) - approximability concave. Yet its gradient does not correspond to a generalized sub- gradient.  


The following is an immediate consequence of Definition (9).  


Proposition 2. The class \(\mathcal{L}_{\alpha}\) is closed under nonnegative linear combinations.  


See Appendix A.1 for the proof.  


Recall that the Fenchel conjugate \(f^{*} : \mathbb{R}^{n} \to \mathbb{R}\) of a function \(f : \mathcal{X} \to \mathbb{R}\) is defined as  


\[f^{*}(y) = \sup \left\{(y, x) - f(x)\right\} .\]  


Being a pointwise supremum of a family of affine functions, the function \(f^{*}\) is convex [Boyd and Vandenberghe, 2004]. The bicontiguity of \(f\) is defined to be the Fenchel conjugate of the function \(f^{*}\) . The following theorem gives equivalent characterizations for the class of \(\alpha\) - approximately convex functions.  


Theorem 3. Let \(f : \mathcal{X} \to \mathbb{R}\) , \(u\) be a non- negative function and \(\alpha \geq 1\) . Then the following statements are equivalent:  


1. \(f\) is a \(\alpha\) - approximately convex. 
2. The bicontiguity of the function \(f\) satisfies, \(f(x) \leq \alpha f^{*}(x)\) , \(\forall x \in \mathcal{X}\) . Since for any function \(f^{*}(x) \leq f(x)\) , the function \(f\) is sandwiched between \(f^{*}\) and \(\alpha f^{*}\) pointwise, i.e.,  


\[f^{*}(x) \leq f(x) \leq \alpha f^{*}(x), \quad \forall x \in \mathcal{X}.\]  


3. There exists a non- negative convex function \(g: \mathcal{X} \to \mathbb{R}_{+}\) such that \(g(x) \leq f(x) \leq \alpha g(x)\) for all \(x \in \mathcal{X}\) .  


(A) PROPOSITION 1. [JENKIN'S INEQUALITY] For any set of \(N\) points \(\{x_{i}\}_{i = 1}^{N}\) , all from the set \(\mathcal{X}\) , and any probability distribution \(p\) on these \(N\) points, the following approximate version of the Jensen's inequality holds:  


\[f\left(\sum_{i}p_{i}x_{i}\right) \leq \alpha \sum_{i}p_{i}f(x_{i}).\]  


The proof of Theorem 3 is given in Appendix A.2. Theorem 3 is useful for establishing \(\alpha\) - approximate

<--- Page Split (4) --->

convexity for many useful non- convex functions. See Appendix A.4 for an example involving the phase retrieval problem.  


## 5 Online Learning with Budget Constraints  


In this Section, we propose an online policy for the constrained learning problem introduced in Section 3 with \(\alpha\) - approximately convex cost and constraint functions (the case of \(\alpha\) - approximately concave functions can be treated similarly). As stated earlier, we benchmark our online policy against the best fixed action in hindsight satisfying the long- term budget constraint (Eqn. (1)).  


The Regret Decomposition Inequality: Let \(Q(t)\) be the amount of resources consumed up to round \(t\) , i.e.,  


\[Q(t) = Q(t - 1) + g_{t}(x_{t}),\quad Q(0) = 0. \quad (10)\]  


Let \(\Phi (\cdot)\) be a non- decreasing and convex Lyapunov function. The increase of the value of the Lyapunov function from round \(t - 1\) to \(t\) can be upper bounded as follows:  


\[\Phi (Q(t)) - \Phi (Q(t - 1))\quad \stackrel {(a)}{\leq}\quad \Phi '(Q(t))(Q(t) - Q(t - 1))\] \[\qquad \stackrel {(b)}{=}\quad \Phi '(Q(t))g_{t}(x_{t}),\]  


where in step (a) we have used the convexity of the function \(\Phi (\cdot)\) and in (b), we have used Eqn. (10). Let \(x^{*}\in \mathcal{X}^{*}\) be any fixed action from the feasible set (1), adding the term \(V(f_{t}(x_{t}) - \alpha_{t}f(x^{*}))\) to both sides of the above inequality, we obtain:  


\[\begin{array}{r l} & {\Phi (Q(t)) - \Phi (Q(t - 1)) + V(f_{t}(x_{t}) - \alpha_{t}f(x^{*}))}\\ & {\leq \quad (Vf_{t}(x_{t}) + \Phi '(Q(t))g_{t}(x_{t})) - \alpha (Vf_{t}(x^{*}) + }\\ & {\quad \Phi '(Q(t))g_{t}(x^{*})) + \alpha \Phi '(Q(t))g_{t}(x^{*}),} \end{array} \quad (11)\]  


where we have added and subtracted the term \(\alpha \Phi '(Q(t))g_{t}(x^{*})\) . Define the surrogate cost function \(\tilde{f}_{t}:\mathcal{X}\mapsto \mathbb{R}_{+}\) for round \(t\) as follows:  


\[\tilde{f}_{t} = Vf_{t} + \Phi '(Q(t))g_{t},\quad t\geq 1, \quad (12)\]  


Since both \(f_{t}\) and \(g_{t}\) are \(\alpha\) - approximately convex and the Lyapunov function \(\Phi (\cdot)\) is non- decreasing, from Proposition 2, it follows that the surrogate cost function \(\tilde{f}_{t}\) is also \(\alpha\) - approximately convex. Summing up the inequalities (11) for \(1\leq t\leq T\) , we obtain the following Regret Decomposition inequality:  


\[\begin{array}{r l} & {\Phi (Q(T)) - \Phi (Q(0)) + V\mathrm{Regret}_{T}(\alpha)}\\ & {\stackrel {(a)}{\leq} \mathrm{Regret}_{T}^{i}(\alpha) + \alpha \Phi '(Q(T))\sum_{t = 1}^{T}g_{t}(x^{*}),} \end{array} \quad (13)\]  


\[\stackrel {(b)}{\leq} \mathrm{Regret}_{T}^{i}(\alpha) + \alpha \Psi '(Q(T))B_{T}, \quad (13)\]  


where \(\mathrm{Regret}_{T}^{i}(\alpha)\) and \(\mathrm{Regret}_{T}^{i}(\alpha)\) respectively denote the \(\alpha\) - regrets for learning the original cost functions \(\{f_{t}\}_{t\geq 1}\) and the surrogate cost functions \(\{\tilde{f}_{t}\}_{t\geq 1}\) w.r.t. the feasible action \(x^{*}\) (see Eqn. (2) for the definition of \(\alpha\) - regret). In step (a) above, we have used the monotonicity of the sequence \(\{Q(t)\}_{t\geq 1}\) and the convexity of the Lyapunov function \(\Phi (\cdot)\) , and in step (b), we have used the fact that the offline benchmark \(x^{*}\) satisfies the long- term budget constraint of \(B_{T}\) .  


### 5.1 Algorithm Design and Analysis  


As mentioned above, the surrogate cost function \(\tilde{f}_{t}\) (12), is non- negative and \(\alpha\) - approximately convex. Let \(H_{t}(x)\) and \(H_{g_{t}}(x)\) be generalized subgradients at \(x\) for \(f_{t}\) and \(g_{t}\) respectively. Then, as in the proof of Proposition 2, the vector \(H_{f_{t}}(x)\) defined as  


\[H_{f_{t}}(x)\equiv VH_{f_{t}}(x) + \Phi '(Q(t))H_{g_{t}}(x) \quad (14)\]  


is a generalized subgradient for the surrogate cost function \(\tilde{f}_{t}\) , i.e., we have  


\[\tilde{f}_{t}(x_{t}) - \alpha f_{t}(x^{*})\leq \langle H_{f_{t}}(x_{t}),x_{t}\rangle -\langle H_{f_{t}}(x_{t}),x^{*}\rangle . \quad (15)\]  


Summing up inequalities (15) for \(1\leq t\leq T\) , we conclude that the \(\alpha\) - regret for the surrogate costs is upper bounded as:  


\[\mathrm{Regret}_{T}^{i}(\alpha)\leq \mathrm{Regret}_{T}^{j}, \quad (16)\]  


where \(\mathrm{Regret}_{T}^{i,j}\) is the standard regret \((\alpha = 1)\) of the surrogate Online Linear Optimization (OLO) problem where the cost function \(f_{t}:\mathcal{X}\mapsto \mathbb{R}_{+}\) on round \(t\) is defined to be:  


\[\tilde{f}_{t}(x) = \langle H_{f_{t}}(x_{t}),x\rangle ,1\leq t\leq T. \quad (17)\]  


Combining Eqns. (13) and (47), we obtain the following inequality, which constitutes the key to the subsequent analysis:  


\[\begin{array}{r l} & {\Phi (Q(T)) - \Psi (Q(0)) + V\mathrm{Regret}_{T}(\alpha)}\\ & {\qquad \leq \mathrm{Regret}_{T}^{i} + \alpha \Psi '(Q(T))B_{T}.} \end{array} \quad (18)\]  


Eqn. (18) suggests that in order to control both \(Q(T)\) and \(\mathrm{Regret}_{T}(\alpha)\) , which are open on the LHS of (18), we can minimize the regret of the corresponding OLO problem, which appear in the upper bound in the inequality (18).  


Standard online policies for the OLO problem, such as Online Gradient Descent [Hazan, 2022], require a uniform upper bound on the norms of the cost function gradients. Since the norm of the gradient of the surrogate OLO problem \(\| H_{f_{t}}(x)\|\) scale with \(Q(t)\) (an

<--- Page Split (5) --->

algorithm- dependent variable), it can not be upper bounded at the beginning of the game. Thus we use an adaptive learning policy, such as ADAGRAD, which does not need us to specify the scale of the gradients, yet achieves near- optimal bounds. Algorithm 1 describes our proposed online learning policy. Theorem  


Algorithm 1 Online policy for \(\alpha\) - approximately convex functions with constraints  


1: Inputs: Convex decision set \(\mathcal{X}\) with a finite Euclidean diameter \(D\) . Random projection operator \(\mathrm{Proj}_{\mathcal{X}}(\cdot)\) on the set \(\mathcal{X}\) , sequence of \(\alpha\) - approximately convex cost functions \(\{f_{t}\}_{t\geq 1}\) , and consumption functions \(\{g_{t}\}_{t\geq 1}\) . Budget \(B_{T}\) , parameters \(V,A\) .  


2: Initialize \(x_{1}\in \mathcal{X}\) arbitrarily 
3: for \(t = 1:T\) do  


4: Play \(x_{t}\) : compute \(H_{f_{t}}(x_{t})\) , and \(H_{g_{t}}(x_{t})\) . 
5: Compute \(H_{f_{t}}(x_{t})\) as follows:  


\[H_{f_{t}}(x_{t})\equiv VH_{f_{t}}(x_{t}) + \Phi^{\prime}(Q(t))H_{g_{t}}(x_{t})\]  


6: Use the ADAGRAD step sizes:  


\[\eta_{t}\leftarrow \frac{\sqrt{2}D}{2\sqrt{\sum_{t = 1}^{T}\|H_{f_{t}}(x_{t})\|^{2}}}.\]  


7: Compute the next action \(x_{t + 1}\) using Online Gradient Descent with step size \(\eta_{t}\) :  


\[x_{t + 1}\leftarrow \mathrm{Proj}_{\mathcal{X}}(x_{t} - \eta_{t}H_{f_{t}}(x_{t})).\]  


## 8: end for  


4 constitutes the main result of this paper. Theorem 4. Consider the constrained online learning problem described in Section 3 with a sequence of \(\alpha\) - approximately convex cost and constraint functions and a long- term budget of \(B_{T}\) . Assume that the generalized subgradients of all cost and constraint functions are upper bounded by \(\alpha G\) for some \(G > 0\) . Then, Algorithm 1, with \(\Phi (x) = \exp (\lambda x)\) , \(\lambda = \frac{1}{2}(\alpha GD\sqrt{2t} +\alpha B_{T})^{- 1}\) , \(V = (\alpha GD)^{- 1}\) , achieves near- optimal \(\alpha\) - regret while consuming close to the allocated budget. Specifically,  


\[Regret_{t}(\alpha) = O(\alpha \sqrt{T}),C C_{T} = \tilde{O} (\alpha B_{T} + G D\sqrt{T}).\]  


The proof of Theorem 4 is given in Section 5.2.  


Remarks: In case of bandit feedback (a.k.a. the Adversarial Bandits with Knapsacks (BwK) problem in the literature), we replace the full- information- based ADAGRAD sub- routine with an adaptive bandit algorithm and use a power- law Lyapunov function for  


technical reasons. Due to space constraints, the details are deferred to Appendix D. Note that, the competitive ratio bound for the BwK problem, given by Immorlica et al. [2022, Theorem 5.1], becomes various unless the budget is at least \(\tilde{\Omega} (\sqrt{T})\) [Immorlica et al., 2022, Remark 5.2]. On the other hand, Theorem 4 and Theorem 10 give non- trivial regret and cumulative consumption bounds for any arbitrary budget \(B_{T}\geq 0\) in the full- information and bandit feedback settings respectively. Furthermore, compared to Immorlica et al. [2022], Castiglioni et al. [2022a], which run two different regret- minimizers - one for the primal and the other for the dual, our primal- only algorithm with a single regret minimizer is computationally efficient. Finally, we do not make any assumption on the Slater condition [Castiglioni et al., 2022a] or the existence of a NULL arm [Immorlica et al., 2022].  


### 5.2 Proof of Theorem 4  


The norm of the gradients of the surrogate OLO cost functions (17) can be upper bounded as follows:  


\[\begin{array}{rcl}\| H_{f_{t}}(x_{t})\|_{2} & \stackrel {\mathrm{(a)}}{\leq} & V\| H_{f_{t}}(x_{t})\|_{2} + \Phi^{\prime}(Q(t))\| H_{g_{t}}(x_{t})\|_{2}\\ & & \stackrel {\mathrm{(b)}}{\leq} & \alpha G(V + \Phi^{\prime}(Q(T))). \end{array} \quad (19)\]  


where (a) follows from using the triangle inequality in Eqn. (14), and (b) follows from the assumption that the norm of the generalized sub- gradients are uniformly bounded by \(\alpha G\) for some \(G > 0\) . Using the adaptive regret bound of the ADAGRAD sub- routine, given by Theorem 5 in Appendix A.6, we have the following upper bound for the standard regret of the surrogate OLO problem:  


\[\mathrm{Regret}_{t}^{G}\leq \sqrt{2G} D\alpha (V + \Phi^{\prime}(Q(T)))\sqrt{T}. \quad (20)\]  


Hence Eqn. (18) yields:  


\[\begin{array}{r}\Phi (Q(T)) + V\mathrm{Regret}_{T}(\alpha)\leq \Phi (Q(0)) + \alpha VG D\sqrt{2T} +\alpha \Phi^{\prime}(Q(T))(GD\sqrt{2T} +B_{T}). \end{array} \quad (21)\]  


We now choose \(\Phi (\cdot)\) to be the exponential Lyapunov function \(\Phi (x) = \exp (\lambda x)\) , where the parameter \(\lambda\) will be fixed below. With this choice for \(\Phi (\cdot)\) , we have  


\[\begin{array}{r}\exp (\lambda Q(T)) + V\mathrm{Regret}_{T}(\alpha)\leq 1 + \alpha VGD\sqrt{2T} +\\ \lambda \alpha \exp (\lambda Q(T))(GD\sqrt{2T} +B_{T}). \end{array} \quad \quad (21)\]  


We now choose the free parameters to be \(\lambda = \frac{1}{\alpha} (GD\sqrt{2T} +B_{T})^{- 1}\) and \(V = (\alpha GD)^{- 1}\) . Hence, the inequality above simplifies to:  


\[\frac{1}{2}\exp (\lambda Q(T)) + (\alpha GD)^{-1}\mathrm{Regret}_{T}(\alpha)\leq 1 + \sqrt{2T}. \quad (22)\]  


The regret and CC bounds follow upon solving the above inequality.

<--- Page Split (6) --->

**Regret Bound:** Using the fact that \(\exp (\lambda Q(T))\geq\) \(\exp (\lambda Q(0))\geq 1\) , Eqn. (22) yields


\[\text {Regret}_{T}(\alpha )\leq \alpha GD\sqrt {2T}+\frac {\alpha }{2}GD,\quad T\geq 1.\]


**CC** **Bound:** Let \(F\) denote the maxi-mum value of \(f_{t}\) over the decision set, i.e., \(F=\max _{1\leq t\leq T}\max _{x\in \mathcal {X}}f_{t}(x)\) . Then, using the non-negativity of the cost functions, we have


\[f_{t}(x_{t})-\alpha f_{t}(x^{*})\geq 0-\alpha F.\]


This implies that \(\text {Regret}_{T}(\alpha )\geq -\alpha F\) . Hence, from Eqn. (22), we have for any \(T\geq 1:\) 


\[\exp (\lambda Q(T))\leq 2(1+FT/GD+\sqrt {2T}).\]


Hence, the total resource consumption over the hori-zon is bounded as:


\[Q(T)\leq \lambda ^{-1}O(\log T)=(\alpha Br+GD\sqrt {T})O(\log T).\]


# 6 Lower bounds


Recall from Theorem 4 that our proposed online policy achieves a cumulative expectation (CQE) bound of \(O(\log T|Br|+O(\sqrt {T})\) . First, setting \(Br=0\)  represents the notion of round-wise feasibility, in which any feasible offline benchmark \((x^{*})\) incurs zero consumption in every round. In this setting, it was previously ex-tablished by Sinha and Vaze [2024, Theorem 3] that the additive \(O(\sqrt {T})\) factor in the CQE bound cannot be improved. In this Section, we further show that the \(O(\log T)\) multiplicative factor in front of \(Br\)  (equi-valently, competitive ratio against any fixed action in hindsight) in the above expression for CQE cannot be improved while maintaining a sublinear regret guarant-e for the cumulative costs. In particular, we demon-strate that this impossibility result holds even when both the cost and consuming functions are linear, i.e., \(\alpha =1\) . For notational simplicity, we state the results in terms of rewards instead of costs.


**Theorem** **6** (Lower bound for the competitive ra-tio). Consider the above constrained learning problem with linear cost and linear consumption functions and a long-term budget of \(Br\)  for a horizon of length \(T\) .Let \(\pi\)  be any online policy and \(\pi ^{*}\)  be a fixed offline op-timal policy in the hindsight that samples actions from a fixed distribution in every round and consumes at most \(Br\)  resources in expectation, thus satisfying the budget constraint. Let \(RBE(\pi ^{*})\)  and \(DPE\)  be the cumu-lative rewards accumulated by \(\pi\)  and \(\pi ^{*}\)  respectively up to round \(T\) . Furthermore, let \(\mathcal {O}_{T}\)  be the cumulative amount of resources consumed by the online policy \(\pi\)  up to round \(T\) . Assume that for some constant \(k>0\) ,


and any \(T\geq 1\) the online policy \(\pi\)  enjoys the following guarantee:


\[DPE_{T}-RBE_{T}(\pi )\leq h(T),\quad \mathcal {O}_{T}(\pi )-\kappa Br\leq s(T),\]


where \(s(T)\)  and \(h(T)\) are some non-negative sublinear functions of the horizon length \(T\) , which do not depend on budget \(Br\) . Then we must have \(\kappa \geq \Omega (\log T)\) .


**Proof** **outline:** Our proof adapts the construction from the lower bound on the competitive ratio for the adversarial Bandits with Knapsacks (BwK) prob-lem [Imamura et al., 2022, Construction 8.7]. At a high-level, the key difference between the adversarial BwK and our settings is that while in the BwK problem,we stop playing as soon as the budget is exhausted (resulting in zero constraint violations), in our prob-lem, we continue playing throughout the entire hori-zon at the expense of violating the prescribed resource constraints. In the following, we show that by appro-priately rescaling the budget, a lower bound for the parameter \(\kappa\)  in Theorem 5 can be obtained from the existing lower bound of the competitive ratio for the adversarial BwK problem.


# 7 Conclusion


In this paper, we introduced a framework for online non-convex optimization with long-term adversarial budget constraints for \(\alpha\) -approximately convex func-tions. We proposed an efficient first-order online policy that guarantees \(O(\sqrt {T})\)  \(\alpha\) -regret while exceeding the budget only by a factor of at most \(O(\log T)\)  in both full-information and bandit settings. We also show that our performance bounds are tight. In the future,it will be interesting to extend the algorithm to more general class of non-convex functions.


# 8 Acknowledgement


AS was supported in part by the Department of Atomic Energy, Government of India, under project no. RTI4001 and in part by a Google India faculty Research Award.

<--- Page Split (7) --->

## References  


Naman Agarwal, Alon Gonen, and Elad Hazan. Learning in non- convex games with an optimization oracle. In Conference on Learning Theory, pages 18- 29. PMLR, 2019. Shipra Agrawal and Nikhil R Devanur. Fast algorithms for online stochastic convex programming. In Proceedings of the twenty- sixth annual ACM- SIAM symposium on Discrete algorithms, pages 1405- 1424. SIAM, 2014. Francis Bach et al. Learning with submodular functions: A convex optimization perspective. Foundations and Trends(r) in machine learning, 6(2- 3): 145- 373, 2015. Aswinikumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Srivatsa. Bandits with knapsacks. Journal of the ACM (JACM), 65(3):1- 55, 2018. Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. Convex analysis and optimization, volume 1. Athena Scientific, 2003. Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk bounds and near- optimal algorithms. Operations research, 57(6): 1407- 1420, 2009. Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning with knapsacks: the best of both worlds. In International Conference on Machine Learning, pages 2767- 2783. PMLR, 2022a. Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Giulia Romana, and Nicola Gatti. A unifying framework for online optimization with long- term constraints. Advances in Neural Information Processing Systems, 35:33589- 33602, 2022b. Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization. In International Conference on Artificial Intelligence and Statistics, pages 1896- 1905. PMLR, 2018. John Duchi, Elad Hazan, and Yuval Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. Xiand Gao, Xiaobo Li, and Shuzhong Zhang. Online learning with non- convex losses and non- stationary regret. In International Conference on Artificial Intelligence and Statistics, pages 235- 243. PMLR, 2018. Michael R Garey and David S Johnson. Computers and intractability, volume 29. wh freeman New York, 2002.  


Hengquan Guo, Xin Liu, Honghao Wei, and Lei Ying. Online convex optimization with hard constraints: Towards the best of two worlds and beyond. Advances in Neural Information Processing Systems, 35:36126- 36439, 2022. Elad Hazan. Introduction to online convex optimization. MIT Press, 2022. Nicole Immorlica, Karthik Sankararaman, Robert Schapire, and Aleksandrs Slivkins. Adversarial bandits with knapsacks. Journal of the ACM, 69(6): 1- 47, 2022. Kishore Jagannathan, Yonina C Eldar, and Babak Hassibi. Phase retrieval: An overview of recent developments. Optical compressive imaging, pages 279- 312, 2016. Nikolaos Liakopoulos. Opsetools Destounis, Georgios Paschos, Thrasyvoulos Spyropoulos, and Panayiotis Mertikopoulos. Cautious regret minimization: Online optimization with long- term budget constraints. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2944- 2952. PMLR, 09- 13 Jun 2019. URL https://proceedings.mlr.press/v97/liakopoulos19a.html. Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for efficiency: online convex optimization with long term constraints. The Journal of Machine Learning Research, 13(1):2503- 2528, 2012. Shie Mannor, John N Tsitsiklis, and Jia Yuan Yu. Online learning with sample path constraints. Journal of Machine Learning Research, 10(3), 2009. Aranyak Mehta et al. Online matching and ad allocation. Foundations and Trends(r) in Theoretical Computer Science, 8(4):265- 368, 2013. Julie Mulvaney- Kemp, SangWoo Park, Ming Jin, and Javad Lavaei. Dynamic regret bounds for constrained online nonconvex optimization based on polyak- lojasiewicz regions. IEEE Transactions on Control of Network Systems, 10(2):599- 611, 2023. Michael J Neely and Hao Yu. Online convex optimization with time- varying constraints. arXiv preprint arXiv:1705.04763, 2017. Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019. Mohammad Pedramfar and Vaneet Aggarwal. From linear to linearizable optimization: a novel framework with applications to stationary and nonstationary di- submodular optimization. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS '24, Red

<--- Page Split (8) --->

Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. Sudeep Raja Putta and Shipra Agrawal. Scale- free adversarial multi armed bandits. In International Conference on Algorithmic Learning Theory, pages 910- 930. PMLR, 2022. Prasanna Raut, Omid Sadeghi, and Maryam Fazel. Online el- submodular maximization: Minimizing regret and constraint violation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9395- 9402, 2021. Adrian Rivera Cachon, He Wang, and Huan Xu. The online saddle point problem and online convex optimization with knapsacks. Mathematics of Operations Research, 40(1):1- 39, 2025. Abhishek Sinha, Bandit- q: Fair Multi- Armed Bandits with Guaranteed Rewards per Arm. arXiv preprint arXiv:2304.03219, 2023. Abhishek Sinha and Rahul Vaze. Optimal algorithms for online convex optimization with adversarial constraints. In The Thirty- eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=Txf7wJNbyJ. Aleksandrs Slivkins. Dynamic all allocation: Bandits with budgets. arXiv preprint arXiv:1306.0155, 2013. Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Christian Kroer. No- regret learning under adversarial resource constraints: A spending plan is all you need! arXiv preprint arXiv:2506.13244, 2025. Arun Sai Suggala and Praneeth Netrapalli. Online non- convex learning: Following the perturbed leader is optimal. In Algorithmic Learning Theory, pages 845- 861, 2020. Wen Sun, Debadeepta Dey, and Ashish Kapoor. Safety- aware algorithms for adversarial contextual bandit. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3280- 3288. PMLR, 06- 11 Aug 2017. URL https://proceedings.mlr.press/v70/sun17a.html. Gang Wang, Georgios B Giannakis, Yousef Saad, and Jie Chen. Phase retrieval via reweighted amplitude flow. IEEE Trans. Signal Process., 66(11):2818- 2833, 2018. Xink Lei, Yuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl Johansson. Regret and cumulative constraint violation analysis for online convex optimization with long term constraints. In International Conference on Machine Learning, pages 11998- 12008. PMLR, 2021. Qixin Zhang, Zengde Deng, Zaiyi Chen, Haoyuan Hu, and Yu Yang. Stochastic continuous submodular maximization: Boosting via non- oblivious function. In International Conference on Machine Learning, pages 26116- 26134. PMLR, 2022.  


strants: A spending plan is all you need! arXiv preprint arXiv:2506.13244,2025. Arun Sai Suggala and Praneeth Netrapalli. Online non- concave learning: Following the perturbed leader is optimal. In Algorithmic Learning Theory, pages 845â€“861, 2020. Wen Sun, Debadeepta Dey, and Ashish Kapor. Safety- aware algorithms for adversarial contextual bandit. In Doina Precup and YeeWhye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 7O of Proceedings of Machine Learning Research, pages 3280â€“3288. PMLR, 06â€“11 Aug 2017. URL https://proceedings.mlr.press/v70/sun1a.html. Gang Wang, Georgios B Giannakis, Yousef Saad, and Ji e Chen. Phase retrieval via reweighted amplitude flow. IEEE Trans. Signal Process., 66 (11):2818â€“ 2833, 2018. Xink Lei, Yuxian Li, Tao Yang,Lihua Xie, Tianyou Chai, and Karl Johansson. Regret and cumulative constraintviolation analysis for online convex optimization with long term constraints. In International Conference on Machine Learning, pages 1 1998â€“12008. PMLR, 2021. Qixin Zhang, Zengde Deng, Zayi Chen, Haoyuan Hu, and Yu Yang. Stochastic continuous submodular maximization:Boosting via non- oblivious function. In International Conference on Machine Learning, pages 26116â€“26134. PMLR, 2022.

<--- Page Split (9) --->

## A Appendix  


## A.1 Proof of Proposition 2  


Suppose we have \(f\in \mathcal{L}_{n}\) and \(g\in \mathcal{L}_{n}\) . Then by definition, we have for all \(x,u\in \mathcal{X}\) :  


\[f(x) - \alpha f(u)\leq \langle H_{f}(x),x - u\rangle \quad (23)\]  


and  


\[g(x) - \alpha g(u)\leq \langle H_{g}(x),x - u\rangle \quad (24)\]  


Let \(h\) be a non- negative linear combination of \(f\) and \(g\) , i.e., \(h = c_{1}f + c_{2}g\) , where \(c_{1},c_{2}\geq 0\) . Then for \(H_{h} = c_{1}H_{f} + c_{2}H_{g}\) the following holds  


\[h(x) - \alpha h(u)\leq \langle H_{h}(x),x - u\rangle \quad (25)\]  


Thus \(h\in \mathcal{L}_{n}\)  


## A.2 Proof of Theorem 3  


(1) \(\Rightarrow\) (2): Since \(f\) is \(\alpha\) -approximately convex, for a given \(x\in \mathcal{X}\)  


\[\begin{array}{r l} & {\exists g^{\prime}\in \mathbb{R}^{n}\mathrm{~s.t.~}f(x)\leq \alpha f(u) + \langle g^{\prime},x - u\rangle ,\forall u\in \mathcal{X}}\\ & {\Leftrightarrow \exists g\in \mathbb{R}^{n}\mathrm{~s.t.~}\frac{f(x)}{\alpha}\leq f(u) + \langle g,x - u\rangle ,\forall u\in \mathcal{X}}\\ & {\Leftrightarrow \frac{f(x)}{\alpha}\leq \sup_{g\in \mathbb{R}^{n}}\inf_{u\in \mathcal{X}}\left(f(u) + \langle g,x - u\rangle\right)}\\ & {\Leftrightarrow \frac{f(x)}{\alpha}\leq \sup_{g\in \mathbb{Z}^{n}}\left(\langle g,x\rangle -f^{*}(g)\right)}\\ & {\Leftrightarrow f(x)\overset {(b)}{\leq}\alpha f^{*}(x),} \end{array} \quad (26)\]  


where in steps (a) and (b), we have used the definition of Fenchel conjugate.  


(2) \(\Rightarrow\) (3): This holds since \(f^{**}\) is a convex function that satisfies \(f^{**}(x)\leq f(x),\forall x\in \mathcal{X}\) [Bertsekas et al., 2003, Proposition 7.1.1].  


(3) \(\Rightarrow\) (1): We have  


\[g(x)\leq f(x)\leq \alpha g(x),\forall x\in \mathcal{X}. \quad (27)\]  


Since \(g\) is convex, for any arbitrary \(u\in \mathcal{X}\) , we have  


\[g(u)\geq g(x) + \langle G(x),u - x\rangle , \quad (27)\]  


where \(G(x)\) is a subgradient of the convex function \(g\) at the point \(x\) . Hence, from the given condition, we have  


\[f(u)\geq g(u)\geq g(x) + \langle G(x),u - x\rangle \geq \frac{f(x)}{\alpha} +\langle G(x),u - x\rangle . \quad (28)\]  


This implies that for all \(x,u\) , we have  


\[f(x) - \alpha f(u)\leq \langle H(x),x - u\rangle , \quad (28)\]  


which shows that the function \(f\) is \(\alpha\) - approximate convex. In the above equation, we have defined \(H(x)\equiv \alpha G(x)\) .  


(3) \(\Rightarrow\) (4): If (3) holds then there is a convex function \(g\) such that \(g(x)\leq f(x)\leq \alpha g(x),\forall x\in \mathcal{X}\) . Then,  


\[f(\sum_{i}p_{i}x_{i})\leq \alpha g(\sum_{i}p_{i}x_{i})\leq \alpha \sum_{i}p_{i}g(x_{i}), \quad (28)\]

<--- Page Split (10) --->

where the last inequality follows from Jensen's inequality.  


(4) \(\Longrightarrow\) (1): Let, for any \(N\geq 1\) , \(p\in \Delta_{N}\) and \(x_{i}\in \mathcal{X},\forall i\in [N]\) . Then, note that \(\langle \sum_{i}p_{i}x_{i},\sum_{i}p_{i}f(x_{i})\rangle \in\) \(\mathrm{co}(\mathrm{epi}(f))\) . Also, by the condition, as \(f(\sum_{i}p_{i}x_{i})\leq \alpha \sum_{i}p_{i}f(x_{i})\) , we have that if \(w\geq \sum_{i}p_{i}f(x_{i})\) , then, \(\alpha w\geq f(\sum_{i}p_{i}x_{i})\) . Consequently, \(\mathrm{co}(\mathrm{epi}(f))\leq \mathrm{cp}(f_{w})\) , where we have defined \(\mathrm{ep}_{w}(f) = \mathrm{ep}_{w}(f\circ x)\) for any \(\mathrm{ep}_{w}(f^{\ast \ast}) = \mathrm{cl}(\mathrm{co}(\mathrm{epi}(f)))\) , we have, \(\mathrm{ep}_{w}(f^{\ast \ast})\leq \mathrm{cl}(\mathrm{ep}_{w}(f))\) . Therefore, for any \(x\in (x,f^{\ast \ast}(x))\in \mathrm{epi}(f^{\ast \ast})\) \(\Rightarrow\) \((x,f^{\ast \ast}(x))\in \mathrm{cl}(\mathrm{ep}_{w}(f))\) . Therefore, there is a sequence \(\{(x_{k},w_{k})\} \in \mathrm{ep}_{w}(f)\) such that \(x_{k}\to x\) and \(w_{k}\to f^{\ast \ast}(x)\) . Since \((x_{k},w_{k})\in \mathrm{ep}_{w}(f)\) , \(f(x_{k})\leq \alpha w_{k}\) . Assuming \(f\) to be a closed function, we have, \(f(x) = \lim_{k}f(x_{k})\leq\) \(\alpha f^{\ast \ast}(x)\) . Then, by (2), \(f\) is \(\alpha\) - approximationly convex.  


## A.3 Approximate Convexity of Weakly DR-submodular functions  


Weakly DR- submodular functions: Consider a product- form decision set \(\mathcal{X} = \prod_{i = 1}^{n}\mathcal{X}_{i}\) where each \(\mathcal{X}_{i}\) is a compact subset of non- negative reals \(\mathbb{R}_{+}\) . For any \((x,y)\in \mathcal{X}\times \mathcal{X}\) we define the partial order \(\leq\) such that \(x\leq y\) iff \(x_{i}\leq y_{i},\forall i\) . We say a differentiable function \(F(\cdot)\) is weakly DR (diminishing return) submodular with parameter \(\gamma\) if we have:  


\[\nabla F(x)\geq \gamma \nabla F(y),\quad \forall (x,y)\in \mathcal{X}\times \mathcal{X},\mathrm{~s.t.~}x\leq y,\]  


This class of functions generalizes the class of differentiable DR submodular functions which have \(\gamma = 1\) [Raut et al., 2021].  


Example An important example of a DR- submodular function is the multilinear extension \(F:[0,1]^{n}\mapsto \mathbb{R}\) of a submodular function \(f:2^{V}\mapsto \mathbb{R}\) defined on the subsets of a ground set \(V\) as below:  


\[F(x) = \sum_{S\subseteq V}\prod_{i\in S}x_{i}\prod_{f\in S}(1 - x_{j})f(S).\]  


In other words, \(F(x)\) is the expectation of the set function \(f(S)\) when the element \(i\) is included in the subset \(S\) independently w.p. \(x_{i},\forall i\) . Some examples of submodular set functions include: the Cut function in a graph, Rank function of a Matroid, Coverage function, Log- determinant function of a positive semidefinite matrix etc. See Bach et al. [2013] for the definition of these functions and an excellent treatment of submodular optimization.  


Theorem 6 below shows that the function \(F\) is \(\frac{1}{1 - \gamma}\) - approximately concave.  


Theorem 6. Let \(F:\mathcal{X}\to \mathbb{R}_{+}\) be a weakly DR- submodular and monotone function with parameter \(\gamma >0\) . Then for any two vectors \(x,y\in \mathcal{X}\) , we have  


\[F(x) - (1 - e^{-\gamma})F(u)\geq \left\langle \nabla \widehat{F} (x),x - u\right\rangle ,\]  


where \(\widehat{F}:\mathcal{X}\to \mathbb{R}_{+}\) is the non- oblivious function corresponding to \(F\) defined as:  


\[\nabla \widehat{F} (x) = \int_{0}^{1}e^{\gamma (z - 1)}\nabla F(xz)dz.\]  


Proof. Lemma 2 of Zhang et al. [2022] states that, for any two vectors \(x,y\in \mathcal{X}\) , we have  


\[\left\langle y - x,\nabla \widehat{F} (x)\right\rangle \geq \gamma \left(\int_{0}^{1}w(z)dz\right)\left(F(y) - \theta (w)F(x)\right),\]  


where the expressions \(w(z)\) and \(\theta (w)\) have been defined in Zhang etal. [2022]. Using expressions of \(w(z)\) and \(\theta (w)\) fromTheorem 1 of Zhang et al. [2022], we obtain the desired result.  


## A.4 Approximate Convexity of Regularized Phase Retrieval  


The Phase Retrieval Problem: Let us consider the problem of \(l_{2}\) - regularized Phase Retrieval (PR), where the problem is estimate an unknown signed vector from the absolute (unsigned) values of its linear measurements [Jaganathan et al., 2016]. The standard approach for the PR problem solves the following optimization problem:  


\[\min_{x\in \mathcal{X}}f(x) = \min_{x\in \mathcal{X}}\frac{1}{2}\| y - |\Phi x|\|^{2} + \frac{\lambda}{2}\| x\|^{2},\]

<--- Page Split (11) --->

where \(\lambda > 0\) is a regularization parameter, \(\Phi \in \mathbb{R}^{m\times n}\) is the measurement matrix and \(y\) is the measurement vector with non- negative co- ordinates and \(\mathcal{X}\) is the constraint set. The objective function \(f(x)\) is known to be non- convex [Wang et al., 2018]. We prove the following:  


Theorem 7. Let \(\| \Phi \|_{2\to 2}\) denote the operator norm of the measurement matrix \(\Phi\) , and let \(\lambda > 0\) in (31). Then \(f\) , defined in (31) is \((1 + 1 / \gamma)\) - approximately convex, where \(\gamma = \frac{2}{\| \Phi \|_{2\to 2}^{2}}\) .  


Proof. In the following, we denote \(\gamma = \frac{2}{\| \Phi \|_{2\to 2}^{2}},\) where \(\| \Phi \|_{2\to 2}\) is the operator norm of \(\Phi\)  


We now define the following candidate function \(g\) which appears in part 3 of Theorem 3:  


\[g(x):= f(x) - \frac{(1 + \gamma)}{2}\left\| \left(\frac{y}{1 + \gamma} -|\Phi x|\right)\right\|^{2}, \quad (32)\]  


where for any vector \(v\) , we define \((v)_{+}\) as the vector with \([(v)_{+}]_{i} = \max \{0,v_{i}\}\) . Clearly \(g(x)\leq f(x)\) . We will now prove that \(g\) is convex.  


To see this, note that we can re- express \(f\) as below:  


\[f(x) = \frac{\|y\|^{2}}{2} -y^{\top}|\Phi x| + \frac{\|{\Phi}x\|^{2}}{2}\left(1 + \frac{\lambda}{\|{\Phi}\|_{2\to 2}^{2}}\right) + \frac{\lambda}{2}\left(\|x\|^{2} - \frac{\|{\Phi}x\|^{2}}{\|{\Phi}\|_{2\to 2}^{2}}\right)\] \[\qquad = \frac{\|y\|^{2}}{2} -y^{\top}|\Phi x|\| \frac{\|{\Phi}x\|^{2}}{2} (1 + \gamma) + \frac{\lambda}{2}\left(\|x\|^{2} - \|{\Phi}x\|^{2}\right)\] \[\qquad = \frac{\|y\|^{2}}{2}\left(1 - \frac{1}{1 + \gamma}\right) + \frac{\|y\|^{2}}{2(1 + \gamma)} -y^{\top}|\Phi x| + \frac{\|{\Phi}x\|^{2}}{\|{\Phi}\|_{2\to 1}^{2}}\left(1 + \gamma\right) + \frac{\lambda}{2}\left(\|x\|^{2} - \left\|{\Phi}x\right\|^{2}\right)\] \[\qquad = \frac{\|y\|^{2}}{2(1 + \gamma)} + \frac{\lambda}{2} x^{\top}\left(I - \frac{\Phi^{\top}\Phi}{\|{\Phi}\|_{2\to 2}^{2}}\right)x + \frac{(1 + \gamma)}{2}\left\| \frac{y}{1 + \gamma} -|\Phi x|\right\|^{2}.\]  


Consequently, we obtain from the definition of \(g\)  


\[g(x) = \frac{\|y\|^{2}}{2(1 + \gamma)} +\frac{\lambda}{2} x^{\top}\left(I - \frac{\Phi^{\top}{\Phi}}{2(1 + \gamma)}\right)x + \frac{(1 + \gamma)}{2}\left\| \left(\left|\Phi x\right| - \frac{y}{1 + \gamma}\right)\right\|_{2}^{2}.\]  


The term \(T_{1}\) is a constant, \(T_{2}\) is convex as the Hessian is \(\lambda \left(I - \frac{\Phi^{\top}\Phi}{2(1 + \gamma)}\right)\) , which is positive semi- definite. The function in \(T_{3}\) can be shown to be convex as below:  


\[\frac{2T_{3}}{1 + \gamma} = \sum_{j = 1}^{m}\left(|\phi_{j}^{\top}x| - \frac{y_{j}}{1 + \gamma}\right)^{2} = \sum_{j = 1}^{m}h_{j}(\phi_{j}^{\top}x),\]  


where \(h_{j}(u) = \left(|u| - \frac{u^{2}}{1 + u}\right)_{+}^{2}, 1\leq j\leq m\) . Since the squared ReLU function is convex, \(h_{j}\) 's are convex, making \(T_{3}\) convex. Consequently, \(g\) is convex.  


To find \(\alpha >1\) such that \(f(x)\leq \alpha g(x)\) , using the expressions of \(f,g\) it therefore suffices to find \(\alpha\) such that  


\[f(x)\leq \alpha \left(f(x) - \frac{(1 + \gamma)}{2}\right)\left(\left(\frac{y}{1 + \gamma} -|\Phi x|\right)_{+}\right)^{2}\Bigg)\] \[\Leftrightarrow \frac{(1 + \gamma)\alpha}{2(\alpha - 1)}\leq \frac{f(x)}{\left\|(\frac{y}{1 + \gamma} -|\Phi x|)_{+}\right\|^{2}}\forall x.\]

<--- Page Split (12) --->

Since the RHS have to be minimized, let us focus on the polyhedron \(\mathcal{C} = \{x:|\Phi x|\leq \frac{\pi}{1+\gamma}\}\) . Then, such an \(\alpha\) can be found if it satisfies the following:  


\[\frac{(1 + \gamma)\alpha}{2(\alpha - 1)}\leq \frac{\frac{\frac{2\gamma}{2(1 + \gamma)} + \frac{\gamma}{2}x^{\top}\left(I - \frac{\frac{2\gamma}{2(1 + \gamma)}}{1 + \gamma} x + \frac{(1 + \gamma)}{2}\right)\left\|\Phi x\right\| - \frac{\pi}{1 + \gamma}\right\|^{2}}{\left\|\frac{\pi}{1 + \gamma} - |\Phi x|\right\|^{2}}, \forall x\in \mathcal{C},\]  


which in turn is satisfied if  


\[\frac{(1 + \gamma)\alpha}{2(\alpha - 1)} \leq \frac{1 + \gamma}{2} + \frac{\frac{\frac{2\gamma}{2(1 + \gamma)}}{2(1 + \gamma)}}{\sum_{j = 1}^{m} \frac{2(1 + \gamma)}{(1 + \gamma)^{2}} (1 - t_{j})^{2}}, t_{j} \in [0, 1], j = 1, 2, \dots , m,\]  


where we define \(t_{j} = \frac{(1 + \gamma)|\phi_{j}^{\top}x|}{y_{j}} \in [0, 1]\) whenever \(x \in \mathcal{C}\) . The above is satisfied if  


\[\frac{(1 + \gamma)\alpha}{2(\alpha - 1) \leq \frac{1 + \gamma}{2} + \frac{\frac{\frac{\frac{2\gamma}{2(1 + \gamma)}}{2(2(1 + \gamma)}}{2}}{\sum_{j = 1}^{m} \frac{2(1 + \gamma)}{1 + \gamma}} = \frac{(1 + \gamma)^{2}}{2}.}\]  


The choice \(\alpha = 1 + \frac{1}{\gamma}\) satisfies the above.  


## A.5 On Bounding the Norms of Generalized Subgradients  


Lemma 8. Let \(f\) be an \(\alpha\) - approximately convex function with domain \(\mathcal{X}\) . Then from Theorem 3, part 3, there exists a convex function \(g\) such that \(g(x) \leq f(x) \leq \alpha g(x), \forall x \in \mathcal{X}\) . If \(h(x)\) is a sub- gradient of \(g\) at the point \(x \in \mathcal{X}\) then \(\alpha h(x)\) is a generalized subgradient of \(f\) at \(x \in \mathcal{X}\) .  


As a corollary, if \(\| h(x)\|_2 \leq G, \forall x \in \mathcal{X}\) , then the \(\ell_{2}\) - norms of the generalized subgradients of \(f\) as constructed above can be uniformly upper bounded by \(\alpha G\) .  


Proof. Note that \(h(x)\) always exists since \(g\) is convex. Therefore, for any \(u \in \mathcal{X}\) , we obtain,  


\[\alpha f(u) + \langle \alpha h(x),x - u\rangle = \alpha (f(u) + \langle h(x),x - u\rangle)\] \[\qquad \overset {(a)}{\geq}\alpha (g(u) + \langle h(x),x - u\rangle)\] \[\qquad \overset {(b)}{\geq}\alpha g(x)\] \[\qquad \overset {(c)}{\geq} f(x),\]  


where step (a) follows from the assumption that \(f(u) \geq g(u)\) , step (b) follows from the fact that \(h(x)\) is a sub- gradient of \(g\) at the point \(x\) , and (c) follows from the assumption that \(\alpha g(x) \geq f(x)\) . The final inequality shows that \(h(x)\) is a generalized sub- gradient of \(f\) at \(x \in \mathcal{X}\) . \(\square\)  


## A.6 Adaptive regret bounds for OCO  


In this Section, we briefly recall the first- order methods (a.k.a. Projected Online Gradient Descent (OGD)) for the standard OCO problem (Orbanon, 2019, Algorithm 2.1) [Hazan, 2022]. These methods differ among each other in the way the step sizes are chosen. For a sequence of convex cost functions \(\{f_{t}\}_{t \geq 1}\) , a projected OGD algorithm selects the successive actions as:  


\[x_{t + 1} = \mathcal{P}_{\mathcal{X}}(x_{t} - \eta_{t}\nabla_{1}), \forall t \geq 1, \quad (40)\]  


where \(\nabla_{1} \equiv \nabla \tilde{f}_{t}(x_{t})\) is a subgradient of the function \(\tilde{f}_{t}\) at \(x_{t}\) , \(\mathcal{P}_{\mathcal{X}}(\cdot)\) is the Euclidean projection operator on the set \(\mathcal{X}\) and \(\{\eta_{t}\}_{t \geq 1}\) is a specified step size schedule. The diagonal version of the) AdaGrad policy adaptively chooses the step size sequence as a function of the previous subgradients as \(\eta_{t} = \frac{\sqrt{2}L}{\sqrt{2} \sum_{i = 1}^{t} G_{i}^{2}}\) , where \(G_{i} = ||\nabla_{i}||_{2}, t \geq 1\) [Duchi et al., 2011]. 1 This algorithm enjoys the following adaptive regret bound.

<--- Page Split (13) --->

Theorem 9. (Orabona, 2019, Theorem 4.14) The AdaGrad policy, with the above step size sequence, achieves the following regret bound for the standard OCO problem:  


\[R e g r e t_{T}\leq \sqrt{2} D\sqrt{\sum_{t = 1}^{T}G_{t}^{2}}. \quad (41)\]  


## B Proof of Theorem 5  


Consider an ensemble of constrained learning problems defined in Section 3 with linear rewards, where each instance consists of two arms \(\mathcal{A}_{0},\mathcal{A}_{1}\) and a budget of \(B_{T} = \sqrt{2}\max (\sqrt{T h(T)},\pi (T))\) . Note that this implies that \(B_{T} = \Omega (\sqrt{T})\) . An online randomized policy selects one of these two arms in every round. In line with our deterministic formulation, we concisely the decision set and work with the expected rewards and consumptions. In particular, the decision set \(\mathcal{X}\) in this problem is taken to be the closed interval \([0,1]\) , which denotes the probability of pulling the arm \(\mathcal{A}_{1}\) .  


We partition the time horizon into \(T / \pi_{T}\) phases of duration \(B_{T}\) each 2. Next, we define \(T / B_{T}\) problem instances: for instance \(I_{s},r\in [\frac{T}{B} ]\) , arm \(\mathcal{A}_{1}\) has positive rewards up to and including phase \(r\) ; rewards for all subsequent phases are zero. In phase \(\sigma \in [\tau ]\) , arm \(\mathcal{A}_{s}\) has reward \(r B_{T} / T\) in each round. Arm \(\mathcal{A}_{1}\) consumes unit resource in each round. On the other hand, arm \(\mathcal{A}_{0}\) has zero rewards and zero consumptions on all rounds for all instances. In every round \(t\geq 1\) , let the randomized policy pulls arm \(\mathcal{A}_{1}\) with probability \(x_{t}\) , and pulls arm \(\mathcal{A}_{0}\) with the complementary probability \(1 - x_{t}\) . Hence, the expected reward and consumption functions for round \(t\) for the instance \(I_{r}\) are given to be  


\[f_{t}^{I_{r}}(x_{t}) = \left\{ \begin{array}{ll}\sigma B x_{t} / T; & t\in [\sigma B / T,(\sigma +1)B / T),\sigma \in [\tau ]\\ 0 & \end{array} \right.\] \[g_{t}^{I_{r}}(x_{t}) = x_{t}.\]  


It should be noted that, for each instance, the cost and constraint functions are linear in the action variable \(x_{t}\) .  


Analysis: We call a policy feasible if it satisfies the long- term budget constraint, i.e., does not violate the budget constraint. Fix some problem instance \(I_{r},r\in [B / T]\) . Let \(\mathtt{OPT}_{T}\) be the reward obtained by the best fixed feasible randomized policy for this instance. Consider any feasible randomized online policy \(\pi^{\prime}\) . From Immorlica et al. [2022, Theorem 8.1, part (b) and Lemma 8.6], it follows that for any feasible policy, there exists a problem instance \(\mathcal{I}_{r}\) s.t.1  


\[\mathtt{OPT}_{T} / \mathtt{R e w}_{T}(\pi^{\prime})\geq \Omega (\log T). \quad (42)\]  


Now consider an online policy \(\pi\) with budget constraint \(B_{T}\) , which pulls arm \(\mathcal{A}_{1}\) with probability \(x_{t}\) in every round \(t\geq 1\) . Note that the policy \(\pi\) is not necessarily feasible as its cumulative consumption after \(T\) rounds may exceed the budget \(B_{T}\) . We now modify the policy \(\pi\) to obtain a new online policy \(\pi^{\prime}\) which is feasible. The modified policy \(\pi^{\prime}\) pulls arm \(\mathcal{A}_{1}\) with probability \(x_{t} / \eta (T)\) and arm \(\mathcal{A}_{0}\) with probability \(1 - x_{t} / \eta (T)\) on round \(t\) , where \(\eta (T) = \left(\kappa +\frac{\eta(T)}{B_{T}}\right)\) . Due to the linearity of the rewards and consumptions with respect to the variable \(x_{t}\) , we have:  


\[\mathtt{R E W}_{T}(\pi^{\prime}) = \frac{1}{\eta(T)}\mathtt{R E W}_{T}(\pi),\mathtt{C O}_{T}(\pi^{\prime}) = \frac{1}{\eta(T)}\mathtt{C O}_{T}(\pi).\]  


Finally, using the cumulative consumption bound for the policy \(\pi\) , we can write  


\[\mathtt{C O}_{T}(\pi^{\prime})\leq \frac{\kappa B_{T} + s(T)}{\eta(T)} = B_{T}.\]  


This shows that the modified policy \(\pi^{\prime}\) is indeed feasible. Furthermore, using the regret guarantee for the policy \(\pi\) , we have  


\[\mathtt{R E W}_{T}(\pi^{\prime})\geq \frac{\mathtt{OPT}_{T} - h(T)}{\eta(T)} = \frac{\mathtt{OPT}_{T} - h(T)}{\kappa + \frac{\eta(T)}{B_{T}}}. \quad (43)\]

<--- Page Split (14) --->

From Immorlica et al. [2022, Lemma 8.9], we have that for any of the constructed instances, we have \(\mathsf{OPT}_{T} \geq \frac{nT}{2}\) . Thus  


\[\mathsf{OPT}_{T} - h(T) = \mathsf{OPT}_{T}(1 - \frac{h(T)}{\mathsf{OPT}_{T}}) \geq \mathsf{OPT}_{T}(1 - \frac{Th(T)}{B_{T}^{\frac{1}{2}}}) \overset {(*)}{\geq} \mathsf{OPT}_{T}(1 - \frac{Th(T)}{2Th(T)}) \geq \mathsf{OPT}_{T / 2}.\]  


where in (a), we have used the fact that \(B_{T} \geq \sqrt{2T h(T)}\) . Thus, from Eqn. (43), we have  


\[{\mathsf{R E W}}_{T}(\pi^{\prime}) \geq \frac{\mathsf{O P T}_{T}/2}{\kappa+\frac{Th(T)}{B_{T}^{\frac{1}{2}}}}\] \[i.e., \kappa+\frac{s(T)}{B_{T}} \geq \frac{\mathsf{O P T}_{T}}{\mathsf{2R B}_{T}(\pi^{\prime})}.\]  


Since \(B_{T} \geq \sqrt{2} s(T)\) , using the lower bound from Eqn. (42), it follows that there exists a problem instance \(I_{\tau}\) with \(\kappa \geq \Omega (\log T)\) .  


## C Extension to Multiple Resources  


Instead of a single resource as described in the main paper, we now assume that there are \(k \geq 1\) separate resources such that each resource has a separate budget constraint of \(B_{T}\) . 3 Note that since the we have a separate long- term budget constraint for each resource, unlike Sinha and Vaze [2024, Section 2.1], we can not reduce multiple resources into a single effective resource by taking the pointwise supremum of the consumption functions.  


This is because Sinha and Vaze [2024] assumed per- round feasibility for all constraints and the same would hold for pointwise supremum. Formally, if the following holds for each resource  


\[f_{t,i}(x^{*})\leq 0\quad \forall t\]  


then  


\[\max_{i}f_{t,i}(x^{*})\leq 0\quad \forall t\]  


We now extend our previous analysis to handle this general case.  


Let \(Q_{i}(t)\) be the cumulative consumption of the \(i^{\mathrm{th}}\) resource, which evolves as follows:  


\[Q_{i}(t) = Q_{i}(t - 1) + g_{i,i}(x_{i}),i\in [k].\]  


Let \(\Phi (\cdot)\) be a non- decreasing and convex Lyapunov function. We compute the drift for the \(i^{\mathrm{th}}\) resource as  


\[\Phi (Q_{i}(t)) - \Phi (Q_{i}(t - 1))\leq \Phi^{\prime}(Q_{i}(t))(Q_{i}(t) - Q_{i}(t - 1)) = \Phi^{\prime}(Q_{i}(t))g_{i,i}(x_{i}).\]  


Summing both sides of the inequality over all resources and then adding \(V(f_{t}(x_{t}) - \alpha f_{t}(x^{*}))\) to both sides, we obtain  


\[V(f_{t}(x_{t}) - \alpha f_{t}(x^{*})) + \sum_{i}(\Phi (Q_{i}(t)) - \Phi (Q_{i}(t - 11)))\] \[\leq (Vf_{t}(x_{t}) + \sum_{i}\Phi^{\prime}(Q_{i}(t))g_{i,i}(x_{i})) - \alpha (Vf_{t}(x^{*}) + \sum_{i}\Phi^{\prime}(Q_{i}(t))g_{t,i}(x^{*})) + \alpha \sum_{i}\Phi^{\prime}(Q_{i}(T))g_{t,i}(x^{*})\]

<--- Page Split (15) --->

where, in the last step, we have used the facts that \(\Phi^{\prime}(\cdot)\) is monotone (since \(\Phi (\cdot)\) is convex), \(Q(t)\) is nondecreasing, and \(g_{t} \geq 0\) . Summing up the above inequality over \(1 \leq t \leq T\) , we have the following regret decomposition inequality  


\[\begin{array}{rcl}\sum_{i}\left(\Phi (Q_{i}(T)) - \Phi (Q_{i}(0))\right) + V\mathrm{Regret}_{T}(\alpha) & \leq & \mathrm{Regret}_{T}^{\prime}(\alpha) + \sum_{i}\Phi^{\prime}(Q_{i}(T))\sum_{t = 1}^{T}g_{t,i}(x^{*}),\\ & \leq & \mathrm{Regret}_{T}^{\prime}(\alpha) + \alpha \sum_{i}\Phi^{\prime}(Q_{i}(T))B \end{array} \quad (44)\]  


where we have used the fact that \(\sum_{t}g_{t,i}(x^{*}) \leq B\) and \(\mathrm{Regret}_{T}^{\prime}(\alpha)\) is defined as the regret for learning the surrogate cost function sequence  


\[\dot{f}_{t} = Vf_{t} + \sum_{i}\Phi^{\prime}(Q_{i}(t))g_{t,i},t\geq 1, \quad (46)\]  


with the comparator taken to be \(\dot{f}_{t}(x^{*})\) , where \(x^{*}\) is a feasible action belonging to the set \(X^{*}\) . Note that the regret decomposition inequality (45) holds for any cost and non- negative constraint functions.  


We now make the assumption that the cost and constraint functions are \(\alpha\) - approximately convex and \(G\) - Lipschitz and we can bound the surrogate \(\alpha\) - regret by the regret incurred by passing \(\dot{f}_{t}(x) = \langle H_{\dot{f}_{t}}(x_{t}),x\rangle\) to an OLO algorithm. Using the analysis of section A.6, we have the following upper bound on the surrogate regret:  


\[\mathrm{Regret}_{T}^{\prime}(\alpha) \leq \mathrm{Regret}^{\prime}(\mathtt{OL}), \quad (47)\]  


\[\| H_{\dot{f}_{t}}(x_{t})\|_{2}\leq V\| H_{\dot{f}_{t}}(x_{t})\|_{2} + \sum_{i}\Phi^{\prime}(Q_{i}(t))\| H_{\dot{g}_{t}}(x_{t})\|_{2}\leq \alpha G(V + \sum_{i}\Phi^{\prime}(Q_{i}(T))). \quad (48)\]  


\[\mathrm{Regret}_{T}^{\prime}(\mathtt{OL})\leq \sqrt{2} GD\alpha (V + \sum_{i}\Phi^{\prime}(Q_{i}(T)))\sqrt{T}. \quad (49)\]  


Hence (45) yields:  


\[\sum_{i}\Phi (Q_{i}(T)) + V\mathrm{Regret}_{T}(x^{*})\leq \sum_{i}\Phi (Q_{i}(0)) + \alpha VGD\sqrt{2T} +\alpha \sum_{i}\Phi^{\prime}(Q_{i}(T))(GD\sqrt{2T} +B). \quad (50)\]  


Consider the exponential Lyapunov function: \(\Phi (x) = \exp (\lambda x)\) , where the value of \(\lambda\) will be fixed later. With this, inequality (50) yields  


\[\sum_{i}\exp (\lambda Q_{i}(T)) + V\mathrm{Regret}_{T}(x^{*})\leq k + \alpha VGD\sqrt{2T} +\lambda \alpha \sum_{i}\exp (\lambda Q_{i}(T))(GD\sqrt{2T} +B).\]  


Now we set \(\lambda = \frac{1}{2} (\alpha GD\sqrt{2T} +\alpha B)^{- 1}\) and \(V = (\alpha GD)^{- 1}\) . With this choice for the parameters, the above inequality yields:  


\[\frac{1}{2}\sum_{i}\exp (\lambda Q_{i}(T)) + V\mathrm{Regret}_T(x^*)\leq 1 + \sqrt{2T}. \quad (51)\]  


Regret Bound: Using the fact that \(\exp (\lambda Q_{i}(T)) \geq 1\) , Eqn. (51) yields  


\[\mathrm{Regret}_{T}(x^{*})\leq \alpha GD\sqrt{2T} +\alpha GD\frac{k}{2},\]  


where \(k\) is the numbers of resources.

<--- Page Split (16) --->

Bounding the \(\mathbb{C}\mathbb{C}\) : Since \(\mathrm{Regret}_{T}(x^{*})\geq -\alpha FT\) , where \(F\) is a uniform upper bound for the losses, Eqn. (51) yields for \(T\geq 1\) :  


\[\sum_{i}\exp (\lambda Q_{i}(T))\leq 2(1+FT/GD+\sqrt{2T})\]  


Hence,  


\[Q_{i}(T)\leq (\alpha B_{T} + GDN\overline{T})O(\log T)\forall i.\]  


## D Adversarial Bandits with Knapsacks  


In this Section, we demonstrate how the proposed online algorithm (Algorithm 1) and its analysis can be extended to the setting where the learner receives bandit feedback, i.e., only the losses and consumption of the selected actions are revealed to the learner. The setting we consider here is the same as the Bandits with Knapsacks (BwK) problem, considered by Immorlica et al. [2022], with the key difference that, in our case, the interaction between the learner and the adversary continues over the entire time- horizon and, consequently, we allow the constraints to be violated. Compared to the primal- dual- based algorithm proposed in Immorlica et al. [2022], which needs to guess the value of the optimal offline algorithm, our algorithm is simpler and does not need any such guesses and offers improved guarantees as shown next.  


Specifically, we consider a Multi- Armed Bandit (MAB) with \(K\) arms and a single resource with the budget constraint \(B_{T}\) . When arm \(a_{i}\in [K]\) is pulled on round \(t\) , it incurs a loss of \(l_{i}(a_{i})\in [0,1]\) and consumes \(c_{i}(a_{i})\in [0,1]\) amount of resource. The objective is to minimize the total loss while consuming close to the allocated budget over a horizon of length \(T\) .  


Because of the bandit feedback, the learner is informed of these two scalars only at round \(t\) . We make the standard assumption that the loss and consumption sequences, i.e., \(\{\mathbf{l}_{t},\mathbf{c}_{t}\}_{t = 1}^{T}\) , are generated in an oblivious fashion, i.e., they are fixed before the game begins. Hence, any randomized policy, that samples an arm \(a_{i}\) from a distribution \(x_{t}\in \Delta_{K}\) 4 on round \(t\) , incurs an expected cost of \(f_{t}(x_{t})\) and consumes an expected \(g_{t}(x_{t})\) amount of resource as given below:  


\[f_{t}(x_{t}) = \langle l_{t},x_{t}\rangle ,\quad g_{t}(x_{t}) = \langle c_{t},x_{t}\rangle .\]  


The regret and cumulative consumptions are defined as in Eqn. (2) and (3) as before. Our proposed constrained bandits algorithm, described in Algorithm 2, simply runs an adaptive bandit algorithm on a sequence of surrogate cost functions defined similar to Eqn. (12) as in the full- information case. However, for technical reasons which will be clear in the analysis, we use a power- law Lyapunov function rather than the exponential Lyapunov function as in the full- information case.  


Algorithm 2 Algorithm for Adversarial Bandits with Knapsacks  


1: Inputs: The set of arms \([K]\) , horizon length \(T\) , sequence of losses \(\{\mathbf{l}_{t}\}_{t = 1}^{T}\) and consumptions \(\{\mathbf{c}_{t}\}_{t = 1}^{T}\) , Budget \(B_{T}\) 2: Parameters: \(V:= \frac{c(\log K T / T + B_{T}\log T))^{a_{k} + 1}}{a_{k}V_{k}(1 / a_{k}T + 1)}\) , Lyapunov function \(\Phi (x) = x^{\log T}\) 3: Initialize a uniform sampling distribution \(x_{1}\in (1 / K,\dots,1 / K)\) and set \(Q(0)\gets \log T\) 4: for \(t = 1:T\) do 5: Sample an arm \(a_{t}\) from the probability distribution \(x_{t}\) 6: Observe \(l_{t}(a_{t})\) and \(c_{t}(a_{t})\) 7: Compute surrogate loss \(\hat{l}_{t}(a_{t}) = V l_{t}(a_{t}) + \epsilon \Phi^{\prime}(Q(t - 1))c_{t}(a_{t})\) 8: Update the cumulative consumed resource \(Q(t) = Q(t - 1) + c_{t}(a_{t})\) 9: Pass the observed surrogate loss \(\hat{l}_{t}(a_{t})\) to the adaptive MAB algorithm (Algorithm 3), which returns the next sampling distribution \(x_{t + 1}\in \Delta_{K}\) . 10: end for  


Theorem 10. Algorithm 2 achieves a regret bound of \(\tilde{O} (K\sqrt{T})\) while consuming at most \(\tilde{O} (K\sqrt{T}) + O(B_{T}\log T)\) amount of resources in expectation.

<--- Page Split (17) --->

Proof. We start with slightly modifying the derivation of the regret decomposition inequality in the full information setting from Section 5. The cumulative consumption evolves as  


\[Q(t) = Q(t - 1) + c_{t}(a_{t}). \quad (52)\]  


As before, let \(\Phi (\cdot)\) be a non- decreasing and convex Lyapunov function, which will be fixed later. We can bound the change in the Lyapunov function at round \(t\) as follows:  


\[\Phi (Q(t)) - \Phi (Q(t - 1))\overset {(a)}{=}\Phi '(Q(t))(Q(t) - Q(t - 1))\overset {(b)}{=}\Phi '(Q(t))c_{t}(a_{t})\overset {(c)}{\leq}\Phi '(Q(t - 1) + 1)c_{t}(a_{t})\overset {(d)}{\leq}c\Phi '(Q(t - 1))c_{t}(a_{t})\]  


where (a) follows from the convexity of \(\Phi (\cdot)\) , (b) follows from Eqn. (52), (c) follows from the fact that \(c_{t}(a_{t})\leq 1\) and (d) holds for our particular choice of the Lyapunov function with a proper initialization for \(Q(0)\) as shown in Lemma 12. Adding \(V(l_{t}(a_{t}) - \dot{l}_{t}(a^{*}))\) to both sides of the above inequality, we obtain:  


\[\Phi (Q(t)) - \Phi (Q(t - 1)) + V(l_{t}(a_{t}) - \dot{l}_{t}(a^{*}))\] \[\leq \quad (V(l_{t}(a_{t}) + c\Phi '(Q(t - 1))c_{t}(a_{t})) - (V\dot{f}_{t}(x^{*}) + c\Phi '(Q(t - 1))c_{t}(a^{*})) + c\Phi '(Q(t - 1))c_{t}(a^{*}),\]  


where the comparator \(a^{*}\) is taken to be a fixed randomized benchmark action that minimizes the expected cumulative costs subject to that satisfies the budget constraint in expectation, i.e., \(a^{*}\sim D^{*}\) where the distribution \(D^{*}\) solves the following optimization problem  


\[\min_{D\in \Delta_{K}}\mathbb{E}_{a^{*}\sim D}\sum_{t = 1}^{T}l_{t}(a^{*}),\mathrm{s.t.}\mathbb{E}_{a^{*}\sim D}\sum_{t = 1}^{T}c_{t}(a^{*})\leq B_{T}. \quad (54)\]  


Clearly, the distribution of the benchmark action \(a^{*}\) may depend on the entire sequence of loss and consumption vectors but not on the actions of the online policy. Similar to Eqn. (12), we now define the surrogate losses for the arms at round \(t\) as follows:  


\[\dot{l}_{t}(a) = V l_{t}(a) + c\Phi '(Q(t - 1))c_{t}(a),\forall a\in [K]. \quad (55)\]  


The main difference between the definitions of surrogate loss in the full information setting (12) and the bandit setting (55) is that the quantity \(\Phi '(Q(t))\) in the former is replaced with \(\Phi '(Q(t - 1))\) in the latter. Thus, in the above definition, the surrogate loss on round \(t\) does not depend on the action \(a_{t}\) of the algorithm at the same round. This is an essential requirement in the bandit feedback setting as, unlike the full- information algorithms, standard adversarial BAD algorithms randomize their actions to estimate the unseen loss components (e.g., using the inverse propensity score). This estimation process fails if the losses at round \(t\) also depend on the action of the policy at the same round. To summarize, Eqn. (55) implies that the surrogate loss \(I_{t}\) is \(\mathcal{F}_{t - 1}\) measurable, where \(\{\mathcal{F}_{r}\}_{r\geq 1}\) is the standard filtration.  


Summing up inequalities (53) for \(1\leq t\leq T\) and telescoping, we conclude that  


\[\Phi (Q(T)) - \Phi (Q(0)) + V\mathrm{Regret}_{T}\leq \mathrm{Regret}_{T}^{t} + c\Phi '(Q(T))\sum_{t = 1}^{T}c_{t}(a^{*}), \quad (56)\]  


where, as before, we have used the non- decreasing property of \(\Phi (\cdot)\) and the non- negativity of the consumption functions. As before, \(\mathrm{Regret}_{T}\) and \(\mathrm{Regret}_{T}^{t}\) correspond to the regrets w.r.t. the original and surrogate losses, both of which are computed against the fixed randomized action \(a^{*}\) . Taking expectations of both sides of (56) w.r.t. the randomness of the policy and the offline benchmark \(a^{*}\) , we have:  


\[\begin{array}{rcl}\mathbb{E}\Phi (Q(T)) - \mathbb{E}\Phi (Q(0)) + V\mathbb{E}\mathrm{Regret}_{T} & \stackrel {(a)}{=} & \mathbb{E}\mathrm{Regret}_{T}^{t} + c\mathbb{E}\Phi '(Q(T))\mathbb{E}\big(\sum_{t = 1}^{T}c_{t}(a^{*})\big)\\ & \stackrel {(b)}{=} & \mathbb{E}\mathrm{Regret}_{T}^{t} + c\not\equiv \Phi '(Q(T))B_{T}. \end{array} \quad (57)\]  


where in step (a), we have used the fact that the benchmark \(a^{*}\) is independent of the online policy, and hence, is independent of \(Q(T)\) , and in step (b), we have used the fact that \(a^{*}\) satisfies the budget constraint in expectation (Eqn. (54)). Eqn. (57) is analogous to the regret decomposition inequality (13) in the full- information setting.

<--- Page Split (18) --->

However, instead of using a full- information online learning policy, we now must use an adversarial MAB policy for learning the surrogate losses (55). Note that due to the factor \(\Phi '(Q(t - 1))\) in the surrogate loss, a tight upper bound to the surrogate losses can not be obtained _a priori_ as the evolution of the sequence \(\{Q(t)\}_{t\geq 1}\) depends on the online policy.  


Because of the above reasons, we use an adaptive MAB policy, proposed by Putta and Agrawal [2022], which does not need any _a priori_ upper bound on the magnitude of the losses, and yields a scale- free regret bound. On a high- level, the MAB algorithm proposed by Putta and Agrawal [2022] uses the FTRL sub- routine with a time- varying adaptive learning rate with the standard inverse- propensity score (IPS) estimator to estimate the unseen losses. For completeness, we give the pseudocode of the policy in Algorithm 3.  


Algorithm 3 Scale- Free Multi Armed Bandit  


1: Parameter initialization: \(x = K\) , \(\gamma_{0} = 1 / 2\)  


2: Regularizer: \(F(q) = \sum_{i = 1}^{K}\left(f(q(i)) - f(1 / K)\right)\) , where \(f(x) = - \log (x)\)  


3: Initialization: \(p_{1} = (1 / K,\ldots ,1 / K)\)  


4: for \(t = 1\) to \(T\) do  


5: Sampling Scheme: \(p_{t}^{\prime} = (1 - \gamma_{t - 1})p_{t} + \frac{\gamma_{t - 1}}{K}\)  


6: Sample arm \(i_{t}\sim p_{t}^{\prime}\) and see loss \(\ell_{t}(i_{t})\)  


7: Estimation Scheme: \(\hat{\ell}_{t}(i) = \frac{\ell_{t}(i)}{\sum_{i = 1}^{K}1(i_{t} = i)}\forall i\)  


8: Compute \(\gamma_{t}\) for next step: \(\gamma_{t} = \min (1 / 2,\sqrt{K / t})\)  


9: Compute \(\eta_{t} = \frac{1}{1 + \sum_{i = 1}^{K}\hat{\ell}_{t}(q_{i} - i)}\) , where  


\[M_{t}(q) = \sup_{q^{\prime}\in \Delta_{K}}\left[\hat{\ell}_{t}^{\top}(p_{t} - q) - \frac{1}{t}\mathrm{Reg}_{F}(q\| p_{t})\right]\]  


10: Find the next sampling distribution using FTRL:  


\[p_{t + 1} = \arg \min_{q\in \Delta_{K}}\left[F(q) + \eta_{t}\sum_{i = 1}^{t}q^{\top}\hat{\ell}_{s}\right]\]  


11: end for  


Theorem 11 (Putta and Agrawal [2022]). The MAB policy described in Algorithm 3, when run with the sequence of loss vectors \(\{\hat{\ell}_{t}\}_{t = 1}^{T}\) , enjoys the following scale- free regret bound:  


\[\mathbb{E}Reg_{T}\leq 2\bigg(1 + \sqrt{\frac{K}{t}\sum_{t = 1}^{T}\|\hat{\ell}_{t}\|_{2}^{2} + \min_{t\in [K]}\|\hat{\ell}_{t}\|_{\infty}\sqrt{KT}}\bigg)\bigg(2 + \log (1 + \| \sum_{t}\hat{\ell}_{t}\|_{\infty})\bigg), \quad (58)\]  


where the expectation is taken over the randomness of the algorithm.  


Remarks: Note that although the surrogate loss (55) on round \(t\) depends on the actions of the online algorithm up to round \(t - 1\) , the benchmark \(a^{*}\) , as discussed above, is oblivious to the action of the algorithms and can be decided at the start of the play by an offline oracle. Since the action of the algorithm at round \(t\) is independent of the losses at round \(t\) , it is clear that we can upper bound the regret of the surrogate problem against the benchmark \(a^{*}\) using any oblivious MAB regret bound.  


We now return to the proof of our main results. Using the adaptive regret bound from Eqn. (58) to the regret decomposition inequality in Eqn. (57) with the surrogate loss \(\hat{\ell}_{t}\) defined in Eqn. (55), we obtain  


\[\leq 2\mathbb{E}\bigg[1 + \sqrt{N\sum_{t = 1}^{T}\|\hat{l}_{t}\|_{2}^{2}} +\max_{t\in [T]}\|\hat{l}_{t}\|_{\infty}\sqrt{KT}\bigg]\bigg(2 + \log (1 + T\max_{t\in [T]}\|\hat{l}_{t}\|_{\infty})\bigg) + \epsilon \mathbb{E}\Phi^{\prime}(Q(T))B_{T}.\]

<--- Page Split (19) --->

Using the fact that \(|\tilde{t} |_{t}|_{2}\leq \sqrt{K} |\tilde{t} |_{t}|_{\infty}\) , and \(\| \sum_{t}\tilde{t}_{t}\|_{\infty}\leq T\max_{t\in [T]}||\tilde{t} |_{t}|_{\infty}\) , we have the following bound  


\[1 + \sqrt{\frac{N}{T}\sum_{t = 1}^{T}||\tilde{t}_{t}||_{2}^{2} + \max_{t\in [T]}||\tilde{t}_{t}||_{\infty}\sqrt{K T}}\leq 1 + \max_{t\in [T]}||\tilde{t}_{t}||_{\infty}K\sqrt{T} +\max_{t\in [T]}||\tilde{t}_{t}||_{\infty}\sqrt{k T}\leq 3\max_{t\in [T]}||\tilde{t}_{t}||_{\infty}K\]  


Further, in Lemma 13, we show that for our choice of the Lyapunov function \(\Phi (\cdot)\) , the parameter \(V\) , and using the trivial bound \(Q(T)\leq T\) , we have \(\max_{t\in [T]}||\tilde{t}_{t}||_{\infty}\leq (e^{2}T\log T)^{\log T}\) . Hence, we can upper bound the logarithmic pre- factor as follows:  


\[2 + \log (1 + T\max_{t\in [T]}||\tilde{t}_{t}||_{\infty})\leq 2 + \log (2T\max_{t\in [T]}||\tilde{t}_{t}||_{\infty})\lesssim 2\log (T\max_{t\in [T]}||\tilde{t}_{t}||_{\infty}) \leq 3(\log T)^{2}.\]  


Plugging in the above bounds in the regret decomposition inequality (59), we obtain  


\[\mathbb{E}\Phi (Q(T)) - \mathbb{E}\Phi (Q(0)) + V\mathbb{E}\mathrm{Regret}_{T}\leq 18\max_{t\in [T]}|\tilde{t}_{t}||_{\infty}K\sqrt{T} (\log T)^{2} + e\mathbb{E}\Phi '(Q(T))B_{T}. \quad (60)\]  


Now, note that  


\[|\tilde{t}_{t}(a)|\leq Vt_{t}(a) + e\Phi '(Q(t - 1))c_{t}(a)\leq V + e\Phi '(Q(T)),\forall t,a,\]  


where, in the above, we have used the fact that \(t_{t}(a)\in [0,1],c_{t}(a)\in [0,1],\forall t,a\) , and the monotonicity of the function \(\Phi^{\prime}(\cdot)\) . This yields  


\[\max_{t\in [T]}|\tilde{t}_{t}||_{\infty}\leq V + e\Phi '(Q(T)).\]  


Using the above bounds, Eqn. (60) simplifies to  


\[\mathbb{E}\Phi (Q(T)) - \mathbb{E}\Phi ( Q(0)) + V\mathbb{E}\mathrm{Regret}_{T} \leq 18(V + e\mathbb{E}\Phi '(Q(T)))K\sqrt{T} (\log T)^{2} + e\mathbb{E}\Phi '(Q(T)))B_{T}\] \[\qquad = 18VK\sqrt{T} (\log T)^{2} + e(18K\sqrt{T} (\log T)^{2} + B_{T})\mathbb{E}\Phi '(Q(T)).\]  


Finally, we choose a power- law Lyapunov function \(\Phi (x) = x^{m}\) with the exponent \(m = \log T\) , and initialize \(Q(0) = \log T\) . With these choices, inequality (61) yields  


\[\mathbb{E}Q^{m}(T) + V\mathbb{E}\mathrm{Regret}_{T}\leq 18VK\sqrt{T} (\log T)^{2} + me(18K\sqrt{T} (\log T)^{2} + B_{T})\mathbb{E}Q^{m - 1}(T) + (\log T)^{m}. \quad (62)\]  


We now analyze the above inequality for bounding both regret and the cumulative consumptions (CC).  


Bounding the Cumulative Consumption (CC): The slices on the loss can each round be bounded by one, we trivially have \(\mathbb{E}\mathrm{Regret}_{T}\geq - T\) . Plugging this in (62) yields  


\[\begin{array}{rcl}\mathbb{E}Q^{m}(T) & \leq & 18VK\sqrt{T} (\log T)^{2} + VT + me(18K\sqrt{T} (\log T)^{2} + B_{T})\mathrm{E}Q^{m - 1}(T) + (\log T)^{m}\\ & \leq & 2\max \left(18VK\sqrt{T} (\log T)^{2} + VT + (\log T)^{m},me(18K\sqrt{T} (\log T)^{2} + B_{T})\mathrm{EQ}^{m - 1}(T)\right). \end{array} \quad (63)\]  


If the first term is dominant in the above \(\max (\cdot)\) operator in Eqn. (63), we have  


\[\begin{array}{r l} & {\mathbb{E}Q^{m}(T)\leq 36VK\sqrt{T} (\log T)^{2} + 2VT + 2(\log T)^{m}}\\ & {\quad \overset {(a)}{\underset {(b)}{\leq}}(\mathbb{E}Q(T))^{m}\leq 36VK\sqrt{T} (\log T)^{2} + 2VT + \underset {(c)}{\leq}2\log T.}\\ & {\quad \overset {(b)}{\underset {(c)}{\leq}}(\mathbb{E}Q(T)\leq (36VK\sqrt{T} (\log T)^{2})^{\frac{1}{m}} + (2VT)^{\frac{1}{m}} + 2\log T.} \end{array} \quad (64)\]  


where (a) follows from the convexity of the mapping \(x\mapsto x^{m}\) and applying Jensen's inequality and (b) follows from the fact that \((a + b)^{1 / m}\leq a^{1 / m} + b^{1 / m}\) for any \(m\geq 1,a\geq 0,b\geq 0\)  


Similarly, if the second term within the \(\max (\cdot)\) operator in (63) is dominant, we have  


\[\mathbb{E}Q^{m}(T)\leq 2me(18K\sqrt{T} (\log T)^{2} + B_{T})\mathbb{EQ}^{m - 1}(T)\]

<--- Page Split (20) --->

\[\begin{array}{rl} & {\overset {(a)}{=}\mathbb{E}[Q^{m - 1}(T)]^{\frac{m}{m + 1}}\leq 2me(18K\sqrt{T}(\log T)^{2} + B_{T})\mathbb{E}Q^{m - 1}(T)}\\ & {\Longrightarrow \mathbb{E}[Q^{m - 1}(T)]^{\frac{1}{m + 1}}\leq 2me(18K\sqrt{T}(\log T)^{3} + B_{T})}\\ & {\overset {(b)}{=}\mathbb{E}Q(T)\leq 2me(18K\sqrt{T}(\log T)^{2} + B_{T}),} \end{array} \quad (65)\]  


where \((a)\) follows from applying Jensen's inequality on the LHS to the convex map \(x\mapsto x^{\frac{m}{m + 1}}\) , resulting in  


\[\mathbb{E}(Q^{m}(T)) = \mathbb{E}\left[(Q^{m - 1}(T))^{\frac{m}{m + 1}}\right]\geq \mathbb{E}\left[Q^{m - 1}(T)\right]^{\frac{m}{m + 1}},\]  


and (b) follows from the convexity of the map \(x\mapsto x^{m - 1}\) and using Jensen's inequality.  


Combining (64) and (65), we conclude that the cumulative consumption of the proposed policy is upper bounded as:  


\[\mathbb{E}Q(T)\leq \max ((36V K\sqrt{T}(\log T)^{2})^{\frac{1}{m}} + (2V T)^{\frac{1}{m}} + 2\log T,2me(18K\sqrt{T}(\log T)^{2} + B_{T})).\]  


Bounding the Regret: Next, we start from (62) to bound the regret as follows. Transforming the term \(\mathbb{E}Q^{m}(T)\) to the right, we have  


\[\mathbb{E}\mathrm{Regret}_{T}\leq 18K\sqrt{T}(\log T)^{2} + \frac{1}{V}\mathbb{E}\Big[\underbrace{(me(18K\sqrt{T}(\log T)^{2} + B_{T}) - Q(T))Q^{m - 1}(T)}_{(me(18K\sqrt{T}(\log T)^{2} + B_{T}))^{m}}\Big].\]  


The upper bound on the last term is obtained by considering two possible cases:  


Case I: \(Q(T) > me(18K\sqrt{T}(\log T)^{2} + B_{T})\) : In this case, the last term is non- positive.  


Case II: \(0\leq Q(T)\leq me(18K\sqrt{T}(\log T)^{2} + B_{T})\) : Inthis case, we simply use the upper bound \(Q(T)\leq\) \(me(18K\sqrt{T}(\log T)^{2} + B_{T})\) to bound \(Q^{m - 1}(T)\)  


Finally, choosing the parameter \(V = \frac{(me(18K\sqrt{T}(\log T)^{2} + B_{T}))^{m}}{36K\sqrt{T}(\log T)^{2}}\) , and \(m = \log T\) , the regret can be bounded as follows:  


\[\mathbb{E}\mathrm{Regret}_{T}\leq 54K\sqrt{T}(\log T)^{2}.\]  


Substituting the above parameter choices in (66), the cumulative consumption can be bounded as follows:  


\[\mathbb{E}Q(T)\leq e^{2}(18K\sqrt{T}(\log T)^{3} + B_{T}\log T).\]  


This concludes our analysis of the proposed algorithm in the bandit setting.  


## Supporting Lemmas:  


Lemma 12. For the power potential \(\Phi (x) = x^{m}\) with \(m = \log T\) and \(Q(0) = \log T\) the following holds  


\[\Phi^{\prime}(Q(t - 1) + 1)\leq e\Phi^{\prime}(Q(t - 1)).\]  


Proof. We have  


\[\Phi^{\prime}(Q(t - 1) + 1) = m(Q(t - 1) + 1)^{m - 1}\] \[= mQ^{m - 1}(t - 1)\big(1 + \frac{1}{Q(t - 1)}\big)^{m - 1}\] \[\overset {(a)}{\leq} mQ^{m - 1}(t - 1)\bigg(1 + \frac{1}{Q(0)}\bigg)^{m - 1}\] \[\overset {(b)}{\leq} e\Phi^{\prime}(Q(t - 1)).\]  


where (a) follows because \(Q(0)\leq Q(t - 1)\) and (b) follows because \(Q(0) = \log T = m\) and \((1 + \frac{1}{m})^{m - 1}<\) \((1 + \frac{1}{m})^{m}\leq e\)

<--- Page Split (21) --->

Lemma 13. The magnitude of the surrogate loss \(\hat{l}_{t}(a) = V_{t}(a) + c\Phi^{\prime}(Q(t - 1))c_{t}(a)\) can be uniformly bounded as follows: 


\[
\min_{t \in [T]} \| \hat{l}_t \|_\infty \leq (c^2 T \log T)^{l \log T}
\]


where \(V = \frac{(me(18K\sqrt{T}(\log T)^2 + B_T))^m}{36K\sqrt{T}(\log T)^2}\) and \(\Phi(x) = x^m\) with \(m = \log T\). 


Proof. 


\[
\min_{t \in [T]} \|\hat{l}_t\|_\infty \le V + c\Phi'(Q(T)) \le (me(18K\sqrt{T}(\log T)^2 + B_T))^m + em(T + \log T)^m \le (c^2 T \log T)^{l \log T}
\]


where the second last inequality follows because \(V < (me(18K\sqrt{T}(\log T)^2 + B_T))^m\), \(B_T \le T\) (the sum of constraint violations is bounded by \(T\)) and \(Q(T) \le T + \log T\). \(\square\)

<--- Page Split (22) --->

